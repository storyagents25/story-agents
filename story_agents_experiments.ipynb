{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install Required Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB88abH4UDBM"
      },
      "outputs": [],
      "source": [
        "# Install all required dependencies\n",
        "pip install -U langchain-community langchain_openai numpy pandas matplotlib seaborn nbformat ipykernel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-trKEVJgOPH"
      },
      "outputs": [],
      "source": [
        "# Standard library\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import csv\n",
        "import pickle\n",
        "import statistics\n",
        "import glob\n",
        "import itertools\n",
        "\n",
        "# Third-party libraries\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "import seaborn as sns\n",
        "from typing import List, Dict, Any, TextIO\n",
        "\n",
        "# Project-specific or custom packages\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage, BaseMessage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vOwokPWUnZv"
      },
      "source": [
        "# Load and Configure LLM Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqJOQxoqUnpn"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Central Configuration\n",
        "# -----------------------------\n",
        "config: Dict[str, Any] = {\n",
        "    \"llm_choice\": os.getenv(\"LLM_CHOICE\", \"llama\"),\n",
        "    \"llama\": {\n",
        "        \"model_name\": os.getenv(\"LLAMA_MODEL_NAME\", \"meta-llama-3.3-70b-instruct-fp8\"),\n",
        "        \"temperature\": float(os.getenv(\"LLAMA_TEMP\", 0.6)),\n",
        "        \"max_tokens\": int(os.getenv(\"LLAMA_MAX_TOKENS\", 500)),\n",
        "        \"api_url\": os.getenv(\"LLAMA_API_URL\")\n",
        "    },\n",
        "    \"openai\": {\n",
        "        \"model_name\": os.getenv(\"OPENAI_MODEL_NAME\", \"gpt-4o-mini\"),\n",
        "        \"temperature\": float(os.getenv(\"OPENAI_TEMP\", 1.0)),\n",
        "        \"max_tokens\": int(os.getenv(\"OPENAI_MAX_TOKENS\", 10)),\n",
        "        \"api_key\": os.getenv(\"OPENAI_API_KEY\")\n",
        "    },\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Llama Helper Class\n",
        "# -----------------------------\n",
        "class Llama:\n",
        "    def __init__(self, model_name: str, temperature: float, max_tokens: int, api_url: str) -> None:\n",
        "        self.model_name = model_name\n",
        "        self.temperature = temperature\n",
        "        self.max_tokens = max_tokens\n",
        "        self.api_url = api_url\n",
        "\n",
        "    def invoke(self, history: List[BaseMessage]) -> AIMessage:\n",
        "        # Build messages in a robust way.\n",
        "        messages: List[Dict[str, str]] = []\n",
        "        for msg in history:\n",
        "            # Try to extract role and content, whether msg is an object or dict.\n",
        "            if hasattr(msg, \"role\") and hasattr(msg, \"content\"):\n",
        "                role = msg.role\n",
        "                content = msg.content\n",
        "            elif isinstance(msg, dict):\n",
        "                role = msg.get(\"role\", \"unknown\")\n",
        "                content = msg.get(\"content\", str(msg))\n",
        "            else:\n",
        "                role = \"unknown\"\n",
        "                content = str(msg)\n",
        "            messages.append({\"role\": role, \"content\": content})\n",
        "\n",
        "        data: Dict[str, Any] = {\n",
        "            \"model\": self.model_name,\n",
        "            \"messages\": messages,\n",
        "            \"max_tokens\": self.max_tokens,\n",
        "            \"temperature\": self.temperature\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(self.api_url, json=data, timeout=30)\n",
        "            response.raise_for_status()\n",
        "        except requests.exceptions.Timeout:\n",
        "            raise RuntimeError(\"Llama API request timed out.\")\n",
        "        except requests.exceptions.HTTPError as http_err:\n",
        "            raise RuntimeError(f\"Llama API HTTP error: {http_err}\")\n",
        "        except requests.exceptions.RequestException as err:\n",
        "            raise RuntimeError(f\"Llama API connection error: {err}\")\n",
        "\n",
        "        try:\n",
        "            response_data = response.json()\n",
        "            content = response_data[\"choices\"][0][\"message\"][\"content\"]\n",
        "        except (ValueError, KeyError, IndexError) as parse_err:\n",
        "            raise RuntimeError(f\"Failed to parse Llama API response: {parse_err}\")\n",
        "\n",
        "        return AIMessage(content=content)\n",
        "\n",
        "def log_records(message: str, records_file: TextIO) -> None:\n",
        "    \"\"\"\n",
        "    Helper function to log a message to records file so we can keep track of all history.\n",
        "    \"\"\"\n",
        "    records_file.write(message + \"\\n\")\n",
        "    records_file.flush() # make sure it's written immediately\n",
        "\n",
        "def calculate_payoffs(contributions: List[float], e: float, m: float, na: int) -> List[float]:\n",
        "    \"\"\"\n",
        "    Given a list of contributions from each agent, compute the payoff for each agent.\n",
        "    \"\"\"\n",
        "    total: float = sum(contributions)\n",
        "    shared_bonus: float = m * total / na\n",
        "    return [e - c + shared_bonus for c in contributions]\n",
        "\n",
        "# -----------------------------\n",
        "# Agent Class\n",
        "# -----------------------------\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    A simple agent that uses an LLM backend determined by LLM_CHOICE.\n",
        "    It maintains its own conversation history so that previous rounds and summaries\n",
        "    can influence its responses.\n",
        "    \"\"\"\n",
        "    def __init__(self, name: str, system_message: str, records_file: TextIO) -> None:\n",
        "        self.name = name\n",
        "        # Start the conversation with a system message.\n",
        "        self.history: List[BaseMessage] = [SystemMessage(content=system_message)]\n",
        "        self.records_file = records_file\n",
        "\n",
        "        # Log the creation of this agent and its system prompt to records.\n",
        "        log_records(f\"CREATING AGENT: {self.name}\", records_file)\n",
        "        log_records(f\"System Prompt for {self.name}: {system_message}\", records_file)\n",
        "\n",
        "        # Create the appropriate LLM instance\n",
        "        llm_choice = config[\"llm_choice\"]\n",
        "        if llm_choice == \"llama\":\n",
        "            llama_cfg = config[\"llama\"]\n",
        "            self.llm = Llama(\n",
        "                model_name=llama_cfg[\"model_name\"],\n",
        "                temperature=llama_cfg[\"temperature\"],\n",
        "                max_tokens=llama_cfg[\"max_tokens\"],\n",
        "                api_url=llama_cfg[\"api_url\"]\n",
        "            )\n",
        "        else:\n",
        "            openai_cfg = config[\"openai\"]\n",
        "            self.llm = ChatOpenAI(\n",
        "                model_name=openai_cfg[\"model_name\"],\n",
        "                temperature=openai_cfg[\"temperature\"],\n",
        "                max_tokens=openai_cfg[\"max_tokens\"],\n",
        "                openai_api_key=openai_cfg[\"api_key\"]\n",
        "            )\n",
        "\n",
        "    def chat(self, message: str) -> str:\n",
        "        \"\"\"\n",
        "        Append a human message, call the LLM, append the assistant's reply,\n",
        "        log everything, and return the response content.\n",
        "        \"\"\"\n",
        "        log_records(f\"{self.name} receives HUMAN message: {message}\", self.records_file)\n",
        "        self.history.append(HumanMessage(content=message))\n",
        "        try:\n",
        "            response = self.llm.invoke(self.history) # Use the pre-created ChatOpenAI instance.\n",
        "        except RuntimeError as err:\n",
        "            log_records(f\"{self.name} ERROR: {err}\", self.records_file) \n",
        "            return f\"[ERROR]: {err}\"\n",
        "\n",
        "        # Store the response in the conversation history\n",
        "        self.history.append(response)\n",
        "        # Brief pause to help avoid rate limits\n",
        "        log_records(f\"{self.name} responds ASSISTANT: {response.content}\", self.records_file)       \n",
        "        time.sleep(0.01)\n",
        "        return response.content\n",
        "\n",
        "# -----------------------------\n",
        "# DummyAgent Class\n",
        "# -----------------------------\n",
        "class DummyAgent:\n",
        "    \"\"\"\n",
        "    A 'dummy' agent that does NOT connect to an LLM and always contributes 0.\n",
        "    \"\"\"\n",
        "    def __init__(self, name: str, system_message: str, records_file: TextIO) -> None:\n",
        "        self.name = name\n",
        "        self.history: List[BaseMessage] = [SystemMessage(content=system_message)]\n",
        "        self.records_file = records_file\n",
        "\n",
        "        log_records(f\"CREATING DUMMY AGENT: {self.name}\", records_file)\n",
        "        log_records(f\"(Dummy) System Prompt for {self.name}: {system_message}\", records_file)\n",
        "\n",
        "    def chat(self, message: str) -> str:\n",
        "        \"\"\"\n",
        "        This agent always contributes \"0\".\n",
        "        We still log the conversation but do not call any LLM.\n",
        "        \"\"\"\n",
        "        log_records(f\"{self.name} (dummy) receives HUMAN message: {message}\", self.records_file)\n",
        "        # We return \"0\" as a string to emulate the minimal integer-based response.\n",
        "        response_content = \"<TOKEN>0</TOKEN>\"\n",
        "        log_records(f\"{self.name} (dummy) responds ASSISTANT: {response_content}\", self.records_file)\n",
        "        time.sleep(0.01)  # a brief pause, mirroring the normal agent\n",
        "        return response_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FYu4yGHVSS0"
      },
      "source": [
        "# Define Agent Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrOukVO1VScq"
      },
      "outputs": [],
      "source": [
        "def load_intermediate_results(exp_name, is_dict=True):\n",
        "    \"\"\"Loads intermediate results from a pickle file if it exists.\"\"\"\n",
        "    intermediate_results_filename = f\"results_{exp_name}.pkl\"\n",
        "    if os.path.exists(intermediate_results_filename):\n",
        "        with open(intermediate_results_filename, 'rb') as f:\n",
        "            intermediate_results = pickle.load(f)\n",
        "        print(f\"Loaded intermediate results from {intermediate_results_filename}.\")\n",
        "    else:\n",
        "        intermediate_results = {} if is_dict else []\n",
        "    return intermediate_results, intermediate_results_filename\n",
        "\n",
        "def save_intermediate_results(intermediate_results, intermediate_results_filename):\n",
        "    \"\"\"Saves intermediate results to a pickle file.\"\"\"\n",
        "    with open(intermediate_results_filename, 'wb') as f:\n",
        "        pickle.dump(intermediate_results, f)\n",
        "\n",
        "def get_system_prompt(na, e, m):\n",
        "    \"\"\"\n",
        "    Generates the system prompt.\n",
        "    \n",
        "    Args:\n",
        "        na (int): Number of agents.\n",
        "        e (int): Endowment per round (tokens given to each agent).\n",
        "        m (float): Contribution multiplier affecting payoffs.\n",
        "    \n",
        "    Returns:\n",
        "        str: The formatted system prompt.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        f\"You are participating in a repeated public goods game with {na - 1} other agents. \"\n",
        "        f\"In each round, you receive {e} tokens. You must decide how many tokens (an integer between 0 and {e}) \"\n",
        "        f\"to contribute to a public pool. After everyone contributes, the total contribution is multiplied by {m} \"\n",
        "        f\"and divided equally among all {na} agents. This means your payoff for a round is calculated as: \\n\\n\"\n",
        "        f\"    {e} - (your contribution) + ({m} * total contributions / {na})\\n\\n\"\n",
        "        \"Please respond **only** with the number of tokens you wish to contribute for this round, enclosed within `<TOKEN>` and `</TOKEN>` tags. \"\n",
        "        \"For example: `<TOKEN>5</TOKEN>`. Do not provide any additional text, explanations, or summaries. \"\n",
        "        \"STORY\"\n",
        "    )\n",
        "\n",
        "def compute_and_print_statistics(scores_by_prompt, exp_name):\n",
        "    \"\"\"Computes and prints mean and standard deviation of scores for each prompt.\"\"\"\n",
        "    print(f\"\\n=== Comparison of Effective Collaboration Scores Over Games for experiment {exp_name} ===\")\n",
        "    for prompt_label, score_list in scores_by_prompt.items():\n",
        "        mean_val = statistics.mean(score_list)\n",
        "        stdev_val = statistics.stdev(score_list) if len(score_list) > 1 else 0\n",
        "        print(f\"{prompt_label}: Mean = {mean_val:.2f}, SD = {stdev_val:.2f}\")\n",
        "\n",
        "def extract_contribution(response_str):\n",
        "    \"\"\"\n",
        "    Extracts a valid contribution from an agent's response.\n",
        "    - First, looks for a contribution enclosed in <TOKEN>...</TOKEN>.\n",
        "    - If found, extracts the first standalone integer from the match.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Look for <TOKEN>...</TOKEN> first\n",
        "    match = re.search(r\"<TOKEN>(\\d+)</TOKEN>\", response_str)\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_valid_contribution(agent, round_num, e, max_retries=5):\n",
        "    \"\"\"\n",
        "    Repeatedly queries the agent for a valid contribution up to max_retries.\n",
        "    If the response is invalid for more than max_retries, defaults to 0.\n",
        "    \"\"\"\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        prompt = f\"Round {round_num}: What is your contribution (0-{e})?\"\n",
        "        \n",
        "        if retries > 0:\n",
        "            prompt += \" Your previous response was invalid. **Only provide a number inside `<TOKEN>...</TOKEN>`** with no extra text. Example: `<TOKEN>5</TOKEN>`.\"\n",
        "        \n",
        "        response_str = agent.chat(prompt).strip()\n",
        "        print(f\"{agent.name} (Story: {agent.story_label}) response (attempt {retries + 1}): {response_str}\")\n",
        "\n",
        "        contribution = extract_contribution(response_str)\n",
        "        \n",
        "        if contribution is not None:\n",
        "            return contribution\n",
        "        \n",
        "        print(f\"Warning: {agent.name} provided an invalid response. Retrying... ({retries + 1}/{max_retries})\")\n",
        "        retries += 1\n",
        "\n",
        "    # If all retries fail, default to 0 and log the failure\n",
        "    print(f\"Error: {agent.name} failed to provide a valid response after {max_retries} attempts. Defaulting to 0.\")\n",
        "    return 0\n",
        "\n",
        "def collect_contributions(agents, round_num, e):\n",
        "    \"\"\"Collects valid contributions from all agents.\"\"\"\n",
        "    contributions = []\n",
        "    for agent in agents:       \n",
        "        contribution = get_valid_contribution(agent, round_num, e)\n",
        "        \n",
        "        # Enforce valid contribution range\n",
        "        available_tokens = e # Each agent gets `e` tokens every round\n",
        "        if contribution > available_tokens:\n",
        "            print(f\"{agent.name} attempted to contribute {contribution} tokens but only has {available_tokens}. \"\n",
        "                    f\"Reducing contribution to {available_tokens}.\")\n",
        "            contribution = available_tokens\n",
        "        contribution = max(0, contribution)\n",
        "        contributions.append(contribution)\n",
        "    print(f\"Round Contributions: {contributions}\")\n",
        "    return contributions\n",
        "\n",
        "def calculate_rewards(contributions, agents, e, m, na, total_rewards, round_num):\n",
        "    \"\"\"Calculates payoffs and updates agent rewards.\"\"\"\n",
        "    round_total = sum(contributions)    \n",
        "    # Calculate payoffs for the round.\n",
        "    payoffs = calculate_payoffs(contributions, e, m, na)\n",
        "    print(f\"Round Payoffs: {payoffs}\")\n",
        "\n",
        "    for idx, agent in enumerate(agents):\n",
        "        total_rewards[idx] += payoffs[idx]\n",
        "        summary = (\n",
        "            f\"Round {round_num} Summary:\\n\"\n",
        "            f\" - Your contribution: {contributions[idx]}\\n\"\n",
        "            f\" - Total contributions: {round_total}\\n\"\n",
        "            f\" - Your payoff this round: {payoffs[idx]:.2f}\\n\"\n",
        "            f\" - Your cumulative reward: {total_rewards[idx]:.2f}\"\n",
        "        )\n",
        "        agent.chat(summary)\n",
        "\n",
        "    return payoffs, round_total, total_rewards\n",
        "\n",
        "\n",
        "def log_round_results(csv_writer, agents, contributions, payoffs, total_rewards, game_index, prompt_label, round_num, exp_type):\n",
        "    \"\"\"Logs round results to CSV.\"\"\"\n",
        "    for idx, agent in enumerate(agents):\n",
        "        story_or_prompt_label = agent.story_label if exp_type == \"different_story\" else prompt_label\n",
        "        # Write per-round info to the CSV file.\n",
        "        csv_writer.writerow([\n",
        "            game_index,\n",
        "            story_or_prompt_label,\n",
        "            round_num,\n",
        "            agent.name,\n",
        "            contributions[idx],\n",
        "            f\"{payoffs[idx]:.2f}\",\n",
        "            f\"{total_rewards[idx]:.2f}\",\n",
        "            \"\" # CollaborationScore left empty for per-round details.\n",
        "        ])\n",
        "def execute_game_rounds(agents, na, nr, e, m, csv_writer, records_file, game_index, prompt_label, exp_type, num_dummy_agents):\n",
        "    \"\"\"\n",
        "    Executes a full game session consisting of multiple rounds where agents contribute to a shared pool.\n",
        "    Args:\n",
        "        agents (list): List of agent objects.\n",
        "        na (int): Number of agents.\n",
        "        nr (int): Number of rounds.\n",
        "        e (int): Endowment per round.\n",
        "        m (float): Multiplier for contributions.\n",
        "        csv_writer (csv.writer): CSV writer object.\n",
        "        records_file (file object): File for logging agent responses.\n",
        "        game_index (int): Game identifier.\n",
        "        prompt_label (str): Label for the prompt or story used.\n",
        "        exp_type (str): \"same_story\" or \"different_story\" (to determine CSV formatting).\n",
        "        num_dummy_agents: Number of dummy agents in the game\n",
        "    Returns:\n",
        "        effective_score (float): The overall collaboration score.\n",
        "        total_rewards (list): Cumulative rewards for each agent.\n",
        "    \"\"\"\n",
        "    total_rewards = [0 for _ in range(na)]\n",
        "    total_game_contributions = 0\n",
        "\n",
        "    print(\"\\n=== Starting a New Game ===\")\n",
        "    for round_num in range(1, nr + 1):\n",
        "        print(f\"\\n--- Round {round_num} ---\")\n",
        "        contributions = collect_contributions(agents, round_num, e)\n",
        "        payoffs, round_total, total_rewards = calculate_rewards(contributions, agents, e, m, na, total_rewards, round_num)\n",
        "        log_round_results(csv_writer, agents, contributions, payoffs, total_rewards, game_index, prompt_label, round_num, exp_type)\n",
        "        total_game_contributions += round_total\n",
        "\n",
        "    max_possible = (na - num_dummy_agents) * e * nr\n",
        "    effective_score = total_game_contributions / max_possible\n",
        "    print(f\"\\nEffective Collaboration Score: {effective_score:.2f}\")\n",
        "\n",
        "    csv_writer.writerow([\n",
        "        game_index,\n",
        "        prompt_label,\n",
        "        \"final\",\n",
        "        \"All\",\n",
        "        \"\",\n",
        "        \"\",\n",
        "        \"\",\n",
        "        f\"{effective_score:.2f}\"\n",
        "    ])\n",
        "\n",
        "    return effective_score, total_rewards\n",
        "\n",
        "def run_single_game(game_index: int, prompt_label: str, system_prompt_used: str,\n",
        "                    na: int, nr: int, e: int, m: float, csv_writer, records_file, num_dummy_agents, exp_type) -> float:\n",
        "    \"\"\"\n",
        "    Run a single game (with nr rounds) using the given system prompt and experiment parameters.\n",
        "    Returns the effective collaboration score for the game.\n",
        "    \"\"\"\n",
        "    # Create new agents for this game.    \n",
        "    agents = []\n",
        "    for i in range(na):\n",
        "        if i < num_dummy_agents:\n",
        "            # Create a dummy agent\n",
        "            agent = DummyAgent(f\"Agent_{i+1}\", system_prompt_used, records_file)\n",
        "        else:\n",
        "            # Create a standard LLM-based agent\n",
        "            agent = Agent(f\"Agent_{i+1}\", system_prompt_used, records_file)\n",
        "        agents.append(agent)\n",
        "        \n",
        "    for agent in agents:\n",
        "        agent.story_label = prompt_label\n",
        "    \n",
        "    # Executes all rounds of the game, tracking contributions, payoffs, and collaboration scores.\n",
        "    effective_score, _ = execute_game_rounds(\n",
        "        agents, na, nr, e, m, csv_writer, records_file, game_index, prompt_label, exp_type, num_dummy_agents\n",
        "    )\n",
        "    return effective_score\n",
        "\n",
        "def run_single_game_random_story(game_index: int, system_prompt_story: str, na: int, nr: int, e: int, m: float,\n",
        "                                csv_writer, records_file, story_prompts: dict, exp_type) -> (float, list):\n",
        "    \"\"\"\n",
        "    Run a single game where each agent gets a random story.\n",
        "    Returns:\n",
        "    - effective_score: overall collaboration score (total game contributions divided by maximum possible)\n",
        "    - agent_results: list of tuples (agent_name, story_label, cumulative_reward) for each agent.\n",
        "    \"\"\"\n",
        "    agents = []\n",
        "\n",
        "    # Create agents with random story prompts.\n",
        "    for i in range(na):\n",
        "        chosen_label, chosen_story = random.choice(list(story_prompts.items()))\n",
        "        # Insert the chosen story into the base system prompt.\n",
        "        prompt_text = system_prompt_story.replace(\"STORY\", chosen_story)\n",
        "        agent = Agent(f\"Agent_{i+1}\", prompt_text, records_file)\n",
        "        agent.story_label = chosen_label\n",
        "        agents.append(agent)\n",
        "\n",
        "    # Executes all rounds of the game, tracking contributions, payoffs, and collaboration scores.\n",
        "    effective_score, total_rewards = execute_game_rounds(\n",
        "        agents, na, nr, e, m, csv_writer, records_file, game_index, \"All\", exp_type, 0\n",
        "    )\n",
        "\n",
        "    # Prepare results: (agent_name, story_label, cumulative_reward)\n",
        "    agent_results = [(agents[i].name, agents[i].story_label, total_rewards[i]) for i in range(na)]\n",
        "    return effective_score, agent_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yDnYCe0Vovd"
      },
      "source": [
        "# Configure and Launch Game Simulations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNYW13HkVo-m"
      },
      "outputs": [],
      "source": [
        "CSV_HEADER = [\"Game\", \"PromptType\", \"Round\", \"AgentName\", \"Contribution\", \"RoundPayoff\", \"CumulativePayoff\", \"CollaborationScore\"]\n",
        "\n",
        "def run_same_story_experiment(is_bad_apple, story_index, num_rounds_list, endowment_list, multiplier_list, num_games, num_agents_list, exp_type):\n",
        "    \"\"\"Runs the same story experiment using Story index.\"\"\"\n",
        "    story_files = sorted(glob.glob(\"stories/*.txt\"))\n",
        "    if story_index >= len(story_files):\n",
        "        print(\"Invalid story index. Exiting.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    selected_story_file = story_files[story_index]\n",
        "    story_name = os.path.splitext(os.path.basename(selected_story_file))[0]\n",
        "\n",
        "    for na, nr, e, m in itertools.product(num_agents_list, num_rounds_list, endowment_list, multiplier_list):\n",
        "        exp_name = f\"{'bad_apple' if is_bad_apple else 'same_story'}_{story_name}_ag{na}_ro{nr}_end{e}_mult{m}\"\n",
        "        num_dummy_agents = 1 if is_bad_apple else 0\n",
        "\n",
        "        print(f\"\\n\\n######################\\nRunning experiment: {exp_name}\\n######################\\n\")\n",
        "        intermediate_results, results_path = load_intermediate_results(exp_name, is_dict=True)\n",
        "        \n",
        "        scores_by_prompt = run_same_story_games(\n",
        "            exp_name, selected_story_file, story_name, na, nr, e, m, num_games, num_dummy_agents,\n",
        "            intermediate_results, results_path, exp_type\n",
        "        )\n",
        "        compute_and_print_statistics(scores_by_prompt, exp_name)\n",
        "\n",
        "\n",
        "def run_different_story_experiment(num_rounds_list, endowment_list, multiplier_list, num_games, num_agents_list, exp_type):\n",
        "    for na, nr, e, m in itertools.product(num_agents_list, num_rounds_list, endowment_list, multiplier_list):\n",
        "        exp_name = f\"different_story_ag{na}_ro{nr}_end{e}_mult{m}\"\n",
        "        print(f\"\\n\\n######################\\nRunning experiment: {exp_name}\\n######################\\n\")\n",
        "\n",
        "        story_prompts = load_all_story_prompts()\n",
        "        system_prompt = get_system_prompt(na, e, m)\n",
        "        intermediate_results, results_path = load_intermediate_results(exp_name, is_dict=False)\n",
        "\n",
        "        scores_list, rewards_by_story = run_different_story_games(\n",
        "            exp_name, na, nr, e, m, num_games, story_prompts, system_prompt,\n",
        "            intermediate_results, results_path, exp_type\n",
        "        )\n",
        "\n",
        "        print(f\"\\n=== Rewards by Story for experiment {exp_name} ===\")\n",
        "        compute_and_print_statistics(rewards_by_story, exp_name)\n",
        "        if scores_list:\n",
        "            print(f\"\\nOverall Effective Collaboration Score: Mean = {statistics.mean(scores_list):.2f}, SD = {statistics.stdev(scores_list):.2f}\")\n",
        "\n",
        "\n",
        "def run_same_story_games(exp_name, story_file, story_name, na, nr, e, m, num_games, num_dummy_agents, intermediate_results, results_path, exp_type):\n",
        "    with open(story_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        story_content = f.read()\n",
        "    if story_name not in [\"maxreward\", \"noinstruct\"]:\n",
        "        story_content = \"Your behavior is influenced by the following bedtime story your mother read to you every night: \" + story_content\n",
        "    # Build a dynamic system prompt using the current parameters.\n",
        "    prompt_text = get_system_prompt(na, e, m).replace(\"STORY\", story_content)\n",
        "\n",
        "    if story_name not in intermediate_results:\n",
        "        intermediate_results[story_name] = []\n",
        "\n",
        "    # If intermediate results exist, re-populate the CSV file with final rows from previous games.\n",
        "    scores = intermediate_results[story_name][:]\n",
        "    with open(f\"records_{exp_name}.txt\", \"w\", encoding=\"utf-8\") as records_file, \\\n",
        "         open(f\"game_results_{exp_name}.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as results_file:\n",
        "\n",
        "        writer = csv.writer(results_file)\n",
        "        writer.writerow(CSV_HEADER)\n",
        "\n",
        "        # Re-write completed game rows if reloading\n",
        "        for idx, score in enumerate(scores, start=1):\n",
        "            writer.writerow([idx, story_name, \"final\", \"All\", \"\", \"\", \"\", f\"{score:.2f}\"])\n",
        "        print(f\"\\n=== Running Games with prompt: {story_name} for experiment {exp_name} ===\")\n",
        "        # Determine how many games have already been run for this prompt.\n",
        "            \n",
        "        for game_index in range(len(scores) + 1, num_games + 1):\n",
        "            print(f\"\\n=== Game {game_index} ({story_name}) for experiment {exp_name} ===\")\n",
        "            score = run_single_game(game_index, story_name, prompt_text, na, nr, e, m, writer, records_file, num_dummy_agents, exp_type)\n",
        "            scores.append(score)\n",
        "            intermediate_results[story_name].append(score)\n",
        "            save_intermediate_results(intermediate_results, results_path)\n",
        "\n",
        "    return {story_name: scores}\n",
        "\n",
        "\n",
        "def run_different_story_games(exp_name, na, nr, e, m, num_games, story_prompts, system_prompt, intermediate_results, results_path, exp_type):\n",
        "    scores_list = []\n",
        "    rewards_by_story = {story: [] for story in story_prompts}\n",
        "\n",
        "    with open(f\"records_{exp_name}.txt\", \"w\", encoding=\"utf-8\") as records_file, \\\n",
        "         open(f\"game_results_{exp_name}.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as results_file:\n",
        "\n",
        "        writer = csv.writer(results_file)\n",
        "        writer.writerow(CSV_HEADER)\n",
        "\n",
        "        for game_index in range(1, num_games + 1):\n",
        "            if game_index <= len(intermediate_results):\n",
        "                print(f\"Skipping game {game_index} as it has already been run.\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n=== Game {game_index} for experiment {exp_name} ===\")\n",
        "            score, agent_results = run_single_game_random_story(\n",
        "                game_index, system_prompt, na, nr, e, m, writer, records_file, story_prompts, exp_type\n",
        "            )\n",
        "            intermediate_results.append((game_index, score, agent_results))\n",
        "            save_intermediate_results(intermediate_results, results_path)\n",
        "\n",
        "            scores_list.append(score)\n",
        "            for _, story_label, reward in agent_results:\n",
        "                rewards_by_story[story_label].append(reward)\n",
        "\n",
        "    return scores_list, rewards_by_story\n",
        "\n",
        "\n",
        "def load_all_story_prompts():\n",
        "    story_prompts = {}\n",
        "    for story_file in sorted(glob.glob(\"stories/*.txt\")):\n",
        "        story_name = os.path.splitext(os.path.basename(story_file))[0]\n",
        "        with open(story_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "        if story_name not in [\"maxreward\", \"noinstruct\"]:\n",
        "            content = \"Your behavior is influenced by the following bedtime story your mother read to you every night: \" + content\n",
        "        story_prompts[story_name] = content\n",
        "    return story_prompts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sYsML6xVxsq"
      },
      "source": [
        "# Main Execution Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpQ_1GJ7fN98"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Configurable Run Function\n",
        "# -----------------------------\n",
        "def run_experiment(exp_type, story_index=0):\n",
        "\n",
        "    # -----------------------------\n",
        "    # Experiment Configurations\n",
        "    # -----------------------------\n",
        "    num_rounds_list = [5]\n",
        "    endowment_list = [10]\n",
        "    multiplier_list = [1.5]\n",
        "        \n",
        "    try:       \n",
        "        if exp_type in [\"same_story\", \"bad_apple\"]:\n",
        "            num_games = 100\n",
        "            num_agents_list = [4, 16, 32] if exp_type == \"same_story\" else [16]\n",
        "            run_same_story_experiment(\n",
        "                is_bad_apple=(exp_type == \"bad_apple\"),\n",
        "                story_index=story_index,\n",
        "                num_rounds_list=num_rounds_list,\n",
        "                endowment_list=endowment_list,\n",
        "                multiplier_list=multiplier_list,\n",
        "                num_games=num_games,\n",
        "                num_agents_list=num_agents_list,\n",
        "                exp_type = exp_type\n",
        "            )\n",
        "        elif exp_type == \"different_story\":\n",
        "            num_games = 400\n",
        "            num_agents_list = [4]\n",
        "            run_different_story_experiment(\n",
        "                num_rounds_list, endowment_list, multiplier_list, num_games, num_agents_list, exp_type\n",
        "            )\n",
        "        else:\n",
        "            print(f\"[ERROR] Unknown experiment type: {exp_type}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Experiment '{exp_type}' failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hpQpsN3V_LW"
      },
      "source": [
        "# Homogenous Experiment\n",
        "\n",
        "Runs 100 games per story for agent sizes [4, 16, 32].\n",
        "\n",
        "(a) Cooperation Among Homogeneous Agents\n",
        "To run across the experiment for all stories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikvqrCqLWRH2"
      },
      "outputs": [],
      "source": [
        "# Run all 12 stories for same_story\n",
        "for i in range(12):\n",
        "    print(f\"Running same_story experiment for story {i}\")\n",
        "    run_experiment(\"same_story\", story_index=i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uom47ImfuD_"
      },
      "source": [
        "(b) Robustness Experiment\n",
        "Same as the same story experiment, but introduces one dummy agent who always contributes 0.\n",
        "\n",
        "To run across the experiment for all stories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CmdzIrAfyLB"
      },
      "outputs": [],
      "source": [
        "# Run all 12 stories for bad_apple\n",
        "for i in range(12):\n",
        "    print(f\"Running bad_apple experiment for story {i}\")\n",
        "    run_experiment(\"bad_apple\", story_index=i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxilIE3Cf1t1"
      },
      "source": [
        "# Heterogenous Experiment\n",
        "\n",
        "Assigns a random story to each agent and runs 400 games with 4 agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qj-R-iN6f5cp"
      },
      "outputs": [],
      "source": [
        "# Run different_story experiment once\n",
        "print(\"Running different_story experiment\")\n",
        "run_experiment(\"different_story\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualization\n",
        "\n",
        "\n",
        "The project includes scripts to visualize collaboration and scaling results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_csv_files(pattern):\n",
        "    \"\"\"Loads all CSV files matching a pattern and merges them into a DataFrame.\"\"\"\n",
        "    files = glob.glob(pattern)\n",
        "    if not files:\n",
        "        print(f\"No files found for pattern: {pattern}\")\n",
        "        return pd.DataFrame()\n",
        "    return pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
        "\n",
        "def preprocess_data(df, metric):\n",
        "    \"\"\"\n",
        "    Prepares data by filtering only final round rows and converting columns to numeric types.\n",
        "    Metric can be 'CollaborationScore' or 'CumulativePayoff'.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    if metric == \"CollaborationScore\":\n",
        "        df = df[df[\"Round\"] == \"final\"].copy()\n",
        "    else:  # \"CumulativePayoff\"\n",
        "        df = df[df[\"Round\"] != \"final\"].copy()\n",
        "        df[\"Round\"] = pd.to_numeric(df[\"Round\"], errors=\"coerce\")\n",
        "        df.dropna(subset=[\"Round\"], inplace=True)\n",
        "        df = df.loc[df.groupby([\"Game\", \"AgentName\"])[\"Round\"].idxmax()].copy()\n",
        "\n",
        "    df[metric] = pd.to_numeric(df[metric], errors=\"coerce\")\n",
        "    df.dropna(subset=[metric], inplace=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## File-Pattern Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "AGENT_SIZES = [4, 16, 32]\n",
        "\n",
        "# Define baseline and meaningful story prompts\n",
        "BASELINE_STORIES = [\"noinstruct\", \"nsCarrot\", \"maxreward\", \"nsPlumber\"]\n",
        "MEANINGFUL_STORIES = [\"OldManSons\", \"Odyssey\", \"Soup\", \"Peacemaker\",\"Musketeers\", \"Teamwork\", \"Spoons\", \"Turnip\"]\n",
        "\n",
        "# Define color gradients for baseline (blue) and meaningful stories (pink)\n",
        "BLUE_SHADES = [\"#87CEFA\", \"#4682B4\", \"#4169E1\", \"#27408B\"]  # Light to Dark Blue\n",
        "PINK_SHADES = [\"#FFB3E6\", \"#FF99CC\", \"#FF66B3\", \"#FF4D9E\", \"#F02278\", \"#D81B60\", \"#B83B7D\", \"#B22272\"]  # Light to Dark Pink\n",
        "\n",
        "# Define color dictionary for plot consistency\n",
        "COLOR_DICT = {\n",
        "    # Baseline condition (Shades of Blue)\n",
        "    \"maxreward\": \"#87CEFA\",\n",
        "    \"noinstruct\": \"#4682B4\",\n",
        "    \"nsCarrot\": \"#4169E1\",\n",
        "    \"nsPlumber\": \"#27408B\",\n",
        "    # Meaningful stories (Shades of Purple/Pink)\n",
        "    \"Odyssey\": \"#FFB3E6\",\n",
        "    \"Soup\": \"#FF99CC\",\n",
        "    \"Peacemaker\": \"#FF66B3\",\n",
        "    \"Musketeers\": \"#FF4D9E\",\n",
        "    \"Teamwork\": \"#F02278\",\n",
        "    \"Spoons\": \"#D81B60\",\n",
        "    \"Turnip\": \"#B83B7D\",\n",
        "    \"OldManSons\": \"#B22272\",\n",
        "}\n",
        "\n",
        "VISUALISATION_EXPERIMENTS = {\n",
        "    \"same_story\":       dict(sizes=AGENT_SIZES,pattern=\"game_results_same_story_*_ag{N}_ro5_end10_mult1.5.csv\"),\n",
        "    \"different_story\":  dict(sizes=[4],pattern=\"game_results_different_story_ag{N}_ro5_end10_mult1.5.csv\"),\n",
        "    \"bad_apple\":        dict(sizes=[4],pattern=\"game_results_bad_apple_*_ag{N}_ro5_end10_mult1.5.csv\"),\n",
        "}\n",
        "\n",
        "#For Distribution Analysis Plots\n",
        "CATEGORIES_FOR_VIOLIN = {\n",
        "    f\"{name}_{N}_agents\": exp[\"pattern\"].format(N=N)\n",
        "    for name, exp in VISUALISATION_EXPERIMENTS.items()\n",
        "    for N in exp[\"sizes\"]\n",
        "}\n",
        "\n",
        "#For Scaling Experiment\n",
        "CATEGORY_GROUPS = {\n",
        "    \"temp_0.6\": {\n",
        "        N: VISUALISATION_EXPERIMENTS[\"same_story\"][\"pattern\"].format(N=N)\n",
        "        for N in AGENT_SIZES\n",
        "    }\n",
        "}\n",
        "\n",
        "#For Latex Table\n",
        "TABLE_KEYS = {\n",
        "    \"same_story\":  \"homogeneous\",\n",
        "    \"different_story\":   \"heterogeneous\",\n",
        "    \"bad_apple\":   \"robustness\",\n",
        "}\n",
        "\n",
        "CATEGORIES_FOR_TABLE = {\n",
        "    TABLE_KEYS[name]: [\n",
        "        VISUALISATION_EXPERIMENTS[name][\"pattern\"].format(N=N)\n",
        "        for N in VISUALISATION_EXPERIMENTS[name][\"sizes\"]\n",
        "    ]\n",
        "    for name in VISUALISATION_EXPERIMENTS\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'same_story_4_agents': 'game_results_same_story_*_ag4_ro5_end10_mult1.5.csv',\n",
              " 'same_story_16_agents': 'game_results_same_story_*_ag16_ro5_end10_mult1.5.csv',\n",
              " 'same_story_32_agents': 'game_results_same_story_*_ag32_ro5_end10_mult1.5.csv',\n",
              " 'different_story_4_agents': 'game_results_different_story_ag4_ro5_end10_mult1.5.csv',\n",
              " 'bad_apple_4_agents': 'game_results_bad_apple_*_ag4_ro5_end10_mult1.5.csv'}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CATEGORIES_FOR_VIOLIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'temp_0.6': {4: 'game_results_same_story_*_ag4_ro5_end10_mult1.5.csv',\n",
              "  16: 'game_results_same_story_*_ag16_ro5_end10_mult1.5.csv',\n",
              "  32: 'game_results_same_story_*_ag32_ro5_end10_mult1.5.csv'}}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CATEGORY_GROUPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'homogeneous': ['game_results_same_story_*_ag4_ro5_end10_mult1.5.csv',\n",
              "  'game_results_same_story_*_ag16_ro5_end10_mult1.5.csv',\n",
              "  'game_results_same_story_*_ag32_ro5_end10_mult1.5.csv'],\n",
              " 'heterogeneous': ['game_results_different_story_ag4_ro5_end10_mult1.5.csv'],\n",
              " 'robustness': ['game_results_bad_apple_*_ag4_ro5_end10_mult1.5.csv']}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CATEGORIES_FOR_TABLE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Distribution Analysis Plots\n",
        "\n",
        "Generates violin plots for different experiment types:\n",
        "\n",
        "Collaboration Score for Homogenous and Robustness experiments.\n",
        "Payoff per Agent for Heterogenous experiment.\n",
        "\n",
        "Run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOa8WPkFhYAk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Ensure vectorized rendering\n",
        "mpl.rcParams['savefig.format'] = 'pdf'\n",
        "mpl.rcParams['pdf.fonttype'] = 42\n",
        "mpl.rcParams['ps.fonttype'] = 42\n",
        "\n",
        "def plot_violin(df, metric, title, output_pdf, plot_mean_line=True, show_legend=False):\n",
        "    \"\"\"\n",
        "    Plots a violin plot for CollaborationScore or CumulativePayoff.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        print(f\"No data to visualize for {title}\")\n",
        "        return\n",
        "\n",
        "    # Compute x-axis order based on mean metric values\n",
        "    order = (df.groupby(\"PromptType\")[metric]\n",
        "            .mean()\n",
        "            .sort_values(ascending=True)\n",
        "            .index.tolist())\n",
        "\n",
        "    plt.figure(figsize=(12, 7))\n",
        "\n",
        "    # Define color palette\n",
        "    palette = {cat: COLOR_DICT.get(cat, \"#888888\") for cat in order}\n",
        "\n",
        "    # Violin plot with embedded box plot\n",
        "    ax = sns.violinplot(\n",
        "        data=df,\n",
        "        x=\"PromptType\",\n",
        "        y=metric,\n",
        "        hue=\"PromptType\",\n",
        "        palette=palette,\n",
        "        inner=\"box\",\n",
        "        dodge=False,\n",
        "        order=order,\n",
        "        bw_adjust=5, # Adjusting KDE bandwidth\n",
        "        scale=\"area\", # Uniform width across all violins\n",
        "    )\n",
        "\n",
        "    if ax.get_legend():\n",
        "        ax.get_legend().remove()\n",
        "\n",
        "    # Overlay scatter points\n",
        "    sns.stripplot(\n",
        "        data=df,\n",
        "        x=\"PromptType\",\n",
        "        y=metric,\n",
        "        color=\"black\",\n",
        "        dodge=False,\n",
        "        alpha=0.2,\n",
        "        size=2,\n",
        "        zorder=2,\n",
        "        order=order\n",
        "    )\n",
        "\n",
        "    # (Optional) Plot mean trend line\n",
        "    if plot_mean_line:\n",
        "        means = df.groupby(\"PromptType\")[metric].mean().loc[order]\n",
        "        x_positions = list(range(len(order)))\n",
        "        plt.plot(\n",
        "            x_positions, means.values, marker='o',\n",
        "            color='black', linestyle='-', linewidth=2,\n",
        "            markersize=6, alpha=0.5, label=\"Mean Trend\"\n",
        "        )\n",
        "        if show_legend:\n",
        "            plt.legend([\"Mean Trend\"])\n",
        "\n",
        "    # Customize plot\n",
        "\n",
        "    if metric == \"CollaborationScore\":\n",
        "        plt.ylim(0, 1.3)  # Set range for Collaboration Score\n",
        "    elif metric == \"CumulativePayoff\":\n",
        "        plt.ylim(0, 120) #Set range for Cumulative Payoff\n",
        "    plt.xlabel(\"Story Prompt\", fontsize=18, labelpad=15)\n",
        "\n",
        "    ylabel_text = \"Payoff per Agent\" if metric == \"CumulativePayoff\" else \"Collaboration Score\"\n",
        "\n",
        "    plt.ylabel(f\"{ylabel_text}\", fontsize=18, labelpad=15)\n",
        "    plt.title(title, fontsize=20, weight=\"bold\" , pad=20)\n",
        "    plt.xticks(rotation=90 if len(order) > 5 else 0, fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "\n",
        "    sns.despine()\n",
        "    plt.grid(False)\n",
        "\n",
        "    # Save the plot as Pdf\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_pdf, bbox_inches='tight', format='pdf', transparent=False)  # Fully vectorized PDF\n",
        "    print(f\"Figure saved as {output_pdf}\")\n",
        "\n",
        "for category, pattern in CATEGORIES_FOR_VIOLIN.items():\n",
        "    agent_count = category.split(\"_\")[-2]  # Extract agent count dynamically\n",
        "\n",
        "    if \"different_story\" in category:\n",
        "        # Different story -> Cumulative Payoff\n",
        "        csv_files = glob.glob(pattern)\n",
        "        for csv_file in csv_files:\n",
        "            output_file = csv_file.replace(\"game_results\", \"cumulative_payoffs\").replace(\".csv\", \".jpg\")\n",
        "            df = preprocess_data(pd.read_csv(csv_file), \"CumulativePayoff\")\n",
        "\n",
        "            title = f\"Heterogenous Experiment\"\n",
        "\n",
        "            plot_violin(df, \"CumulativePayoff\", title, output_file)\n",
        "    else:\n",
        "        # Same story & bad apple -> Collaboration Score\n",
        "        df = load_csv_files(pattern)\n",
        "        df = preprocess_data(df, \"CollaborationScore\")\n",
        "        if df is not None:\n",
        "            output_file = f\"{category}_collaboration_scores.pdf\"\n",
        "\n",
        "            if \"bad_apple\" in category:\n",
        "                title = f\"Robustness\"\n",
        "            else:\n",
        "                title = f\"Homogenous Experiment\"\n",
        "\n",
        "            plot_violin(df, \"CollaborationScore\", title, output_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzY4p1R5iFkp"
      },
      "source": [
        "## 2. Scaling Experiment Visualization\n",
        "\n",
        "Plots the mean collaboration score across agent sizes to analyze scaling effects in Homogenous Experiment\n",
        "\n",
        "Run:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0bXJYQniJ1R"
      },
      "outputs": [],
      "source": [
        "def process_category_scaling(temp_label, CATEGORIES):\n",
        "    \"\"\"Processes a single temperature condition and generates a visualization.\"\"\"\n",
        "    story_scores = {}\n",
        "\n",
        "    # Load CSV data and extract mean collaboration scores\n",
        "    for agent_count, pattern in CATEGORIES.items():\n",
        "        \n",
        "        df = load_csv_files(pattern)\n",
        "        if df is None or df.empty:\n",
        "            continue\n",
        "\n",
        "        # Compute mean collaboration scores per story\n",
        "        story_means = df.groupby(\"PromptType\")[\"CollaborationScore\"].mean()\n",
        "        story_scores[agent_count] = story_means.to_dict()  # Store scores\n",
        "\n",
        "    # Extract the order of stories as they appear at N=4 in ascending order\n",
        "    starting_order = sorted(story_scores[4].items(), key=lambda x: x[1])\n",
        "    starting_stories = [story for story, _ in starting_order]\n",
        "\n",
        "    print(f\"Processing {temp_label}: starting_stories = {starting_stories}\")\n",
        "\n",
        "    # Dynamically assign colors based on order in starting_stories\n",
        "    COLOR_DICT = {}\n",
        "    blue_idx, pink_idx = 0, 0  # Track index for blue and pink shades\n",
        "\n",
        "    for story in starting_stories:\n",
        "        if story in [\"noinstruct\", \"nsCarrot\", \"nsPlumber\", \"maxreward\"]:  # Baseline stories\n",
        "            COLOR_DICT[story] = BLUE_SHADES[blue_idx]\n",
        "            blue_idx += 1  # Move to next darker shade\n",
        "        else:  # Meaningful stories\n",
        "            COLOR_DICT[story] = PINK_SHADES[pink_idx]\n",
        "            pink_idx += 1  # Move to next darker shade\n",
        "\n",
        "    # Plot settings\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    legend_handles = []\n",
        "\n",
        "    # Plot each story's progression across agent sizes\n",
        "    for story in COLOR_DICT.keys():  # Only plot stories in COLOR_DICT\n",
        "        positions = []\n",
        "\n",
        "        for agent_count in AGENT_SIZES:\n",
        "            if agent_count in story_scores and story in story_scores[agent_count]:\n",
        "                x_pos = story_scores[agent_count][story]\n",
        "                y_pos = AGENT_SIZES.index(agent_count)\n",
        "                positions.append((x_pos, y_pos))\n",
        "\n",
        "                # Scatter plot for each point\n",
        "                plt.scatter(x_pos, y_pos, s=70, facecolors=\"none\", edgecolors=COLOR_DICT[story], linewidths=1.5, label=story if agent_count == 4 else \"\")\n",
        "\n",
        "        if len(positions) > 1:\n",
        "            x_vals, y_vals = zip(*positions)\n",
        "            plt.plot(x_vals, y_vals, linestyle=\"dashed\", color=COLOR_DICT[story], alpha=0.7)\n",
        "\n",
        "    for story in starting_stories:\n",
        "        legend_handles.append(\n",
        "            mlines.Line2D(\n",
        "                [], [], marker=\"o\", linestyle=\"None\", markersize=8, color=COLOR_DICT.get(story, \"#888888\"), label=story\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Customizing plot\n",
        "    plt.xlabel(\"Mean Collaboration Score\", fontsize=18, labelpad=15)\n",
        "    plt.ylabel(\"Agent Size\", fontsize=18, labelpad=15)\n",
        "    plt.yticks(range(len(AGENT_SIZES)), [f\"N = {n}\" for n in AGENT_SIZES], fontsize=14, weight=\"bold\")\n",
        "    plt.title(f\"Scaling Experiment\", fontsize=20, weight=\"bold\", pad=20)\n",
        "    plt.grid(axis=\"y\", linestyle=\"dotted\")\n",
        "\n",
        "    plt.legend(\n",
        "        handles=legend_handles, title=\"Story\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=12\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save as Pdf\n",
        "    filename_base = f\"scaling_experiment_collab_score_{temp_label}\"\n",
        "    plt.savefig(f\"{filename_base}.pdf\", bbox_inches=\"tight\", format=\"pdf\")\n",
        "\n",
        "    print(f\"Scaling experiment figures saved as {filename_base}.pdf\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Run the process for each category\n",
        "for temp_label, category_dict in CATEGORY_GROUPS.items():\n",
        "    process_category_scaling(temp_label, category_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Bootstrapped Pairwise Differences Visualization\n",
        "\n",
        "Generates bootstrap-resampled distributions of pairwise differences in collaboration scores (and payoffs) between experiment conditions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fixed pair ordering for same_story (homogeneous) and same_story_bad_apple (robustness)\n",
        "fixed_pairs_regular = [\n",
        "    (\"Turnip\", \"OldManSons\"),\n",
        "    (\"Spoons\", \"OldManSons\"),\n",
        "    (\"Spoons\", \"Turnip\"),\n",
        "    (\"Teamwork\", \"OldManSons\"),\n",
        "    (\"Teamwork\", \"Turnip\"),\n",
        "    (\"Teamwork\", \"Spoons\"),\n",
        "    (\"Musketeers\", \"OldManSons\"),\n",
        "    (\"Musketeers\", \"Turnip\"),\n",
        "    (\"Musketeers\", \"Spoons\"),\n",
        "    (\"Musketeers\", \"Teamwork\"),\n",
        "    (\"Peacemaker\", \"OldManSons\"),\n",
        "    (\"Peacemaker\", \"Turnip\"),\n",
        "    (\"Peacemaker\", \"Spoons\"),\n",
        "    (\"Peacemaker\", \"Teamwork\"),\n",
        "    (\"Peacemaker\", \"Musketeers\"),\n",
        "    (\"Soup\", \"OldManSons\"),\n",
        "    (\"Soup\", \"Turnip\"),\n",
        "    (\"Soup\", \"Spoons\"),\n",
        "    (\"Soup\", \"Teamwork\"),\n",
        "    (\"Soup\", \"Musketeers\"),\n",
        "    (\"Soup\", \"Peacemaker\"),\n",
        "    (\"nsPlumber\", \"OldManSons\"),\n",
        "    (\"nsPlumber\", \"Turnip\"),\n",
        "    (\"nsPlumber\", \"Spoons\"),\n",
        "    (\"nsPlumber\", \"Teamwork\"),\n",
        "    (\"nsPlumber\", \"Musketeers\"),\n",
        "    (\"nsPlumber\", \"Peacemaker\"),\n",
        "    (\"nsPlumber\", \"Soup\"),\n",
        "    (\"Odyssey\", \"OldManSons\"),\n",
        "    (\"Odyssey\", \"Turnip\"),\n",
        "    (\"Odyssey\", \"Spoons\"),\n",
        "    (\"Odyssey\", \"Teamwork\"),\n",
        "    (\"Odyssey\", \"Musketeers\"),\n",
        "    (\"Odyssey\", \"Peacemaker\"),\n",
        "    (\"Odyssey\", \"Soup\"),\n",
        "    (\"Odyssey\", \"nsPlumber\"),\n",
        "    (\"nsCarrot\", \"OldManSons\"),\n",
        "    (\"nsCarrot\", \"Turnip\"),\n",
        "    (\"nsCarrot\", \"Spoons\"),\n",
        "    (\"nsCarrot\", \"Teamwork\"),\n",
        "    (\"nsCarrot\", \"Musketeers\"),\n",
        "    (\"nsCarrot\", \"Peacemaker\"),\n",
        "    (\"nsCarrot\", \"Soup\"),\n",
        "    (\"nsCarrot\", \"nsPlumber\"),\n",
        "    (\"nsCarrot\", \"Odyssey\"),\n",
        "    (\"noinstruct\", \"OldManSons\"),\n",
        "    (\"noinstruct\", \"Turnip\"),\n",
        "    (\"noinstruct\", \"Spoons\"),\n",
        "    (\"noinstruct\", \"Teamwork\"),\n",
        "    (\"noinstruct\", \"Musketeers\"),\n",
        "    (\"noinstruct\", \"Peacemaker\"),\n",
        "    (\"noinstruct\", \"Soup\"),\n",
        "    (\"noinstruct\", \"nsPlumber\"),\n",
        "    (\"noinstruct\", \"Odyssey\"),\n",
        "    (\"noinstruct\", \"nsCarrot\"),\n",
        "    (\"maxreward\", \"OldManSons\"),\n",
        "    (\"maxreward\", \"Turnip\"),\n",
        "    (\"maxreward\", \"Spoons\"),\n",
        "    (\"maxreward\", \"Teamwork\"),\n",
        "    (\"maxreward\", \"Musketeers\"),\n",
        "    (\"maxreward\", \"Peacemaker\"),\n",
        "    (\"maxreward\", \"Soup\"),\n",
        "    (\"maxreward\", \"nsPlumber\"),\n",
        "    (\"maxreward\", \"Odyssey\"),\n",
        "    (\"maxreward\", \"nsCarrot\"),\n",
        "    (\"maxreward\", \"noinstruct\")\n",
        "]\n",
        "fixed_pairs_temp = fixed_pairs_regular.copy()\n",
        " \n",
        "def analyze_data(data,\n",
        "                 csv_filename=\"pairwise_confidence_intervals.csv\",\n",
        "                 ci_plot_filename=\"pairwise_CI_plot.png\",\n",
        "                 n_bootstrap=1000,\n",
        "                 subtitle=\"\",\n",
        "                 fixed_pairs=None):\n",
        "    \"\"\"\n",
        "    Computes bootstrapped 95% confidence intervals for pairwise differences,\n",
        "    saves results to CSV, and creates errorbar plots.\n",
        "    \"\"\"\n",
        "    # Bootstrap helper: resample and compute diff of means\n",
        "    def bootstrap_diff(data1, data2):\n",
        "        diffs = []\n",
        "        for _ in range(n_bootstrap):\n",
        "            s1 = np.random.choice(data1, size=len(data1), replace=True)\n",
        "            s2 = np.random.choice(data2, size=len(data2), replace=True)\n",
        "            diffs.append(np.mean(s2) - np.mean(s1))\n",
        "        return np.percentile(diffs, 2.5), np.percentile(diffs, 97.5)\n",
        " \n",
        "    # Determine which pairs to compare\n",
        "    if fixed_pairs is None:\n",
        "        # heterogeneous: sort categories by their sample mean\n",
        "        means = {cat: np.mean(vals) for cat, vals in data.items()}\n",
        "        cats = sorted(means, key=means.get)\n",
        "        pair_comparisons = [(cats[i], cats[j]) for i in range(len(cats)) for j in range(i+1, len(cats))]\n",
        "    else:\n",
        "        pair_comparisons = fixed_pairs\n",
        " \n",
        "    # Compute CIs\n",
        "    results = []\n",
        "    for a, b in pair_comparisons:\n",
        "        if a not in data or b not in data:\n",
        "            # missing data => NaNs\n",
        "            results.append({\"Category1\": a, \"Category2\": b, \"Lower_bound\": np.nan, \"Upper_bound\": np.nan})\n",
        "            print(f\"Warning: {a} or {b} not found; CI set to NaN.\")\n",
        "        else:\n",
        "            # ensure diff is always mean(b) - mean(a)\n",
        "            if np.mean(data[a]) < np.mean(data[b]):\n",
        "                low, up = bootstrap_diff(data[a], data[b])\n",
        "            else:\n",
        "                low, up = bootstrap_diff(data[b], data[a])\n",
        "            results.append({\"Category1\": a, \"Category2\": b, \"Lower_bound\": low, \"Upper_bound\": up})\n",
        " \n",
        "    # For different_story, drop any \"All vs ...\" comparisons\n",
        "    if \"different story\" in subtitle.lower():\n",
        "        results = [r for r in results if \"All\" not in r[\"Category1\"] and \"All\" not in r[\"Category2\"]]\n",
        " \n",
        "    # Save filtered results\n",
        "    pd.DataFrame(results).to_csv(csv_filename, index=False)\n",
        "    print(f\"Pairwise confidence intervals saved to '{csv_filename}'.\")\n",
        " \n",
        "    # Plot error bars\n",
        "    fig, ax = plt.subplots(figsize=(8, 10))\n",
        "    ax.axvline(0, color='red', linestyle='--')  \n",
        " \n",
        "    y = np.arange(len(results))\n",
        "    for idx, r in enumerate(results):\n",
        "        l, u = r[\"Lower_bound\"], r[\"Upper_bound\"]\n",
        "        if np.isnan(l):\n",
        "            ax.plot(0, idx, 'o', color='lightgray')\n",
        "        else:\n",
        "            center = (l + u) / 2\n",
        "            err_low, err_high = center - l, u - center\n",
        "            crosses = (l < 0 < u) or (l == 0)\n",
        "            mcol = 'red' if crosses else 'black'\n",
        "            ecol = 'red' if crosses else 'gray'\n",
        "            ax.errorbar(center, idx, xerr=[[err_low], [err_high]],\n",
        "                        fmt='o', color=mcol, ecolor=ecol, capsize=3)\n",
        " \n",
        "    labels = [f\"{r['Category1']} vs {r['Category2']}\" for r in results]\n",
        "    ax.set_yticks(y)\n",
        "    ax.set_yticklabels(labels)\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_xlabel(\"Difference in Means (95% CI)\")\n",
        "    ax.set_title(subtitle)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(ci_plot_filename)\n",
        "    plt.savefig(ci_plot_filename.replace(\".png\", \".pdf\"))\n",
        "    plt.show()\n",
        " \n",
        " \n",
        "def analyze_collaboration_scores_all(csv_files: list):\n",
        "    \"\"\"\n",
        "    1. Load and concatenate CSV files.\n",
        "    2. Filter to 'final' for same_story/bad_apple, keep all rows for different_story.\n",
        "    3. Build data dict by PromptType.\n",
        "    4. Derive subtitle and bundle_key from filenames.\n",
        "    5. Select fixed_pairs (None for different_story).\n",
        "    6. Call analyze_data().\n",
        "    \"\"\"\n",
        "    # Load and combine\n",
        "    dfs = [pd.read_csv(f) for f in csv_files]\n",
        "    df = pd.concat(dfs, ignore_index=True)\n",
        " \n",
        "    # Normalize Round\n",
        "    if \"Round\" in df.columns:\n",
        "        df[\"Round\"] = df[\"Round\"].astype(str).str.lower().str.strip()\n",
        " \n",
        "    first = os.path.basename(csv_files[0]).lower()\n",
        "    if \"different_story\" in first:\n",
        "        score_col, used = \"CumulativePayoff\", df\n",
        "    else:\n",
        "        score_col = \"CollaborationScore\"\n",
        "        used = df[df[\"Round\"] == \"final\"].copy()\n",
        "        used[score_col] = pd.to_numeric(used[score_col], errors=\"coerce\")\n",
        "        used.dropna(subset=[score_col], inplace=True)\n",
        " \n",
        "    # Build data dictionary\n",
        "    data_dict = {pt: grp[score_col].tolist() for pt, grp in used.groupby(\"PromptType\")}\n",
        "    print(\"Data dictionary (PromptType: count):\")\n",
        "    for k, v in data_dict.items():\n",
        "        print(f\"{k}: {len(v)}\")\n",
        " \n",
        "    # Subtitle logic\n",
        "    if \"different_story\" in first:\n",
        "        subtitle = \"Different Story 4 Agents\"\n",
        "    elif \"bad_apple\" in first:\n",
        "        subtitle = \"Same Story Robust 4 Agents\"\n",
        "    elif \"same_story\" in first:\n",
        "        if \"ag4\" in first:\n",
        "            subtitle = \"Same Story 4 Agents\"\n",
        "        elif \"ag16\" in first:\n",
        "            subtitle = \"Same Story 16 Agents\"\n",
        "        elif \"ag32\" in first:\n",
        "            subtitle = \"Same Story 32 Agents\"\n",
        "        else:\n",
        "            subtitle = \"Same Story\"\n",
        "    else:\n",
        "        subtitle = first\n",
        " \n",
        "    # Bundle key for filenames\n",
        "    if first.startswith(\"game_results_\"):\n",
        "        suff = first[len(\"game_results_\"):-4]\n",
        "    else:\n",
        "        suff = first[:-4]\n",
        "    parts = suff.split('_')\n",
        "    if len(parts) >= 3 and parts[0] in [\"same\", \"bad\"]:\n",
        "        bundle_key = \"_\".join(parts[:2] + parts[3:])\n",
        "    else:\n",
        "        bundle_key = suff\n",
        " \n",
        "    # Choose fixed_pairs\n",
        "    if \"different_story\" in first:\n",
        "        fixed_pairs = None\n",
        "    elif \"Spoons\" in data_dict:\n",
        "        fixed_pairs = fixed_pairs_regular\n",
        "    elif \"temp0.6\" in first:\n",
        "        fixed_pairs = fixed_pairs_temp\n",
        "    else:\n",
        "        fixed_pairs = fixed_pairs_regular\n",
        " \n",
        "    # Output filenames\n",
        "    csv_out = f\"combined_pairwise_confidence_intervals_{bundle_key}.csv\"\n",
        "    ci_out = f\"combined_pairwise_CI_plot_{bundle_key}.png\"\n",
        " \n",
        "    analyze_data(data_dict,\n",
        "                 csv_filename=csv_out,\n",
        "                 ci_plot_filename=ci_out,\n",
        "                 n_bootstrap=1000,\n",
        "                 subtitle=subtitle,\n",
        "                 fixed_pairs=fixed_pairs)\n",
        " \n",
        " \n",
        "if __name__ == \"__main__\":\n",
        "    # Find and group game_results bundles\n",
        "    files = glob.glob(\"game_results_*.csv\")\n",
        "    if not files:\n",
        "        print(\"No game_results CSV files found.\")\n",
        "    else:\n",
        "        bundles = {}\n",
        "        for fpath in files:\n",
        "            base = os.path.basename(fpath)\n",
        "            if base.startswith(\"game_results_\"):\n",
        "                key_suff = base[len(\"game_results_\"):-4]\n",
        "            else:\n",
        "                key_suff = base[:-4]\n",
        "            toks = key_suff.split('_')\n",
        "            if len(toks) >= 3 and toks[0] in [\"same\", \"bad\"]:\n",
        "                key = \"_\".join(toks[:2] + toks[3:])\n",
        "            else:\n",
        "                key = key_suff\n",
        "            bundles.setdefault(key, []).append(fpath)\n",
        " \n",
        "        for key, group in bundles.items():\n",
        "            print(f\"\\nProcessing bundle: {key} ({len(group)} files)\")\n",
        "            analyze_collaboration_scores_all(group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Summary Statistics Table\n",
        "\n",
        "Generates a LaTeX-formatted table of mean  std for final Collaboration Scores and Cumulative Payoffs across all story prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def truncate_float(val, decimals=2):\n",
        "    factor = 10 ** decimals\n",
        "    return int(val * factor) / factor\n",
        "\n",
        "# Extract agent size from filename pattern\n",
        "def extract_agent_size(pattern):\n",
        "    match = re.search(r'_ag(\\d+)_', pattern)\n",
        "    return match.group(1) if match else \"Unknown\"\n",
        "\n",
        "# Dictionary to store collaboration scores \n",
        "COLLAB_SCORES = {story: {\"Homogeneous_4\": None, \"Homogeneous_16\": None, \"Homogeneous_32\": None,\n",
        "                        \"Robustness_4\": None, \"Heterogeneous_4\": None} for story in BASELINE_STORIES + MEANINGFUL_STORIES}\n",
        "\n",
        "# Process experiment data and extract Mean  SD for each story\n",
        "for category, patterns in CATEGORIES_FOR_TABLE.items():\n",
        "    for pattern in patterns:\n",
        "        \n",
        "        df = load_csv_files(pattern)\n",
        "        if df is None or df.empty:\n",
        "            continue\n",
        "        \n",
        "        stats = df.groupby(\"PromptType\")[\"CumulativePayoff\"].agg([\"mean\", \"std\"]).reset_index()       \n",
        "        agent_size = extract_agent_size(pattern)\n",
        "        \n",
        "        if category in (\"homogeneous\", \"robustness\"):\n",
        "            metric = \"CollaborationScore\"\n",
        "        else:\n",
        "            metric = \"CumulativePayoff\"\n",
        "\n",
        "        df_proc = preprocess_data(df, metric)\n",
        "        if df_proc is None or df_proc.empty:\n",
        "            continue\n",
        "\n",
        "        stats = df_proc.groupby(\"PromptType\")[metric].agg(mean=\"mean\", std=\"std\").reset_index()\n",
        "        \n",
        "        # Map results to correct table column\n",
        "        column_key = f\"{category.capitalize()}_{agent_size}\"\n",
        "        \n",
        "        for _, row in stats.iterrows():\n",
        "            story = row[\"PromptType\"]           \n",
        "            if story in COLLAB_SCORES:\n",
        "                mean_val = row['mean']\n",
        "                std_val = row['std']\n",
        "                # Format dynamically: more decimals for small std\n",
        "                if std_val < 0.01:\n",
        "                    formatted = f\"{mean_val:.4f}  {std_val:.4f}\"\n",
        "                else:\n",
        "                    formatted = f\"{mean_val:.2f}  {std_val:.2f}\"\n",
        "                COLLAB_SCORES[story][column_key] = formatted\n",
        "\n",
        "\n",
        "latex_output = \"\"\"\\\\begin{table*}[t]\n",
        "    \\\\centering\n",
        "    \\\\caption{Mean  standard deviation of final Collaboration Scores (for homogeneous and robustness agents) and final Cumulative Payoffs (for heterogeneous agents) across all story prompts. Values are shown with higher decimal precision where variation is small, to reflect statistically meaningful differences observed in pairwise confidence intervals.}\n",
        "    \\\\setlength{\\\\tabcolsep}{8pt} \n",
        "    \\\\renewcommand{\\\\arraystretch}{1.3} \n",
        "    \\\\fontsize{11pt}{13pt}\\\\selectfont \n",
        "    \\\\resizebox{\\\\textwidth}{!}{\n",
        "    \\\\begin{tabular}{llccccc}\n",
        "        \\\\toprule\n",
        "        \\\\multirow{2}{*}{\\\\textbf{Story Type}} & \\\\multirow{2}{*}{\\\\textbf{Story Prompt}} & \\\\multicolumn{3}{c}{\\\\textbf{Homogeneous Agents}} & \\\\textbf{Robustness} & \\\\textbf{Heterogeneous} \\\\\\\\\n",
        "        \\\\cmidrule(lr){3-5} \\\\cmidrule(lr){6-6} \\\\cmidrule(lr){7-7}\n",
        "        & & \\\\textbf{N=4} & \\\\textbf{N=16} & \\\\textbf{N=32} & \\\\textbf{N=4} & \\\\textbf{N=4} \\\\\\\\\n",
        "        \\\\midrule\n",
        "\"\"\"\n",
        "\n",
        "# Add Baseline Stories Section\n",
        "latex_output += \"        \\\\multirow{4}{*}{\\\\centering \\\\textbf{Baseline Stories}}  \\n\"\n",
        "for i, story in enumerate(BASELINE_STORIES):\n",
        "    row_data = [COLLAB_SCORES[story].get(col, \"N/A\") for col in [\"Homogeneous_4\", \"Homogeneous_16\", \"Homogeneous_32\", \"Robustness_4\", \"Heterogeneous_4\"]]\n",
        "    prefix = \"        & \"\n",
        "    latex_output += f\"{prefix}{story}  & {' & '.join(row_data)} \\\\\\\\\\n\"\n",
        "\n",
        "latex_output += \"        \\\\midrule\\n\"\n",
        "\n",
        "# Add Meaningful Stories Section\n",
        "latex_output += \"        \\\\multirow{8}{*}{\\\\centering \\\\textbf{Meaningful Stories}}  \\n\"\n",
        "for i, story in enumerate(MEANINGFUL_STORIES):\n",
        "    row_data = [COLLAB_SCORES[story].get(col, \"N/A\") for col in [\"Homogeneous_4\", \"Homogeneous_16\", \"Homogeneous_32\", \"Robustness_4\", \"Heterogeneous_4\"]]\n",
        "    prefix = \"        & \"\n",
        "    latex_output += f\"{prefix}{story}  & {' & '.join(row_data)} \\\\\\\\\\n\"\n",
        "\n",
        "latex_output += \"\"\"        \\\\bottomrule\n",
        "    \\\\end{tabular}\n",
        "    }\n",
        "    \\\\label{tab:all_agents_scores}\n",
        "\\\\end{table*}\n",
        "\"\"\"\n",
        "print(latex_output)\n",
        "# Save to file\n",
        "with open(\"all_agents_table.tex\", \"w\") as f:\n",
        "    f.write(latex_output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm_deliberation",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
