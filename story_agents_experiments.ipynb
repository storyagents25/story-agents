{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mL_AfgWIS3Ig"
   },
   "source": [
    "# Install Dependencies\n",
    "Install all required Python packages for LLM agents, data processing, visualization, and network analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "keJlQJUNMSMH"
   },
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "%pip install -U langchain-community langchain_openai numpy pandas matplotlib seaborn nbformat ipykernel networkx imageio pillow scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmrhZojGS6f3"
   },
   "source": [
    "# Import Libraries\n",
    "Import core libraries: standard Python modules, data science tools (pandas, matplotlib), LangChain for LLM interaction, and NetworkX for multi-pool topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "am7ou16QS8OJ"
   },
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "import pickle\n",
    "import statistics\n",
    "import glob\n",
    "import itertools\n",
    "import logging\n",
    "import io\n",
    "\n",
    "# Third-party libraries\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from typing import List, Dict, Any, TextIO, Tuple, Set\n",
    "from PIL import Image\n",
    "\n",
    "# Project-specific packages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage, BaseMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tym1zdzvS90z"
   },
   "source": [
    "# Configure LLM Backend\n",
    "Set up configuration for different LLM providers (OpenAI, Llama) and create the Llama API wrapper class with proper authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-T1fxXB_S_P4"
   },
   "outputs": [],
   "source": [
    "# Central Configuration\n",
    "\n",
    "config: Dict[str, Any] = {\n",
    "    \"llm_choice\": os.getenv(\"LLM_CHOICE\", \"llama\"),\n",
    "    \"llama\": {\n",
    "        \"model_name\": os.getenv(\"LLAMA_MODEL_NAME\", \"meta-llama-3.3-70b-instruct-fp8\"),\n",
    "        \"temperature\": float(os.getenv(\"LLAMA_TEMP\", 0.6)),\n",
    "        \"max_tokens\": int(os.getenv(\"LLAMA_MAX_TOKENS\", 500)),\n",
    "        \"api_url\": os.getenv(\"LLAMA_API_URL\"),\n",
    "        \"api_key\": os.getenv(\"LLAMA_API_KEY\")\n",
    "    },\n",
    "    \"openai\": {\n",
    "        \"model_name\": os.getenv(\"OPENAI_MODEL_NAME\", \"gpt-4o-mini\"),\n",
    "        \"temperature\": float(os.getenv(\"OPENAI_TEMP\", 1.0)),\n",
    "        \"max_tokens\": int(os.getenv(\"OPENAI_MAX_TOKENS\", 10)),\n",
    "        \"api_key\": os.getenv(\"OPENAI_API_KEY\")\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-A-owFqTA1p"
   },
   "source": [
    "# Define Agent Classes\n",
    "Create LLM-powered Agent and DummyAgent (always contributes 0) classes.\n",
    "\n",
    "Agents make contribution decisions, maintain conversation history, and log all interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNF9JBQZTCT7"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Llama Helper Class\n",
    "# -----------------------------\n",
    "class Llama:\n",
    "    def __init__(self, model_name: str, temperature: float, max_tokens: int, api_url: str, api_key: str = None) -> None:\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.api_url = api_url\n",
    "        self.api_key = api_key or os.environ.get(\"LLAMA_API_KEY\")\n",
    "\n",
    "    def invoke(self, history: List[BaseMessage]) -> AIMessage:\n",
    "        # Build messages, mapping LangChain roles to OpenAI roles\n",
    "        messages: List[Dict[str, str]] = []\n",
    "        for msg in history:\n",
    "            if hasattr(msg, \"content\"):\n",
    "                content = msg.content\n",
    "            else:\n",
    "                content = str(msg)\n",
    "\n",
    "            # Normalize role\n",
    "            if isinstance(msg, SystemMessage):\n",
    "                role = \"system\"\n",
    "            elif isinstance(msg, HumanMessage):\n",
    "                role = \"user\"\n",
    "            elif isinstance(msg, AIMessage):\n",
    "                role = \"assistant\"\n",
    "            else:\n",
    "                raw = getattr(msg, \"role\", None)\n",
    "                role = raw if raw in (\"system\", \"user\", \"assistant\") else \"user\"\n",
    "\n",
    "            messages.append({\"role\": role, \"content\": content})\n",
    "\n",
    "        data: Dict[str, Any] = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"temperature\": self.temperature\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\"\n",
    "        } if self.api_key else {}\n",
    "\n",
    "        try:\n",
    "            response = requests.post(self.api_url, json=data, headers=headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.Timeout:\n",
    "            raise RuntimeError(\"Llama API request timed out.\")\n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            raise RuntimeError(f\"Llama API HTTP error: {http_err}\")\n",
    "        except requests.exceptions.RequestException as err:\n",
    "            raise RuntimeError(f\"Llama API connection error: {err}\")\n",
    "\n",
    "        try:\n",
    "            response_data = response.json()\n",
    "            content = response_data[\"choices\"][0][\"message\"][\"content\"]\n",
    "        except (ValueError, KeyError, IndexError) as parse_err:\n",
    "            raise RuntimeError(f\"Failed to parse Llama API response: {parse_err}\")\n",
    "\n",
    "        return AIMessage(content=content)\n",
    "\n",
    "def log_records(message: str, records_file: TextIO) -> None:\n",
    "    \"\"\"\n",
    "    Helper function to log a message to records file so we can keep track of all history.\n",
    "    \"\"\"\n",
    "    records_file.write(message + \"\\n\")\n",
    "    records_file.flush() # make sure it's written immediately\n",
    "\n",
    "# Payoff calculation to handle multiple pools\n",
    "def calculate_payoffs(contributions: List[float], e: float, m: float, na: int) -> List[float]:\n",
    "    \"\"\"\n",
    "    Given a list of contributions from each agent, compute the payoff for each agent. For single pool case (backwards compatibility).\n",
    "    \"\"\"\n",
    "    total: float = sum(contributions)\n",
    "    shared_bonus: float = m * total / na\n",
    "    return [e - c + shared_bonus for c in contributions]\n",
    "\n",
    "def calculate_payoff_for_pool(contribs: List[float], m: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate payoff bonus from a single pool for multi-pool games.\n",
    "    \"\"\"\n",
    "    total = sum(contribs)\n",
    "    return m * total / len(contribs) if contribs else 0.0\n",
    "\n",
    "# -----------------------------\n",
    "# Agent Class\n",
    "# -----------------------------\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Agent class for Public Goods Game participation. Uses configurable LLM backend (OpenAI or Llama). Maintains conversation history across rounds.\n",
    "    Logs all interactions for experiment analysis. Responds to contribution prompts with <TOKEN>amount</TOKEN> format.\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, system_message: str, records_file: TextIO) -> None:\n",
    "        self.name = name\n",
    "        # Start the conversation with a system message.\n",
    "        self.history: List[BaseMessage] = [SystemMessage(content=system_message)]\n",
    "        self.records_file = records_file\n",
    "\n",
    "        # Log the creation of this agent and its system prompt to records.\n",
    "        log_records(f\"CREATING AGENT: {self.name}\", records_file)\n",
    "        log_records(f\"System Prompt for {self.name}: {system_message}\", records_file)\n",
    "\n",
    "        # Create the appropriate LLM instance\n",
    "        llm_choice = config[\"llm_choice\"]\n",
    "        if llm_choice == \"llama\":\n",
    "            llama_cfg = config[\"llama\"]\n",
    "            self.llm = Llama(\n",
    "                model_name=llama_cfg[\"model_name\"],\n",
    "                temperature=llama_cfg[\"temperature\"],\n",
    "                max_tokens=llama_cfg[\"max_tokens\"],\n",
    "                api_url=llama_cfg[\"api_url\"],\n",
    "                api_key=llama_cfg[\"api_key\"]\n",
    "            )\n",
    "        else:\n",
    "            openai_cfg = config[\"openai\"]\n",
    "            self.llm = ChatOpenAI(\n",
    "                model_name=openai_cfg[\"model_name\"],\n",
    "                temperature=openai_cfg[\"temperature\"],\n",
    "                max_tokens=openai_cfg[\"max_tokens\"],\n",
    "                openai_api_key=openai_cfg[\"api_key\"]\n",
    "            )\n",
    "\n",
    "    def chat(self, message: str) -> str:\n",
    "        \"\"\"\n",
    "        Append a human message, call the LLM, append the assistant's reply,\n",
    "        log everything, and return the response content.\n",
    "        \"\"\"\n",
    "        log_records(f\"{self.name} receives HUMAN message: {message}\", self.records_file)\n",
    "        self.history.append(HumanMessage(content=message))\n",
    "        try:\n",
    "            response = self.llm.invoke(self.history)\n",
    "        except RuntimeError as err:\n",
    "            log_records(f\"{self.name} ERROR: {err}\", self.records_file)\n",
    "            return f\"[ERROR]: {err}\"\n",
    "\n",
    "        # Store the response in the conversation history\n",
    "        self.history.append(response)\n",
    "        # Brief pause to help avoid rate limits\n",
    "        log_records(f\"{self.name} responds ASSISTANT: {response.content}\", self.records_file)\n",
    "        time.sleep(0.01)\n",
    "        return response.content\n",
    "\n",
    "# -----------------------------\n",
    "# DummyAgent Class\n",
    "# -----------------------------\n",
    "class DummyAgent:\n",
    "    \"\"\"\n",
    "    DummyAgent for robustness testing. Always contributes 0 tokens (simulates free-rider).\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, system_message: str, records_file: TextIO) -> None:\n",
    "        self.name = name\n",
    "        self.history: List[BaseMessage] = [SystemMessage(content=system_message)]\n",
    "        self.records_file = records_file\n",
    "\n",
    "        log_records(f\"CREATING DUMMY AGENT: {self.name}\", records_file)\n",
    "        log_records(f\"(Dummy) System Prompt for {self.name}: {system_message}\", records_file)\n",
    "\n",
    "    def chat(self, message: str) -> str:\n",
    "        \"\"\"\n",
    "        This agent always contributes \"0\".\n",
    "        We still log the conversation but do not call any LLM.\n",
    "        \"\"\"\n",
    "        log_records(f\"{self.name} (dummy) receives HUMAN message: {message}\", self.records_file)\n",
    "        response_content = \"<TOKEN>0</TOKEN>\"\n",
    "        log_records(f\"{self.name} (dummy) responds ASSISTANT: {response_content}\", self.records_file)\n",
    "        time.sleep(0.01)\n",
    "        return response_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hijbwNZxTD2I"
   },
   "source": [
    "# Network and Pool Utilities\n",
    "Functions to build pool structures for single-pool (global only) and multi-pool (global + smaller pools) game configurations.\n",
    "\n",
    "Building pool topology enables studying cooperation across different network structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iX2f74XgTFoF"
   },
   "outputs": [],
   "source": [
    "def build_pools(num_agents: int, pool_sizes: List[int] = None) -> Dict[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Build pool structure. Single-pool: one global pool containing all agents. Multi-pool: global pool + local pools (subsets of agents).\n",
    "    For 4 agents: 3 total pools, each agent belongs to exactly 2 pools (global + one local). Enables studying cooperation in complex network structures.\n",
    "    \"\"\"\n",
    "    pools = {\"global\": list(range(num_agents))}\n",
    "\n",
    "    if pool_sizes:  # Multi-pool case\n",
    "        # Only support pool_sizes=[2] for simplicity\n",
    "        if pool_sizes != [2]:\n",
    "            raise ValueError(\"Only pool_sizes=[2] is supported in simplified version\")\n",
    "\n",
    "        # For 4 agents, create 2 non-overlapping pairs\n",
    "        if num_agents != 4:\n",
    "            raise ValueError(\"Multi-pool configuration currently only supports 4 agents\")\n",
    "\n",
    "        indices = list(range(num_agents))\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        # Create 2 non-overlapping pairs (each agent in exactly one pair)\n",
    "        for i in range(0, num_agents, 2):\n",
    "            if i + 1 < num_agents:\n",
    "                pair = [indices[i], indices[i + 1]]\n",
    "                pool_id = f\"group_2_{i//2}\"\n",
    "                pools[pool_id] = pair\n",
    "\n",
    "    return pools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_kMFHblTHej"
   },
   "source": [
    "# Agent Utility Functions\n",
    "Helper functions for experiment management: loading/saving checkpoints, generating system prompts, extracting and parsing contributions, and computing statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bXsk_Np2TJKD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def get_experiment_directory(exp_type, story_name, na, nr, e, m, pool_sizes=None):\n",
    "    \"\"\"Generate standardized experiment directory path. Strucrure: experiments/{topology}/{exp_type}/ whre topology = \"single_pool\" or \"multi_pool\" \"\"\"\n",
    "    # Determine pool topology\n",
    "    if pool_sizes:\n",
    "        topology = \"multi_pool\"\n",
    "    else:\n",
    "        topology = \"single_pool\"\n",
    "\n",
    "    # Build directory path: experiments/{topology}/{exp_type}/\n",
    "    exp_dir = Path(\"experiments\") / topology / exp_type\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return exp_dir\n",
    "\n",
    "def get_visualization_directory(exp_type, pool_sizes=None):\n",
    "    \"\"\"Generate visualization directory path\"\"\"\n",
    "    # Determine pool topology\n",
    "    if pool_sizes:\n",
    "        topology = \"multi_pool\"\n",
    "    else:\n",
    "        topology = \"single_pool\"\n",
    "\n",
    "    # Build directory path: experiments/vis/{topology}/{exp_type}/\n",
    "    vis_dir = Path(\"experiments\") / \"vis\" / topology / exp_type\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    vis_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return vis_dir\n",
    "\n",
    "def get_visualization_filename(plot_type, exp_type, na, nr, e, m, pool_sizes=None, story_name=None):\n",
    "    \"\"\"Generate visualization filenames\"\"\"\n",
    "    # Determine pool topology string\n",
    "    if pool_sizes:\n",
    "        if pool_sizes == [2]:\n",
    "            pool_str = \"multi_pool_2\"\n",
    "        else:\n",
    "            raise ValueError(\"Only pool_sizes=[2] is supported in simplified version\")\n",
    "    else:\n",
    "        pool_str = \"single_pool\"\n",
    "\n",
    "    # Build parameter string\n",
    "    params = f\"ag{na}_ro{nr}_end{e}_mult{m}\"\n",
    "\n",
    "    # Build filename: {plot_type}_{pool_topology}_{exp_type}_{params}.pdf\n",
    "    if exp_type == \"different_story\" or story_name is None:\n",
    "        return f\"{plot_type}_{pool_str}_{exp_type}_{params}.pdf\"\n",
    "    else:\n",
    "        return f\"{plot_type}_{pool_str}_{exp_type}_{story_name}_{params}.pdf\"\n",
    "\n",
    "def get_experiment_filename(file_type, exp_type, story_name, na, nr, e, m, pool_sizes=None, game_idx=None):\n",
    "    \"\"\"Generate standardized filenames\"\"\"\n",
    "    # Determine pool topology string\n",
    "    if pool_sizes:\n",
    "        # SIMPLIFIED: only support pool_sizes=[2]\n",
    "        if pool_sizes == [2]:\n",
    "            pool_str = \"multi_pool_2\"\n",
    "        else:\n",
    "            raise ValueError(\"Only pool_sizes=[2] is supported in simplified version\")\n",
    "    else:\n",
    "        pool_str = \"single_pool\"\n",
    "\n",
    "    # Build parameter string\n",
    "    params = f\"ag{na}_ro{nr}_end{e}_mult{m}\"\n",
    "\n",
    "    # Build filename based on file type\n",
    "    if file_type == \"game_results\":\n",
    "        if exp_type == \"different_story\":\n",
    "            return f\"game_results_{pool_str}_{exp_type}_{params}.csv\"\n",
    "        else:\n",
    "            return f\"game_results_{pool_str}_{exp_type}_{story_name}_{params}.csv\"\n",
    "\n",
    "    elif file_type == \"game_records\":\n",
    "        if exp_type == \"different_story\":\n",
    "            base = f\"game_records_{pool_str}_{exp_type}_{params}\"\n",
    "        else:\n",
    "            base = f\"game_records_{pool_str}_{exp_type}_{story_name}_{params}\"\n",
    "\n",
    "        if game_idx:\n",
    "            return f\"{base}_game{game_idx:03d}.txt\"\n",
    "        else:\n",
    "            return f\"{base}.txt\"\n",
    "\n",
    "    elif file_type == \"checkpoint\":\n",
    "        if exp_type == \"different_story\":\n",
    "            return f\"checkpoint_{pool_str}_{exp_type}_{params}.pkl\"\n",
    "        else:\n",
    "            return f\"checkpoint_{pool_str}_{exp_type}_{story_name}_{params}.pkl\"\n",
    "\n",
    "    else:\n",
    "        return f\"{file_type}.txt\"\n",
    "\n",
    "def load_intermediate_results_new(exp_type, story_name, na, nr, e, m, pool_sizes=None):\n",
    "    \"\"\"Load intermediate results for resuming interrupted experiments.\"\"\"\n",
    "    exp_dir = get_experiment_directory(exp_type, story_name, na, nr, e, m, pool_sizes)\n",
    "    checkpoint_filename = get_experiment_filename(\"checkpoint\", exp_type, story_name, na, nr, e, m, pool_sizes)\n",
    "    new_path = exp_dir / checkpoint_filename\n",
    "\n",
    "    # Check new location first\n",
    "    if new_path.exists():\n",
    "        with open(new_path, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "        print(f\"Loaded checkpoint from new location: {new_path}\")\n",
    "        return results, new_path\n",
    "\n",
    "    # Backward compatibility: check old location pattern (from original notebook)\n",
    "    old_filename = f\"results_{exp_type}_{story_name}_ag{na}_ro{nr}_end{e}_mult{m}\"\n",
    "    if pool_sizes:\n",
    "        old_filename += f\"_pools{'-'.join(map(str, pool_sizes))}\"\n",
    "    old_filename += \".pkl\"\n",
    "\n",
    "    if os.path.exists(old_filename):\n",
    "        with open(old_filename, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "        print(f\"Loaded checkpoint from old location: {old_filename}\")\n",
    "        # Migrate to new location\n",
    "        with open(new_path, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "        print(f\"Migrated checkpoint to: {new_path}\")\n",
    "        return results, new_path\n",
    "\n",
    "    # No existing results\n",
    "    print(f\"No existing checkpoint found. Starting fresh at: {new_path}\")\n",
    "    return {} if exp_type != \"different_story\" else [], new_path\n",
    "\n",
    "def save_intermediate_results_new(results, checkpoint_path):\n",
    "    \"\"\"Save intermediate results to checkpoint file for resumability.\"\"\"\n",
    "    with open(checkpoint_path, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "# These functions remain the same - no changes needed:\n",
    "def get_system_prompt(na, e, m, is_networked=False):\n",
    "    \"\"\"\n",
    "    Generate system prompt for agents based on game configuration.\n",
    "    Args:\n",
    "        na: Number of agents\n",
    "        e: Token endowment per round\n",
    "        m: Multiplier for shared pool returns\n",
    "        is_networked: True for multi-pool, False for single-pool\n",
    "    Returns:\n",
    "        System prompt template with STORY placeholder\n",
    "    \"\"\"\n",
    "    if is_networked:\n",
    "        base = (\n",
    "            f\"You are participating in a repeated networked public goods game with {na} agents. \"\n",
    "            f\"In each round, you receive {e} tokens. \"\n",
    "            \"You will be prompted sequentially for contributions to each shared pool - some with all agents (global) and some with smaller subsets. \"\n",
    "            f\"After everyone contributes, each pool's total is multiplied by {m} and divided equally among its members. \"\n",
    "            \"Respond only with <TOKEN>k</TOKEN> where k is your integer contribution to the current pool.\"\n",
    "        )\n",
    "    else:\n",
    "        base = (\n",
    "            f\"You are participating in a repeated public goods game with {na - 1} other agents. \"\n",
    "            f\"In each round, you receive {e} tokens. You must decide how many tokens (an integer between 0 and {e}) \"\n",
    "            f\"to contribute to a public pool. After everyone contributes, the total contribution is multiplied by {m} \"\n",
    "            f\"and divided equally among all {na} agents. This means your payoff for a round is calculated as: \\\\n\\\\n\"\n",
    "            f\"    {e} - (your contribution) + ({m} * total contributions / {na})\\\\n\\\\n\"\n",
    "            \"Please respond **only** with the number of tokens you wish to contribute for this round, enclosed within `<TOKEN>` and `</TOKEN>` tags. \"\n",
    "            \"For example: `<TOKEN>5</TOKEN>`. Do not provide any additional text, explanations, or summaries. \"\n",
    "        )\n",
    "    return base + \"STORY\"\n",
    "\n",
    "def compute_and_print_statistics(scores_by_prompt, exp_name):\n",
    "    \"\"\"Computes and prints mean and standard deviation of scores for each prompt.\"\"\"\n",
    "    print(f\"\\\\n=== Collaboration Scores for {exp_name} ===\")\n",
    "    for prompt_label, score_list in scores_by_prompt.items():\n",
    "        mean_val = statistics.mean(score_list)\n",
    "        stdev_val = statistics.stdev(score_list) if len(score_list) > 1 else 0\n",
    "        print(f\"{prompt_label}: Mean = {mean_val:.4f}, SD = {stdev_val:.4f}\")\n",
    "\n",
    "def extract_contribution(response_str):\n",
    "    \"\"\"Extract valid contribution from agent's response.\"\"\"\n",
    "    match = re.search(r\"<TOKEN>(\\d+)</TOKEN>\", response_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def get_valid_contribution(agent, round_num, e, max_retries=5, pool_context=\"\"):\n",
    "    \"\"\"Get valid contribution from agent with retries.\"\"\"\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        prompt = f\"Round {round_num}: {pool_context}What is your contribution (0-{e})?\"\n",
    "\n",
    "        if retries > 0:\n",
    "            prompt += \" Your previous response was invalid. **Only provide a number inside `<TOKEN>...</TOKEN>`** with no extra text. Example: `<TOKEN>5</TOKEN>`.\"\n",
    "\n",
    "        response_str = agent.chat(prompt).strip()\n",
    "        print(f\"{agent.name} response (attempt {retries + 1}): {response_str}\")\n",
    "\n",
    "        contribution = extract_contribution(response_str)\n",
    "\n",
    "        if contribution is not None:\n",
    "            return contribution\n",
    "\n",
    "        print(f\"Warning: {agent.name} provided an invalid response. Retrying... ({retries + 1}/{max_retries})\")\n",
    "        retries += 1\n",
    "\n",
    "    print(f\"Error: {agent.name} failed to provide a valid response after {max_retries} attempts. Defaulting to 0.\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8TZf8IMTM-4"
   },
   "source": [
    "# Game Logic Functions\n",
    "Core game mechanics: collects contributions, calculates payoffs, and logs data in consistent CSV format for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZzeUy_OTOXc"
   },
   "outputs": [],
   "source": [
    "CSV_HEADER = [\n",
    "    \"Game\", \"PromptType\", \"Round\", \"AgentName\",\n",
    "    \"Contribution\",      # Legacy column: total contribution amount\n",
    "    \"GlobalContrib\",     # Global pool contribution\n",
    "    \"LocalContrib\",      # Sum of local pool contributions\n",
    "    \"TotalContrib\",      # Total across all pools (Global + Local)\n",
    "    \"RoundPayoff\", \"CumulativePayoff\", \"CollaborationScore\"\n",
    "]\n",
    "\n",
    "def collect_contributions_single_pool(agents, round_num, e):\n",
    "    \"\"\"Collects valid contributions from all agents for single pool game.\"\"\"\n",
    "    contributions = []\n",
    "    for agent in agents:\n",
    "        contribution = get_valid_contribution(agent, round_num, e)\n",
    "\n",
    "        # Enforce valid contribution range\n",
    "        if contribution > e:\n",
    "            print(f\"{agent.name} attempted to contribute {contribution} tokens but only has {e}. \"\n",
    "                    f\"Reducing contribution to {e}.\")\n",
    "            contribution = e\n",
    "        contribution = max(0, contribution)\n",
    "        contributions.append(contribution)\n",
    "    print(f\"Round Contributions: {contributions}\")\n",
    "    return contributions\n",
    "\n",
    "def collect_contributions_multi_pool(agents: List[Agent], pools: Dict[str, List[int]], e: int) -> Dict[str, Dict[int, int]]:\n",
    "    \"\"\"Collects contributions from agents for multi-pool games.\"\"\"\n",
    "    remaining = {i: e for i in range(len(agents))}\n",
    "    contributions = {pid: {} for pid in pools}\n",
    "\n",
    "    # Randomize pool order (agents don't know order)\n",
    "    for pid in random.sample(list(pools.keys()), len(pools)):\n",
    "        # Randomize agent order within pool\n",
    "        members = random.sample(pools[pid], len(pools[pid]))\n",
    "        for i in members:\n",
    "            others = [agents[j].name for j in pools[pid] if j != i]\n",
    "            pool_context = f\"Pool '{pid}' with {others}. You have {remaining[i]} tokens remaining. \"\n",
    "\n",
    "            contribution = get_valid_contribution(\n",
    "                agent=agents[i],\n",
    "                round_num=0,  # Will be set by caller\n",
    "                e=remaining[i],\n",
    "                pool_context=pool_context\n",
    "            )\n",
    "            contribution = max(0, min(contribution, remaining[i]))\n",
    "            remaining[i] -= contribution\n",
    "            contributions[pid][i] = contribution\n",
    "\n",
    "    return contributions\n",
    "\n",
    "def calculate_rewards_single_pool(contributions, agents, e, m, na, total_rewards, round_num):\n",
    "    \"\"\"Calculates payoffs and updates agent rewards for single pool.\"\"\"\n",
    "    round_total = sum(contributions)\n",
    "    # Calculate payoffs for the round using original formula\n",
    "    payoffs = calculate_payoffs(contributions, e, m, na)\n",
    "    print(f\"Round Payoffs: {payoffs}\")\n",
    "\n",
    "    for idx, agent in enumerate(agents):\n",
    "        total_rewards[idx] += payoffs[idx]\n",
    "        summary = (\n",
    "            f\"Round {round_num} Summary:\\\\n\"\n",
    "            f\" - Your contribution: {contributions[idx]}\\\\n\"\n",
    "            f\" - Total contributions: {round_total}\\\\n\"\n",
    "            f\" - Your payoff this round: {payoffs[idx]:.2f}\\\\n\"\n",
    "            f\" - Your cumulative reward: {total_rewards[idx]:.2f}\"\n",
    "        )\n",
    "        agent.chat(summary)\n",
    "\n",
    "    return payoffs, round_total, total_rewards\n",
    "\n",
    "def calculate_rewards_multi_pool(\n",
    "    contributions: Dict[str, Dict[int,int]],\n",
    "    pools: Dict[str, List[int]],\n",
    "    agents: List[Agent],\n",
    "    e: int,\n",
    "    m: float,\n",
    "    total_rewards: List[float],\n",
    "    round_num: int\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Calculates payoffs using the formula: π_i = Σ_{p: i ∈ M_p} (m * T_p / |M_p|) + (T - Σ_p t_{i,p})\n",
    "    \"\"\"\n",
    "    n = len(agents)\n",
    "    round_payoffs = [0.0] * n\n",
    "\n",
    "    # First term: Σ_{p: i ∈ M_p} (m * T_p / |M_p|)\n",
    "    for pid, pool_contributions in contributions.items():\n",
    "        members = list(pool_contributions.keys())\n",
    "        pool_total = sum(pool_contributions.values())\n",
    "        pool_size = len(members)\n",
    "\n",
    "        if pool_size > 0:\n",
    "            bonus_per_member = m * pool_total / pool_size\n",
    "            for agent_idx in members:\n",
    "                round_payoffs[agent_idx] += bonus_per_member\n",
    "\n",
    "    # Second term: (T - Σ_p t_{i,p}) where T = e (endowment)\n",
    "    for i in range(n):\n",
    "        total_contributed = sum(contributions[pid].get(i, 0) for pid in contributions)\n",
    "        kept_tokens = e - total_contributed\n",
    "        round_payoffs[i] += kept_tokens\n",
    "        total_rewards[i] += round_payoffs[i]\n",
    "\n",
    "    # Send feedback to agents\n",
    "    for i, agent in enumerate(agents):\n",
    "        msg = [f\"Round {round_num} Results:\"]\n",
    "        agent_pools = []\n",
    "        for pid, pool_contribs in contributions.items():\n",
    "            if i in pool_contribs:\n",
    "                others_in_pool = [j for j in pools[pid] if j != i]\n",
    "                other_contributions = [f\"{agents[j].name}→{pool_contribs[j]}\" for j in others_in_pool if j in pool_contribs]\n",
    "                msg.append(f\"Pool '{pid}': You→{pool_contribs[i]}, \" + \", \".join(other_contributions))\n",
    "\n",
    "        msg.append(f\"Round payoff: {round_payoffs[i]:.2f}, Cumulative: {total_rewards[i]:.2f}\")\n",
    "        agent.chat(\"\\\\n\".join(msg))\n",
    "\n",
    "    return round_payoffs\n",
    "\n",
    "def get_csv_header():\n",
    "    \"\"\"Returns the unified CSV header for all experiment types.\"\"\"\n",
    "    return CSV_HEADER\n",
    "\n",
    "def log_round_results(\n",
    "    csv_writer,\n",
    "    agents,\n",
    "    game_index,\n",
    "    prompt_label,\n",
    "    round_num,\n",
    "    payoffs,\n",
    "    total_rewards,\n",
    "    exp_type,\n",
    "    # Single-pool specific\n",
    "    contributions_single=None,\n",
    "    # Multi-pool specific\n",
    "    pool_contributions=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Log round results to CSV in unified format supporting both pool topologies.\n",
    "\n",
    "    Single-pool: Uses contributions_single parameter\n",
    "    Multi-pool: Uses pool_contributions parameter (dict by pool_id)\n",
    "\n",
    "    Maintains consistent column structure for easy analysis across experiment types.\n",
    "    \"\"\"\n",
    "    for idx, agent in enumerate(agents):\n",
    "        # Determine story label based on experiment type\n",
    "        if exp_type == \"different_story\" and hasattr(agent, 'story_label'):\n",
    "            story_label = agent.story_label\n",
    "        else:\n",
    "            story_label = prompt_label\n",
    "\n",
    "        if pool_contributions:  # Multi-pool case\n",
    "            global_contrib = pool_contributions.get(\"global\", {}).get(idx, 0)\n",
    "            local_contrib = sum(\n",
    "                pool_contributions[pid].get(idx, 0)\n",
    "                for pid in pool_contributions\n",
    "                if pid != \"global\"\n",
    "            )\n",
    "            total_contrib = global_contrib + local_contrib\n",
    "            # For backward compatibility\n",
    "            contribution = total_contrib\n",
    "\n",
    "        else:  # Single-pool case\n",
    "            contribution = contributions_single[idx] if contributions_single else 0\n",
    "            global_contrib = contribution  # In single-pool, all goes to \"global\"\n",
    "            local_contrib = 0\n",
    "            total_contrib = contribution\n",
    "\n",
    "        csv_writer.writerow([\n",
    "            game_index,\n",
    "            story_label,\n",
    "            round_num,\n",
    "            agent.name,\n",
    "            contribution,      # Backward compatibility column\n",
    "            global_contrib,\n",
    "            local_contrib,\n",
    "            total_contrib,\n",
    "            f\"{payoffs[idx]:.2f}\",\n",
    "            f\"{total_rewards[idx]:.2f}\",\n",
    "            \"\"  # CollaborationScore (empty for per-round rows)\n",
    "        ])\n",
    "\n",
    "def log_final_score(csv_writer, game_index, prompt_label, effective_score):\n",
    "    \"\"\"Log the final collaboration score row.\"\"\"\n",
    "    csv_writer.writerow([\n",
    "        game_index,\n",
    "        prompt_label,\n",
    "        \"final\",\n",
    "        \"All\",\n",
    "        \"\",  # Contribution\n",
    "        \"\",  # GlobalContrib\n",
    "        \"\",  # LocalContrib\n",
    "        \"\",  # TotalContrib\n",
    "        \"\",  # RoundPayoff\n",
    "        \"\",  # CumulativePayoff\n",
    "        f\"{effective_score:.4f}\"  # CollaborationScore\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWRjCCccTPVx"
   },
   "source": [
    "# Main Game Execution\n",
    "Functions to run complete games with multiple rounds, handle both single-pool and multi-pool experiment types, and manage the overall game flow and data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSfZMA0QTSME"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Execute complete game sessions with multiple rounds.\n",
    "\n",
    "Features:\n",
    "- Handles both single-pool and multi-pool topologies seamlessly\n",
    "- Collects agent contributions and calculates payoffs\n",
    "- Provides round-by-round feedback to agents\n",
    "- Logs detailed results with descriptive filenames\n",
    "- Saves data in consistent CSV format for analysis\n",
    "\"\"\"\n",
    "\n",
    "def execute_game_rounds(agents, na, nr, e, m, exp_dir, game_index, prompt_label, exp_type, num_dummy_agents, pools=None, csv_filename=None):\n",
    "    is_networked = pools is not None and len(pools) > 1\n",
    "    total_rewards = [0 for _ in range(na)]\n",
    "    total_game_contributions = 0\n",
    "\n",
    "    # Open CSV file in experiment directory with descriptive filename\n",
    "    csv_path = exp_dir / csv_filename\n",
    "    csv_exists = csv_path.exists()\n",
    "\n",
    "    with open(csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "\n",
    "        # Write header if new file\n",
    "        if not csv_exists:\n",
    "            writer.writerow(CSV_HEADER)\n",
    "\n",
    "        print(\"\\\\n=== Starting a New Game ===\")\n",
    "        for round_num in range(1, nr + 1):\n",
    "            print(f\"\\\\n--- Round {round_num} ---\")\n",
    "\n",
    "            if is_networked:\n",
    "                # Multi-pool game\n",
    "                pool_contributions = collect_contributions_multi_pool(agents, pools, e)\n",
    "                payoffs = calculate_rewards_multi_pool(pool_contributions, pools, agents, e, m, total_rewards, round_num)\n",
    "                log_round_results(\n",
    "                    writer, agents, game_index, prompt_label, round_num,\n",
    "                    payoffs, total_rewards, exp_type, pool_contributions=pool_contributions\n",
    "                )\n",
    "                total_game_contributions += sum(sum(v.values()) for v in pool_contributions.values())\n",
    "            else:\n",
    "                # Single-pool game\n",
    "                contributions = collect_contributions_single_pool(agents, round_num, e)\n",
    "                payoffs, round_total, total_rewards = calculate_rewards_single_pool(contributions, agents, e, m, na, total_rewards, round_num)\n",
    "                log_round_results(\n",
    "                    writer, agents, game_index, prompt_label, round_num,\n",
    "                    payoffs, total_rewards, exp_type, contributions_single=contributions\n",
    "                )\n",
    "                total_game_contributions += round_total\n",
    "\n",
    "        # Calculate and log final score\n",
    "        if is_networked:\n",
    "            max_possible = na * e * nr  # Total possible contributions\n",
    "        else:\n",
    "            max_possible = (na - num_dummy_agents) * e * nr\n",
    "\n",
    "        effective_score = total_game_contributions / max_possible\n",
    "        print(f\"\\\\nEffective Collaboration Score: {effective_score:.4f}\")\n",
    "\n",
    "        # Log final score\n",
    "        log_final_score(writer, game_index, prompt_label, effective_score)\n",
    "\n",
    "    return effective_score, total_rewards\n",
    "\n",
    "def run_single_game(game_index: int, prompt_label: str, system_prompt_used: str,\n",
    "                    na: int, nr: int, e: int, m: float, exp_dir, exp_type: str, story_name: str,\n",
    "                    num_dummy_agents, pools=None, pool_sizes=None) -> float:\n",
    "    \"\"\"\n",
    "    Run a single game.\n",
    "\n",
    "    Creates fresh agent instances, runs all rounds, calculates final collaboration score.\n",
    "    Automatically handles single-pool vs multi-pool logic based on pools parameter.\n",
    "\n",
    "    Returns:\n",
    "        Final collaboration score for the game\n",
    "    \"\"\"\n",
    "    # Generate descriptive records filename\n",
    "    records_filename = get_experiment_filename(\"game_records\", exp_type, story_name, na, nr, e, m, pool_sizes, game_index)\n",
    "    records_path = exp_dir / records_filename\n",
    "\n",
    "    # Generate descriptive CSV filename\n",
    "    csv_filename = get_experiment_filename(\"game_results\", exp_type, story_name, na, nr, e, m, pool_sizes)\n",
    "\n",
    "    with open(records_path, \"w\", encoding=\"utf-8\") as records_file:\n",
    "        # Create new agents for this game\n",
    "        agents = []\n",
    "        for i in range(na):\n",
    "            if i < num_dummy_agents:\n",
    "                agent = DummyAgent(f\"Agent_{i+1}\", system_prompt_used, records_file)\n",
    "            else:\n",
    "                agent = Agent(f\"Agent_{i+1}\", system_prompt_used, records_file)\n",
    "            agents.append(agent)\n",
    "\n",
    "        for agent in agents:\n",
    "            agent.story_label = prompt_label\n",
    "\n",
    "        # Execute all rounds of the game\n",
    "        effective_score, _ = execute_game_rounds(\n",
    "            agents, na, nr, e, m, exp_dir, game_index, prompt_label, exp_type, num_dummy_agents, pools, csv_filename\n",
    "        )\n",
    "\n",
    "    return effective_score\n",
    "\n",
    "def run_single_game_random_story(game_index: int, system_prompt_story: str, na: int, nr: int, e: int, m: float,\n",
    "                                exp_dir, exp_type: str, story_prompts: dict, pools=None, pool_sizes=None) -> (float, list):\n",
    "    \"\"\"\n",
    "    Run a single game where each agent gets a random story.\n",
    "    \"\"\"\n",
    "    # Generate descriptive records filename\n",
    "    records_filename = get_experiment_filename(\"game_records\", exp_type, None, na, nr, e, m, pool_sizes, game_index)\n",
    "    records_path = exp_dir / records_filename\n",
    "\n",
    "    # Generate descriptive CSV filename\n",
    "    csv_filename = get_experiment_filename(\"game_results\", exp_type, None, na, nr, e, m, pool_sizes)\n",
    "\n",
    "    with open(records_path, \"w\", encoding=\"utf-8\") as records_file:\n",
    "        agents = []\n",
    "\n",
    "        # Create agents with random story prompts\n",
    "        for i in range(na):\n",
    "            chosen_label, chosen_story = random.choice(list(story_prompts.items()))\n",
    "            prompt_text = system_prompt_story.replace(\"STORY\", chosen_story)\n",
    "            agent = Agent(f\"Agent_{i+1}\", prompt_text, records_file)\n",
    "            agent.story_label = chosen_label\n",
    "            agents.append(agent)\n",
    "\n",
    "        # Execute all rounds of the game\n",
    "        effective_score, total_rewards = execute_game_rounds(\n",
    "            agents, na, nr, e, m, exp_dir, game_index, \"All\", exp_type, 0, pools, csv_filename\n",
    "        )\n",
    "\n",
    "        # Prepare results: (agent_name, story_label, cumulative_reward)\n",
    "        agent_results = [(agents[i].name, agents[i].story_label, total_rewards[i]) for i in range(na)]\n",
    "\n",
    "    return effective_score, agent_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4Tcmm1FTS_S"
   },
   "source": [
    "# Experiment Configuration\n",
    "High-level experiment runners for three experiment types: Homogeneous (same story), Robustness (with dummy agent), Heterogeneous (random stories).\n",
    "\n",
    "Supports both pool topologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5ZHSGLuTU9n"
   },
   "outputs": [],
   "source": [
    "def run_same_story_experiment(is_bad_apple, story_index, num_rounds_list, endowment_list, multiplier_list, num_games, num_agents_list, exp_type, pool_sizes=None):\n",
    "    \"\"\"Runs the same story experiment where all agents receive the same story.\"\"\"\n",
    "    story_files = sorted(glob.glob(\"stories/*.txt\"))\n",
    "    if story_index >= len(story_files):\n",
    "        print(\"Invalid story index. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    selected_story_file = story_files[story_index]\n",
    "    story_name = os.path.splitext(os.path.basename(selected_story_file))[0]\n",
    "\n",
    "    for na, nr, e, m in itertools.product(num_agents_list, num_rounds_list, endowment_list, multiplier_list):\n",
    "        # Determine experiment type name\n",
    "        base_exp_type = \"bad_apple\" if is_bad_apple else \"same_story\"\n",
    "        num_dummy_agents = 1 if is_bad_apple else 0\n",
    "\n",
    "        print(f\"\\\\n\\\\n######################\")\n",
    "        print(f\"Running {base_exp_type} experiment: {story_name}\")\n",
    "        print(f\"Agents: {na}, Rounds: {nr}, Endowment: {e}, Multiplier: {m}\")\n",
    "        if pool_sizes:\n",
    "            print(f\"Pool sizes: {pool_sizes}\")\n",
    "        print(f\"######################\\\\n\")\n",
    "\n",
    "        scores_by_prompt = run_same_story_games(\n",
    "            base_exp_type, selected_story_file, story_name, na, nr, e, m, num_games, num_dummy_agents,\n",
    "            base_exp_type, pool_sizes  # Pass base_exp_type as game_exp_type parameter\n",
    "        )\n",
    "        compute_and_print_statistics(scores_by_prompt, f\"{base_exp_type}_{story_name}\")\n",
    "\n",
    "def run_different_story_experiment(num_rounds_list, endowment_list, multiplier_list, num_games, num_agents_list, exp_type, pool_sizes=None):\n",
    "    \"\"\"Runs different story experiment where each agent receives a random story.\"\"\"\n",
    "\n",
    "    for na, nr, e, m in itertools.product(num_agents_list, num_rounds_list, endowment_list, multiplier_list):\n",
    "        print(f\"\\\\n\\\\n######################\")\n",
    "        print(f\"Running different_story experiment\")\n",
    "        print(f\"Agents: {na}, Rounds: {nr}, Endowment: {e}, Multiplier: {m}\")\n",
    "        if pool_sizes:\n",
    "            print(f\"Pool sizes: {pool_sizes}\")\n",
    "        print(f\"######################\\\\n\")\n",
    "\n",
    "        story_prompts = load_all_story_prompts()\n",
    "        system_prompt = get_system_prompt(na, e, m, pool_sizes is not None)\n",
    "\n",
    "        scores_list, rewards_by_story = run_different_story_games(\n",
    "            na, nr, e, m, num_games, story_prompts, system_prompt, exp_type, pool_sizes\n",
    "        )\n",
    "\n",
    "        print(f\"\\\\n=== Rewards by Story for different_story experiment ===\")\n",
    "        compute_and_print_statistics(rewards_by_story, \"different_story\")\n",
    "        if scores_list:\n",
    "            print(f\"\\\\nOverall Effective Collaboration Score: Mean = {statistics.mean(scores_list):.4f}, SD = {statistics.stdev(scores_list):.4f}\")\n",
    "\n",
    "def run_same_story_games(exp_type, story_file, story_name, na, nr, e, m, num_games, num_dummy_agents, game_exp_type, pool_sizes=None):\n",
    "    \"\"\"\n",
    "    Execute multiple games with identical story prompts for all agents.\n",
    "    Supports checkpoint/resume functionality for long experiments.\n",
    "    Automatically generates descriptive filenames for data organization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get experiment directory\n",
    "    exp_dir = get_experiment_directory(exp_type, story_name, na, nr, e, m, pool_sizes)\n",
    "\n",
    "    # Load story content\n",
    "    with open(story_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        story_content = f.read()\n",
    "    if story_name not in [\"maxreward\", \"noinstruct\"]:\n",
    "        story_content = \"Your behavior is influenced by the following bedtime story your mother read to you every night: \" + story_content\n",
    "\n",
    "    # Create system prompt\n",
    "    is_networked = pool_sizes is not None\n",
    "    prompt_text = get_system_prompt(na, e, m, is_networked).replace(\"STORY\", story_content)\n",
    "\n",
    "    # Load existing results with backward compatibility\n",
    "    intermediate_results, checkpoint_path = load_intermediate_results_new(exp_type, story_name, na, nr, e, m, pool_sizes)\n",
    "\n",
    "    if story_name not in intermediate_results:\n",
    "        intermediate_results[story_name] = []\n",
    "\n",
    "    scores = intermediate_results[story_name][:]\n",
    "    print(f\"\\\\n=== Running Games: {story_name} ===\")\n",
    "    print(f\"Experiment directory: {exp_dir}\")\n",
    "    print(f\"Completed games: {len(scores)}\")\n",
    "\n",
    "    for game_index in range(len(scores) + 1, num_games + 1):\n",
    "        print(f\"\\\\n=== Game {game_index}/{num_games} ({story_name}) ===\")\n",
    "\n",
    "        # Build pools if networked\n",
    "        pools = build_pools(na, pool_sizes) if is_networked else None\n",
    "\n",
    "        try:\n",
    "            score = run_single_game(\n",
    "                game_index, story_name, prompt_text, na, nr, e, m, exp_dir,\n",
    "                exp_type, story_name, num_dummy_agents, pools, pool_sizes\n",
    "            )\n",
    "            scores.append(score)\n",
    "            intermediate_results[story_name].append(score)\n",
    "            save_intermediate_results_new(intermediate_results, checkpoint_path)\n",
    "            print(f\"Game {game_index} completed. Score: {score:.4f}\")\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f\"Game {game_index} failed: {exc}\")\n",
    "            continue\n",
    "\n",
    "    return {story_name: scores}\n",
    "\n",
    "def run_different_story_games(na, nr, e, m, num_games, story_prompts, system_prompt, exp_type, pool_sizes=None):\n",
    "    \"\"\"Run different story games.\"\"\"\n",
    "\n",
    "    # Get experiment directory\n",
    "    exp_dir = get_experiment_directory(\"different_story\", None, na, nr, e, m, pool_sizes)\n",
    "\n",
    "    # Load existing results\n",
    "    intermediate_results, checkpoint_path = load_intermediate_results_new(\"different_story\", None, na, nr, e, m, pool_sizes)\n",
    "\n",
    "    scores_list = []\n",
    "    rewards_by_story = {story: [] for story in story_prompts}\n",
    "\n",
    "    print(f\"\\\\n=== Running Different Story Games ===\")\n",
    "    print(f\"Experiment directory: {exp_dir}\")\n",
    "    print(f\"Completed games: {len(intermediate_results)}\")\n",
    "\n",
    "    for game_index in range(len(intermediate_results) + 1, num_games + 1):\n",
    "        print(f\"\\\\n=== Game {game_index}/{num_games} (different_story) ===\")\n",
    "\n",
    "        # Build pools if networked\n",
    "        pools = build_pools(na, pool_sizes) if pool_sizes else None\n",
    "\n",
    "        try:\n",
    "            score, agent_results = run_single_game_random_story(\n",
    "                game_index, system_prompt, na, nr, e, m, exp_dir, exp_type, story_prompts, pools, pool_sizes\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            intermediate_results.append((game_index, score, agent_results))\n",
    "            save_intermediate_results_new(intermediate_results, checkpoint_path)\n",
    "\n",
    "            scores_list.append(score)\n",
    "            for _, story_label, reward in agent_results:\n",
    "                rewards_by_story[story_label].append(reward)\n",
    "\n",
    "            print(f\"Game {game_index} completed. Score: {score:.4f}\")\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f\"Game {game_index} failed: {exc}\")\n",
    "            continue\n",
    "\n",
    "    return scores_list, rewards_by_story\n",
    "\n",
    "def load_all_story_prompts(stories_dir=\"stories\"):\n",
    "    \"\"\"Load all story prompts from files.\"\"\"\n",
    "    story_prompts = {}\n",
    "    for story_file in sorted(glob.glob(f\"{stories_dir}/*.txt\")):\n",
    "        story_name = os.path.splitext(os.path.basename(story_file))[0]\n",
    "        with open(story_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        if story_name not in [\"maxreward\", \"noinstruct\"]:\n",
    "            content = \"Your behavior is influenced by the following bedtime story your mother read to you every night: \" + content\n",
    "        story_prompts[story_name] = content\n",
    "    return story_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMB88ldLTWt8"
   },
   "source": [
    "# Main Execution Block\n",
    "Main experiment runner function: run any experiment type with optional pool_sizes parameter.\n",
    "\n",
    "Use pool_sizes=None for single-pool or pool_sizes=[2] for multi-pool mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjm4H-npTYQi"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Configurable Run Function\n",
    "# -----------------------------\n",
    "\n",
    "def run_experiment(exp_type, story_index=0, pool_sizes=None):\n",
    "    \"\"\"\n",
    "    Main experiment entry point supporting all experiment types and pool topologies.\n",
    "\n",
    "    Args:\n",
    "        exp_type: \"same_story\", \"bad_apple\", \"different_story\"\n",
    "        story_index: Index of story to use (for same_story/bad_apple)\n",
    "        pool_sizes: None for single-pool, [2] for multi-pool pairs\n",
    "    \"\"\"\n",
    "    # Experiment configurations\n",
    "    num_rounds_list = [5]\n",
    "    endowment_list = [10]\n",
    "    multiplier_list = [1.5]\n",
    "\n",
    "    try:\n",
    "        if exp_type in [\"same_story\", \"bad_apple\"]:\n",
    "            num_games = 100\n",
    "            num_agents_list = [4, 16, 32] if exp_type == \"same_story\" else [4]\n",
    "            run_same_story_experiment(\n",
    "                is_bad_apple=(exp_type == \"bad_apple\"),\n",
    "                story_index=story_index,\n",
    "                num_rounds_list=num_rounds_list,\n",
    "                endowment_list=endowment_list,\n",
    "                multiplier_list=multiplier_list,\n",
    "                num_games=num_games,\n",
    "                num_agents_list=num_agents_list,\n",
    "                exp_type=exp_type,\n",
    "                pool_sizes=pool_sizes\n",
    "            )\n",
    "        elif exp_type == \"different_story\":\n",
    "            num_games = 400\n",
    "            num_agents_list = [4]\n",
    "            run_different_story_experiment(\n",
    "                num_rounds_list, endowment_list, multiplier_list, num_games, num_agents_list, exp_type, pool_sizes\n",
    "            )\n",
    "        else:\n",
    "            print(f\"[ERROR] Unknown experiment type: {exp_type}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Experiment '{exp_type}' failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHUeU-cBTZ97"
   },
   "source": [
    "# Single-Pool Experiments – Homogeneous Agents\n",
    "Baseline experiments: All agents get the same story prompt. Tests how different narratives affect cooperation in standard public goods setting.\n",
    "\n",
    "Run all 12 stories with same-story configuration across different agent sizes (4, 16, 32 agents) for 100 games each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdBk4LaITb4T"
   },
   "outputs": [],
   "source": [
    "# Execute homogeneous experiments: all agents receive identical story prompts\n",
    "# Tests baseline cooperation patterns for each of the 12 story types\n",
    "\n",
    "for i in range(12):\n",
    "    print(f\"Running same_story experiment for story {i}\")\n",
    "    run_experiment(\"same_story\", story_index=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4VGtWuRTdy9"
   },
   "source": [
    "# Single-Pool Experiments – Robustness Testing\n",
    "\n",
    "Same as homogeneous but with one dummy agent (always contributes 0) to test cooperation resilience (robustness) to non-cooperative agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dx3PeIECTfTI"
   },
   "outputs": [],
   "source": [
    "# Execute robustness experiments: same as homogeneous but includes one non-cooperative agent\n",
    "# Tests resilience of cooperation when facing persistent free-riding behavior\n",
    "\n",
    "for i in range(12):\n",
    "    print(f\"Running bad_apple experiment for story {i}\")\n",
    "    run_experiment(\"bad_apple\", story_index=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "En445BTqTgrK"
   },
   "source": [
    "# Single-Pool Experiments – Heterogeneous Agents\n",
    "\n",
    "Run experiments where each agent gets a random story from the story corpus, testing performance in mixed narrative environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GTWkgNd_TiOi"
   },
   "outputs": [],
   "source": [
    "# Execute heterogeneous experiment: each agent receives random story prompt\n",
    "# Tests cooperation patterns when agents have diverse narrative influences\n",
    "\n",
    "print(\"Running different_story experiment\")\n",
    "run_experiment(\"different_story\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laNfQ3KKTkZW"
   },
   "source": [
    "# Multi-Pool Experiments\n",
    "\n",
    "Test cooperation in complex networks. Agents contribute to both global pool (all agents) and local pools (subsets).\n",
    "\n",
    "Reveals how network structure affects behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGlHY4pQTmN8"
   },
   "outputs": [],
   "source": [
    "# Multi-pool topology: global pool (all agents) + local pools (agent subsets)\n",
    "\n",
    "pool_sizes = [2]  # Creates 2 non-overlapping pools of size 2, plus 1 global pool (3 pools total)\n",
    "\n",
    "# Homogeneous multi-pool\n",
    "for i in range(12):\n",
    "    print(f\"Running networked same_story experiment for story {i}\")\n",
    "    run_experiment(\"same_story\", story_index=i, pool_sizes=pool_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gZt74_iTn45"
   },
   "outputs": [],
   "source": [
    "# Multi-pool topology: global pool (all agents) + local pools (agent subsets)\n",
    "\n",
    "pool_sizes = [2]  # Creates 2 non-overlapping pools of size 2, plus 1 global pool (3 pools total)\n",
    "\n",
    "# Heterogeneous multi-pool\n",
    "print(\"Running networked different_story experiment\")\n",
    "run_experiment(\"different_story\", pool_sizes=pool_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eATtOpDXTpTL"
   },
   "source": [
    "# Single-Pool Experiments – Visualization\n",
    "Data loading, preprocessing, and visualization functions for single-pool experiments: violin plots, scaling analysis, and statistical comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfMWU4G1TskH"
   },
   "source": [
    "## Data Loading and Preprocessing\n",
    "Load CSV files, filter data by experiment type, and prepare datasets for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVzU7AMOTq92"
   },
   "outputs": [],
   "source": [
    "def load_csv_files(pattern):\n",
    "    \"\"\"Loads all CSV files matching a pattern and merges them into a DataFrame.\"\"\"\n",
    "    files = glob.glob(pattern)\n",
    "    if not files:\n",
    "        print(f\"No files found for pattern: {pattern}\")\n",
    "        return pd.DataFrame()\n",
    "    return pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
    "\n",
    "def preprocess_data(df, metric):\n",
    "    \"\"\"\n",
    "    Prepares data by filtering only final round rows and converting columns to numeric types.\n",
    "    Metric can be 'CollaborationScore' or 'CumulativePayoff'.\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "\n",
    "    if metric == \"CollaborationScore\":\n",
    "        df = df[df[\"Round\"] == \"final\"].copy()\n",
    "    else:  # \"CumulativePayoff\"\n",
    "        df = df[df[\"Round\"] != \"final\"].copy()\n",
    "        df[\"Round\"] = pd.to_numeric(df[\"Round\"], errors=\"coerce\")\n",
    "        df.dropna(subset=[\"Round\"], inplace=True)\n",
    "        df = df.loc[df.groupby([\"Game\", \"AgentName\"])[\"Round\"].idxmax()].copy()\n",
    "\n",
    "    df[metric] = pd.to_numeric(df[metric], errors=\"coerce\")\n",
    "    df.dropna(subset=[metric], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7dUzgJ9TvZm"
   },
   "source": [
    "## File-Pattern Configurations\n",
    "Define file patterns and color schemes for consistent visualization across all experiment types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKhkMB8ATxEw"
   },
   "outputs": [],
   "source": [
    "AGENT_SIZES = [4, 16, 32]\n",
    "\n",
    "# Define baseline and meaningful story prompts\n",
    "BASELINE_STORIES = [\"noinstruct\", \"nsCarrot\", \"maxreward\", \"nsPlumber\"]\n",
    "MEANINGFUL_STORIES = [\"OldManSons\", \"Odyssey\", \"Soup\", \"Peacemaker\",\"Musketeers\", \"Teamwork\", \"Spoons\", \"Turnip\"]\n",
    "\n",
    "# Define color gradients for baseline (blue) and meaningful stories (pink)\n",
    "BLUE_SHADES = [\"#87CEFA\", \"#4682B4\", \"#4169E1\", \"#27408B\"]  # Light to Dark Blue\n",
    "PINK_SHADES = [\"#FFB3E6\", \"#FF99CC\", \"#FF66B3\", \"#FF4D9E\", \"#F02278\", \"#D81B60\", \"#B83B7D\", \"#B22272\"]  # Light to Dark Pink\n",
    "\n",
    "# Define color dictionary for plot consistency\n",
    "COLOR_DICT = {\n",
    "    # Baseline condition (Shades of Blue)\n",
    "    \"maxreward\": \"#87CEFA\",\n",
    "    \"noinstruct\": \"#4682B4\",\n",
    "    \"nsCarrot\": \"#4169E1\",\n",
    "    \"nsPlumber\": \"#27408B\",\n",
    "    # Meaningful stories (Shades of Purple/Pink)\n",
    "    \"Odyssey\": \"#FFB3E6\",\n",
    "    \"Soup\": \"#FF99CC\",\n",
    "    \"Peacemaker\": \"#FF66B3\",\n",
    "    \"Musketeers\": \"#FF4D9E\",\n",
    "    \"Teamwork\": \"#F02278\",\n",
    "    \"Spoons\": \"#D81B60\",\n",
    "    \"Turnip\": \"#B83B7D\",\n",
    "    \"OldManSons\": \"#B22272\",\n",
    "}\n",
    "\n",
    "VISUALISATION_EXPERIMENTS = {\n",
    "    \"same_story\": dict(\n",
    "        sizes=AGENT_SIZES,\n",
    "        pattern=\"experiments/single_pool/same_story/game_results_single_pool_same_story_*_ag{N}_ro5_end10_mult1.5.csv\"\n",
    "    ),\n",
    "    \"different_story\": dict(\n",
    "        sizes=[4],\n",
    "        pattern=\"experiments/single_pool/different_story/game_results_single_pool_different_story_ag{N}_ro5_end10_mult1.5.csv\"\n",
    "    ),\n",
    "    \"bad_apple\": dict(\n",
    "        sizes=[4],\n",
    "        pattern=\"experiments/single_pool/bad_apple/game_results_single_pool_bad_apple_*_ag{N}_ro5_end10_mult1.5.csv\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "CATEGORIES_FOR_VIOLIN = {\n",
    "    f\"{name}_{N}_agents\": exp[\"pattern\"].format(N=N)\n",
    "    for name, exp in VISUALISATION_EXPERIMENTS.items()\n",
    "    for N in exp[\"sizes\"]\n",
    "}\n",
    "\n",
    "CATEGORY_GROUPS = {\n",
    "    \"temp_0.6\": {\n",
    "        N: VISUALISATION_EXPERIMENTS[\"same_story\"][\"pattern\"].format(N=N)\n",
    "        for N in AGENT_SIZES\n",
    "    }\n",
    "}\n",
    "\n",
    "# For Latex Table\n",
    "TABLE_KEYS = {\n",
    "    \"same_story\": \"homogeneous\",\n",
    "    \"different_story\": \"heterogeneous\",\n",
    "    \"bad_apple\": \"robustness\",\n",
    "}\n",
    "\n",
    "# For table generation\n",
    "CATEGORIES_FOR_TABLE = {\n",
    "    TABLE_KEYS[name]: [\n",
    "        VISUALISATION_EXPERIMENTS[name][\"pattern\"].format(N=N)\n",
    "        for N in VISUALISATION_EXPERIMENTS[name][\"sizes\"]\n",
    "    ]\n",
    "    for name in VISUALISATION_EXPERIMENTS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZRKPfgkT01N"
   },
   "outputs": [],
   "source": [
    "CATEGORIES_FOR_VIOLIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPM1WR36T1tT"
   },
   "outputs": [],
   "source": [
    "CATEGORY_GROUPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nlyrlseT3Bh"
   },
   "outputs": [],
   "source": [
    "CATEGORIES_FOR_TABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvNmc4f2T4fX"
   },
   "source": [
    "## 1. Distribution Analysis Plots\n",
    "Generate violin plots for different experiment types:\n",
    "\n",
    "Collaboration Score for Homogenous and Robustness experiments. Payoff per Agent for Heterogenous experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TE59yu8T6HD"
   },
   "outputs": [],
   "source": [
    "# Ensure vectorized rendering\n",
    "mpl.rcParams['savefig.format'] = 'pdf'\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "def plot_violin(df, metric, title, output_pdf, plot_mean_line=True, show_legend=False):\n",
    "    \"\"\"\n",
    "    Generate violin plots showing distribution of collaboration scores or payoffs.\n",
    "    Features:\n",
    "    - Orders stories by mean performance (low to high)\n",
    "    - Color-codes baseline vs meaningful stories\n",
    "    - Overlays mean trend line\n",
    "    - Saves as PDF\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(f\"No data to visualize for {title}\")\n",
    "        return\n",
    "\n",
    "    # Compute x-axis order based on mean metric values\n",
    "    order = (df.groupby(\"PromptType\")[metric]\n",
    "            .mean()\n",
    "            .sort_values(ascending=True)\n",
    "            .index.tolist())\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "\n",
    "    # Define color palette\n",
    "    palette = {cat: COLOR_DICT.get(cat, \"#888888\") for cat in order}\n",
    "\n",
    "    # Violin plot with embedded box plot\n",
    "    ax = sns.violinplot(\n",
    "        data=df,\n",
    "        x=\"PromptType\",\n",
    "        y=metric,\n",
    "        hue=\"PromptType\",\n",
    "        palette=palette,\n",
    "        inner=\"box\",\n",
    "        dodge=False,\n",
    "        order=order,\n",
    "        bw_adjust=5, # Adjusting KDE bandwidth\n",
    "        scale=\"area\", # Uniform width across all violins\n",
    "    )\n",
    "\n",
    "    if ax.get_legend():\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "    # Overlay scatter points\n",
    "    sns.stripplot(\n",
    "        data=df,\n",
    "        x=\"PromptType\",\n",
    "        y=metric,\n",
    "        color=\"black\",\n",
    "        dodge=False,\n",
    "        alpha=0.2,\n",
    "        size=2,\n",
    "        zorder=2,\n",
    "        order=order\n",
    "    )\n",
    "\n",
    "    # (Optional) Plot mean trend line\n",
    "    if plot_mean_line:\n",
    "        means = df.groupby(\"PromptType\")[metric].mean().loc[order]\n",
    "        x_positions = list(range(len(order)))\n",
    "        plt.plot(\n",
    "            x_positions, means.values, marker='o',\n",
    "            color='black', linestyle='-', linewidth=2,\n",
    "            markersize=6, alpha=0.5, label=\"Mean Trend\"\n",
    "        )\n",
    "        if show_legend:\n",
    "            plt.legend([\"Mean Trend\"])\n",
    "\n",
    "    # Customize plot\n",
    "    if metric == \"CollaborationScore\":\n",
    "        plt.ylim(0, 1.3)  # Set range for Collaboration Score\n",
    "    elif metric == \"CumulativePayoff\":\n",
    "        plt.ylim(0, 120) #Set range for Cumulative Payoff\n",
    "    plt.xlabel(\"Story Prompt\", fontsize=18, labelpad=15)\n",
    "\n",
    "    ylabel_text = \"Payoff per Agent\" if metric == \"CumulativePayoff\" else \"Collaboration Score\"\n",
    "\n",
    "    plt.ylabel(f\"{ylabel_text}\", fontsize=18, labelpad=15)\n",
    "    plt.title(title, fontsize=20, weight=\"bold\" , pad=20)\n",
    "    plt.xticks(rotation=90 if len(order) > 5 else 0, fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "    sns.despine()\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Save the plot as Pdf\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_pdf, bbox_inches='tight', format='pdf', transparent=False)  # Fully vectorized PDF\n",
    "    print(f\"Figure saved as {output_pdf}\")\n",
    "\n",
    "# Generate plots\n",
    "for category, pattern in CATEGORIES_FOR_VIOLIN.items():\n",
    "    agent_count = category.split(\"_\")[-2]  # Extract agent count dynamically\n",
    "\n",
    "    if \"different_story\" in category:\n",
    "        # Different story -> Cumulative Payoff\n",
    "        csv_files = glob.glob(pattern)\n",
    "        for csv_file in csv_files:\n",
    "            vis_dir = get_visualization_directory(\"different_story\")\n",
    "            output_filename = f\"cumulative_payoffs_{category}.pdf\"\n",
    "            output_path = vis_dir / output_filename\n",
    "\n",
    "            df = preprocess_data(pd.read_csv(csv_file), \"CumulativePayoff\")\n",
    "            title = f\"Heterogenous Experiment\"\n",
    "            plot_violin(df, \"CumulativePayoff\", title, str(output_path))\n",
    "    else:\n",
    "        # Same story & bad apple -> Collaboration Score\n",
    "        df = load_csv_files(pattern)\n",
    "        df = preprocess_data(df, \"CollaborationScore\")\n",
    "        if df is not None:\n",
    "            exp_type = \"same_story\" if \"same_story\" in category else \"bad_apple\"\n",
    "            vis_dir = get_visualization_directory(exp_type)\n",
    "            output_filename = f\"collaboration_violin_{category}.pdf\"\n",
    "            output_path = vis_dir / output_filename\n",
    "\n",
    "            if \"bad_apple\" in category:\n",
    "                title = f\"Robustness\"\n",
    "            else:\n",
    "                title = f\"Homogenous Experiment\"\n",
    "\n",
    "            plot_violin(df, \"CollaborationScore\", title, str(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJMoYntdT7nh"
   },
   "source": [
    "## 2. Scaling Analysis: Agent Size Effects\n",
    "Analyze how cooperation changes with group size (4→16→32 agents).\n",
    "\n",
    "Shows trajectory of each story's performance as groups scale up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-PGUvp-T9Rz"
   },
   "outputs": [],
   "source": [
    "def process_category_scaling(temp_label, CATEGORIES):\n",
    "    \"\"\"\n",
    "    Analyze how cooperation changes with group size (scaling experiment).\n",
    "    Creates trajectory plot showing each story's performance across agent sizes.\n",
    "    Reveals whether cooperation patterns are robust to group scaling.\n",
    "    \"\"\"\n",
    "    story_scores = {}\n",
    "\n",
    "    # Load CSV data and extract mean collaboration scores\n",
    "    for agent_count, pattern in CATEGORIES.items():\n",
    "\n",
    "        df = load_csv_files(pattern)\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "\n",
    "        # Compute mean collaboration scores per story\n",
    "        story_means = df.groupby(\"PromptType\")[\"CollaborationScore\"].mean()\n",
    "        story_scores[agent_count] = story_means.to_dict()  # Store scores\n",
    "\n",
    "    # Extract the order of stories as they appear at N=4 in ascending order\n",
    "    starting_order = sorted(story_scores[4].items(), key=lambda x: x[1])\n",
    "    starting_stories = [story for story, _ in starting_order]\n",
    "\n",
    "    print(f\"Processing {temp_label}: starting_stories = {starting_stories}\")\n",
    "\n",
    "    # Dynamically assign colors based on order in starting_stories\n",
    "    COLOR_DICT_LOCAL = {}\n",
    "    blue_idx, pink_idx = 0, 0  # Track index for blue and pink shades\n",
    "\n",
    "    for story in starting_stories:\n",
    "        if story in [\"noinstruct\", \"nsCarrot\", \"nsPlumber\", \"maxreward\"]:  # Baseline stories\n",
    "            COLOR_DICT_LOCAL[story] = BLUE_SHADES[blue_idx]\n",
    "            blue_idx += 1  # Move to next darker shade\n",
    "        else:  # Meaningful stories\n",
    "            COLOR_DICT_LOCAL[story] = PINK_SHADES[pink_idx]\n",
    "            pink_idx += 1  # Move to next darker shade\n",
    "\n",
    "    # Plot settings\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    legend_handles = []\n",
    "\n",
    "    # Plot each story's progression across agent sizes\n",
    "    for story in COLOR_DICT_LOCAL.keys():  # Only plot stories in COLOR_DICT_LOCAL\n",
    "        positions = []\n",
    "\n",
    "        for agent_count in AGENT_SIZES:\n",
    "            if agent_count in story_scores and story in story_scores[agent_count]:\n",
    "                x_pos = story_scores[agent_count][story]\n",
    "                y_pos = AGENT_SIZES.index(agent_count)\n",
    "                positions.append((x_pos, y_pos))\n",
    "\n",
    "                # Scatter plot for each point\n",
    "                plt.scatter(x_pos, y_pos, s=70, facecolors=\"none\", edgecolors=COLOR_DICT_LOCAL[story], linewidths=1.5, label=story if agent_count == 4 else \"\")\n",
    "\n",
    "        if len(positions) > 1:\n",
    "            x_vals, y_vals = zip(*positions)\n",
    "            plt.plot(x_vals, y_vals, linestyle=\"dashed\", color=COLOR_DICT_LOCAL[story], alpha=0.7)\n",
    "\n",
    "    for story in starting_stories:\n",
    "        legend_handles.append(\n",
    "            mlines.Line2D(\n",
    "                [], [], marker=\"o\", linestyle=\"None\", markersize=8, color=COLOR_DICT_LOCAL.get(story, \"#888888\"), label=story\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Customizing plot\n",
    "    plt.xlabel(\"Mean Collaboration Score\", fontsize=18, labelpad=15)\n",
    "    plt.ylabel(\"Agent Size\", fontsize=18, labelpad=15)\n",
    "    plt.yticks(range(len(AGENT_SIZES)), [f\"N = {n}\" for n in AGENT_SIZES], fontsize=14, weight=\"bold\")\n",
    "    plt.title(f\"Scaling Experiment\", fontsize=20, weight=\"bold\", pad=20)\n",
    "    plt.grid(axis=\"y\", linestyle=\"dotted\")\n",
    "\n",
    "    plt.legend(\n",
    "        handles=legend_handles, title=\"Story\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=12\n",
    "    )\n",
    "\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    vis_dir = get_visualization_directory(\"same_story\")\n",
    "    filename = f\"scaling_experiment_collab_score.pdf\"\n",
    "    output_path = vis_dir / filename\n",
    "    plt.savefig(str(output_path), bbox_inches=\"tight\", format=\"pdf\")\n",
    "\n",
    "    print(f\"Scaling experiment figure saved as {output_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# Run the process for each category\n",
    "for temp_label, category_dict in CATEGORY_GROUPS.items():\n",
    "    process_category_scaling(temp_label, category_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GL4-tvST_Cx"
   },
   "source": [
    "## 3. Summary Statistics Table\n",
    "Generate a LaTeX-formatted table of mean ± std for final Collaboration Scores and Cumulative Payoffs across all story prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tCqQ-hcUAx9"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import statistics\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_agent_size(pattern):\n",
    "    import re\n",
    "    match = re.search(r'ag(\\d+)', pattern)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return \"Unknown\"\n",
    "\n",
    "# File patterns\n",
    "AGENT_SIZES = [4, 16, 32]\n",
    "BASELINE_STORIES = [\"noinstruct\", \"nsCarrot\", \"maxreward\", \"nsPlumber\"]\n",
    "MEANINGFUL_STORIES = [\"OldManSons\", \"Odyssey\", \"Soup\", \"Peacemaker\",\"Musketeers\", \"Teamwork\", \"Spoons\", \"Turnip\"]\n",
    "\n",
    "VISUALISATION_EXPERIMENTS = {\n",
    "    \"same_story\": dict(\n",
    "        sizes=AGENT_SIZES,\n",
    "        pattern=\"experiments/single_pool/same_story/game_results_single_pool_same_story_*_ag{N}_ro5_end10_mult1.5.csv\"\n",
    "    ),\n",
    "    \"different_story\": dict(\n",
    "        sizes=[4],\n",
    "        pattern=\"experiments/single_pool/different_story/game_results_single_pool_different_story_ag{N}_ro5_end10_mult1.5.csv\"\n",
    "    ),\n",
    "    \"bad_apple\": dict(\n",
    "        sizes=[4],\n",
    "        pattern=\"experiments/single_pool/bad_apple/game_results_single_pool_bad_apple_*_ag{N}_ro5_end10_mult1.5.csv\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "TABLE_KEYS = {\n",
    "    \"same_story\": \"homogeneous\",\n",
    "    \"different_story\": \"heterogeneous\",\n",
    "    \"bad_apple\": \"robustness\",\n",
    "}\n",
    "\n",
    "CATEGORIES_FOR_TABLE = {\n",
    "    TABLE_KEYS[name]: [\n",
    "        VISUALISATION_EXPERIMENTS[name][\"pattern\"].format(N=N)\n",
    "        for N in VISUALISATION_EXPERIMENTS[name][\"sizes\"]\n",
    "    ]\n",
    "    for name in VISUALISATION_EXPERIMENTS\n",
    "}\n",
    "\n",
    "# Initialize scores dictionary\n",
    "COLLAB_SCORES = {story: {\"Homogeneous_4\": None, \"Homogeneous_16\": None, \"Homogeneous_32\": None,\n",
    "                        \"Robustness_4\": None, \"Heterogeneous_4\": None}\n",
    "                for story in BASELINE_STORIES + MEANINGFUL_STORIES}\n",
    "\n",
    "# Data processing functions\n",
    "def load_csv_files(pattern):\n",
    "    \"\"\"Loads all CSV files matching a pattern and merges them into a DataFrame.\"\"\"\n",
    "    files = glob.glob(pattern)\n",
    "    if not files:\n",
    "        print(f\"No files found for pattern: {pattern}\")\n",
    "        return pd.DataFrame()\n",
    "    return pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
    "\n",
    "def preprocess_data(df, metric):\n",
    "    \"\"\"Prepares data by filtering and converting columns to numeric types.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "\n",
    "    if metric == \"CollaborationScore\":\n",
    "        df = df[df[\"Round\"] == \"final\"].copy()\n",
    "    else:  # \"CumulativePayoff\"\n",
    "        df = df[df[\"Round\"] != \"final\"].copy()\n",
    "        df[\"Round\"] = pd.to_numeric(df[\"Round\"], errors=\"coerce\")\n",
    "        df.dropna(subset=[\"Round\"], inplace=True)\n",
    "        df = df.loc[df.groupby([\"Game\", \"AgentName\"])[\"Round\"].idxmax()].copy()\n",
    "\n",
    "    df[metric] = pd.to_numeric(df[metric], errors=\"coerce\")\n",
    "    df.dropna(subset=[metric], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Process experiment data\n",
    "print(\"Processing data for summary statistics table...\")\n",
    "data_found = False\n",
    "\n",
    "for category, patterns in CATEGORIES_FOR_TABLE.items():\n",
    "    for pattern in patterns:\n",
    "        print(f\"Checking pattern: {pattern}\")\n",
    "\n",
    "        df = load_csv_files(pattern)\n",
    "        if df is None or df.empty:\n",
    "            print(f\"No data found for pattern: {pattern}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Loaded {len(df)} rows\")\n",
    "        data_found = True\n",
    "\n",
    "        agent_size = extract_agent_size(pattern)\n",
    "        print(f\"Agent size extracted: '{agent_size}'\")\n",
    "\n",
    "        if category in (\"homogeneous\", \"robustness\"):\n",
    "            metric = \"CollaborationScore\"\n",
    "        else:\n",
    "            metric = \"CumulativePayoff\"\n",
    "\n",
    "        df_proc = preprocess_data(df, metric)\n",
    "        if df_proc is None or df_proc.empty:\n",
    "            print(f\"No data after preprocessing\")\n",
    "            continue\n",
    "\n",
    "        stats = df_proc.groupby(\"PromptType\")[metric].agg(mean=\"mean\", std=\"std\").reset_index()\n",
    "        column_key = f\"{category.capitalize()}_{agent_size}\"\n",
    "        print(f\"Mapping to column: {column_key}\")\n",
    "\n",
    "        for _, row in stats.iterrows():\n",
    "            story = row[\"PromptType\"]\n",
    "            if story in COLLAB_SCORES:\n",
    "                mean_val = row['mean']\n",
    "                std_val = row['std']\n",
    "                if std_val < 0.01:\n",
    "                    formatted = f\"{mean_val:.4f} ± {std_val:.4f}\"\n",
    "                else:\n",
    "                    formatted = f\"{mean_val:.2f} ± {std_val:.2f}\"\n",
    "                COLLAB_SCORES[story][column_key] = formatted\n",
    "                print(f\"Set {story} -> {column_key} = {formatted}\")\n",
    "\n",
    "# Generate LaTeX table if data found\n",
    "if data_found:\n",
    "    print(\"Generating LaTeX summary table...\")\n",
    "\n",
    "    latex_output = \"\"\"\\\\begin{table*}[t]\n",
    "        \\\\centering\n",
    "        \\\\caption{Mean ± standard deviation of final Collaboration Scores (for homogeneous and robustness agents) and final Cumulative Payoffs (for heterogeneous agents) across all story prompts. Values are shown with higher decimal precision where variation is small, to reflect statistically meaningful differences observed in pairwise confidence intervals.}\n",
    "        \\\\setlength{\\\\tabcolsep}{8pt}\n",
    "        \\\\renewcommand{\\\\arraystretch}{1.3}\n",
    "        \\\\fontsize{11pt}{13pt}\\\\selectfont\n",
    "        \\\\resizebox{\\\\textwidth}{!}{\n",
    "        \\\\begin{tabular}{llccccc}\n",
    "            \\\\toprule\n",
    "            \\\\multirow{2}{*}{\\\\textbf{Story Type}} & \\\\multirow{2}{*}{\\\\textbf{Story Prompt}} & \\\\multicolumn{3}{c}{\\\\textbf{Homogeneous Agents}} & \\\\textbf{Robustness} & \\\\textbf{Heterogeneous} \\\\\\\\\n",
    "            \\\\cmidrule(lr){3-5} \\\\cmidrule(lr){6-6} \\\\cmidrule(lr){7-7}\n",
    "            & & \\\\textbf{N=4} & \\\\textbf{N=16} & \\\\textbf{N=32} & \\\\textbf{N=4} & \\\\textbf{N=4} \\\\\\\\\n",
    "            \\\\midrule\n",
    "    \"\"\"\n",
    "\n",
    "    # Add Baseline Stories Section\n",
    "    latex_output += \"        \\\\multirow{4}{*}{\\\\centering \\\\textbf{Baseline Stories}}  \\n\"\n",
    "    for story in BASELINE_STORIES:\n",
    "        row_data = [COLLAB_SCORES[story].get(col, \"N/A\") or \"N/A\" for col in [\"Homogeneous_4\", \"Homogeneous_16\", \"Homogeneous_32\", \"Robustness_4\", \"Heterogeneous_4\"]]\n",
    "        latex_output += f\"        & {story}  & {' & '.join(row_data)} \\\\\\\\\\n\"\n",
    "\n",
    "    latex_output += \"        \\\\midrule\\n\"\n",
    "\n",
    "    # Add Meaningful Stories Section\n",
    "    latex_output += \"        \\\\multirow{8}{*}{\\\\centering \\\\textbf{Meaningful Stories}}  \\n\"\n",
    "    for story in MEANINGFUL_STORIES:\n",
    "        row_data = [COLLAB_SCORES[story].get(col, \"N/A\") or \"N/A\" for col in [\"Homogeneous_4\", \"Homogeneous_16\", \"Homogeneous_32\", \"Robustness_4\", \"Heterogeneous_4\"]]\n",
    "        latex_output += f\"        & {story}  & {' & '.join(row_data)} \\\\\\\\\\n\"\n",
    "\n",
    "    latex_output += \"\"\"        \\\\bottomrule\n",
    "        \\\\end{tabular}\n",
    "        }\n",
    "        \\\\label{tab:all_agents_scores}\n",
    "    \\\\end{table*}\n",
    "    \"\"\"\n",
    "\n",
    "    print(latex_output)\n",
    "\n",
    "    # Save to visualization directory\n",
    "    vis_dir = Path(\"experiments\") / \"vis\" / \"single_pool\" / \"same_story\"\n",
    "    vis_dir.mkdir(parents=True, exist_ok=True)\n",
    "    table_path = vis_dir / \"all_agents_table.tex\"\n",
    "    with open(table_path, \"w\") as f:\n",
    "        f.write(latex_output)\n",
    "    print(f\"LaTeX table saved to: {table_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"No experimental data found. Run experiments first before generating summary table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVxwfUkMUCiu"
   },
   "source": [
    "# Multi-Pool Experiments – Visualisation\n",
    "Visualize multi-pool results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOQaf9BbUEMY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "from pathlib import Path\n",
    "\n",
    "# Story colors\n",
    "COLOR_DICT = {\n",
    "    \"maxreward\": \"#87CEFA\",\n",
    "    \"noinstruct\": \"#4682B4\",\n",
    "    \"nsCarrot\": \"#4169E1\",\n",
    "    \"nsPlumber\": \"#27408B\",\n",
    "    \"Odyssey\": \"#FFB3E6\",\n",
    "    \"Soup\": \"#FF99CC\",\n",
    "    \"Peacemaker\": \"#FF66B3\",\n",
    "    \"Musketeers\": \"#FF4D9E\",\n",
    "    \"Teamwork\": \"#F02278\",\n",
    "    \"Spoons\": \"#D81B60\",\n",
    "    \"OldManSons\": \"#B22272\",\n",
    "    \"Turnip\": \"#B83B7D\"\n",
    "}\n",
    "\n",
    "\n",
    "# SAME STORY: Violin Plot (Collaboration Scores by Story)\n",
    "\n",
    "def plot_same_story_violin_collaboration():\n",
    "    \"\"\"\n",
    "    Violin plot of collaboration scores by story for multi-pool same story experiments.\n",
    "    Shows how different stories perform when agents must allocate contributions\n",
    "    across both global and local pools.\n",
    "    \"\"\"\n",
    "    pattern = \"experiments/multi_pool/same_story/game_results_multi_pool_2_same_story_*_ag4_ro5_end10_mult1.5.csv\"\n",
    "    csv_files = glob.glob(pattern)\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"No files found for pattern: {pattern}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(csv_files)} files for multi-pool same story\")\n",
    "\n",
    "    # Combine all CSV files\n",
    "    df_list = []\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df_list.append(df)\n",
    "\n",
    "    df_all = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Filter to final rows and get collaboration scores\n",
    "    df_final = df_all[df_all[\"Round\"] == \"final\"].copy()\n",
    "    df_final[\"CollaborationScore\"] = pd.to_numeric(df_final[\"CollaborationScore\"], errors=\"coerce\")\n",
    "    df_final.dropna(subset=[\"CollaborationScore\"], inplace=True)\n",
    "\n",
    "    # Calculate means and order\n",
    "    means = df_final.groupby(\"PromptType\")[\"CollaborationScore\"].mean()\n",
    "    order = means.sort_values(ascending=True).index.tolist()\n",
    "\n",
    "    # Create violin plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    sns.violinplot(\n",
    "        data=df_final,\n",
    "        x=\"PromptType\",\n",
    "        y=\"CollaborationScore\",\n",
    "        order=order,\n",
    "        palette=[COLOR_DICT.get(s, \"#888888\") for s in order],\n",
    "        inner=\"box\",\n",
    "        scale=\"area\",\n",
    "        bw_adjust=5,\n",
    "        cut=2\n",
    "    )\n",
    "\n",
    "    # Overlay mean trend line\n",
    "    x_positions = np.arange(len(order))\n",
    "    plt.plot(\n",
    "        x_positions,\n",
    "        means.loc[order].values,\n",
    "        marker=\"o\",\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2,\n",
    "        alpha=0.5,\n",
    "        color=\"black\"\n",
    "    )\n",
    "\n",
    "    plt.title(\"Multi-Pool Homogeneous\\n\", fontsize=24, fontweight=\"bold\")\n",
    "    plt.ylabel(\"Collaboration Score\", fontsize=22)\n",
    "    plt.xlabel(\"Story Prompt\", fontsize=22, labelpad=16)\n",
    "    plt.xticks(rotation=90, ha=\"center\", fontsize=22)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(0, 1.0)\n",
    "\n",
    "    sns.despine()\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    output_dir = Path(\"experiments\") / \"vis\" / \"multi_pool\" / \"same_story\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = output_dir / \"multipool_same_story_violin_collaboration.pdf\"\n",
    "    plt.savefig(str(out_path), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(f\"→ Saved same story violin plot to {out_path}\")\n",
    "\n",
    "\n",
    "# SAME STORY: Scatter Plot (Collaboration Score vs Global Pool Allocation)\n",
    "\n",
    "def plot_same_story_scatter_collaboration():\n",
    "    \"\"\"Scatter plot of collaboration score vs global pool allocation for multi-pool same story.\"\"\"\n",
    "    pattern = \"experiments/multi_pool/same_story/game_results_multi_pool_2_same_story_*_ag4_ro5_end10_mult1.5.csv\"\n",
    "    csv_files = glob.glob(pattern)\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"No files found for pattern: {pattern}\")\n",
    "        return\n",
    "\n",
    "    df_list = []\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df_list.append(df)\n",
    "\n",
    "    df_all = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Calculate collaboration scores (from final rows)\n",
    "    df_final = df_all[df_all[\"Round\"] == \"final\"].copy()\n",
    "    collab_scores = df_final.groupby(\"PromptType\")[\"CollaborationScore\"].mean()\n",
    "\n",
    "    # Calculate global pool proportions (from non-final rows)\n",
    "    df_rounds = df_all[df_all[\"Round\"] != \"final\"].copy()\n",
    "    df_rounds[\"GlobalContrib\"] = pd.to_numeric(df_rounds[\"GlobalContrib\"], errors=\"coerce\")\n",
    "    df_rounds[\"TotalContrib\"] = pd.to_numeric(df_rounds[\"TotalContrib\"], errors=\"coerce\")\n",
    "\n",
    "    global_props = {}\n",
    "    for story in collab_scores.index:\n",
    "        story_data = df_rounds[df_rounds[\"PromptType\"] == story]\n",
    "        if len(story_data) > 0:\n",
    "            story_data[\"GlobalProp\"] = story_data[\"GlobalContrib\"] / story_data[\"TotalContrib\"]\n",
    "            story_data[\"GlobalProp\"] = story_data[\"GlobalProp\"].fillna(0.5)\n",
    "            global_props[story] = story_data[\"GlobalProp\"].mean()\n",
    "\n",
    "    # Filter to stories with both metrics\n",
    "    valid_stories = [s for s in collab_scores.index if s in global_props]\n",
    "    xs = [global_props[s] for s in valid_stories]\n",
    "    ys = [collab_scores[s] for s in valid_stories]\n",
    "\n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    colors = [COLOR_DICT.get(s, \"#888888\") for s in valid_stories]\n",
    "    plt.scatter(xs, ys, s=120, c=colors, edgecolors=\"white\", linewidths=1.5, alpha=1.0)\n",
    "    plt.axvline(0.5, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "    # Create legend\n",
    "    legend_elems = [\n",
    "        Line2D([0], [0], marker=\"o\", color=\"w\", label=s,\n",
    "               markerfacecolor=COLOR_DICT.get(s, \"#888888\"),\n",
    "               markeredgecolor=\"white\", markersize=10)\n",
    "        for s in valid_stories\n",
    "    ]\n",
    "    plt.legend(handles=legend_elems, bbox_to_anchor=(1.05, 1), loc=\"upper left\",\n",
    "               title=\"Story\", fontsize=18, title_fontsize=20, frameon=True)\n",
    "\n",
    "    plt.title(\"Multi-Pool Homogeneous\\nCollaboration Score vs Global-Pool Contribution Fraction\",\n",
    "              fontsize=20, fontweight=\"bold\", pad=20)\n",
    "    plt.xlabel(\"Proportion of Contributions to Global Pool\", fontsize=22)\n",
    "    plt.ylabel(\"Collaboration Score\", fontsize=22)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.grid(ls=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    output_dir = Path(\"experiments\") / \"vis\" / \"multi_pool\" / \"same_story\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = output_dir / \"multipool_same_story_scatter_collab_vs_global.pdf\"\n",
    "    plt.savefig(str(out_path), dpi=300, bbox_inches=\"tight\", pad_inches=0.2)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(f\"→ Saved same story scatter plot to {out_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# DIFFERENT STORY: Violin Plot (Payoffs by Story)\n",
    "\n",
    "def plot_different_story_violin_payoff():\n",
    "    \"\"\"Violin plot of payoffs by story for multi-pool different story experiments.\"\"\"\n",
    "    pattern = \"experiments/multi_pool/different_story/game_results_multi_pool_2_different_story_ag4_ro5_end10_mult1.5.csv\"\n",
    "    csv_files = glob.glob(pattern)\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"No files found for pattern: {pattern}\")\n",
    "        return\n",
    "\n",
    "    df_all = pd.read_csv(csv_files[0])\n",
    "\n",
    "    # Filter to non-final rounds to get cumulative payoffs\n",
    "    df_rounds = df_all[df_all[\"Round\"] != \"final\"].copy()\n",
    "    df_rounds[\"CumulativePayoff\"] = pd.to_numeric(df_rounds[\"CumulativePayoff\"], errors=\"coerce\")\n",
    "    df_rounds.dropna(subset=[\"CumulativePayoff\"], inplace=True)\n",
    "\n",
    "    # Get final payoffs (max round per agent per game)\n",
    "    df_final_payoffs = df_rounds.loc[df_rounds.groupby([\"Game\", \"AgentName\"])[\"Round\"].idxmax()].copy()\n",
    "\n",
    "    # Remove 'All' if it exists\n",
    "    df_final_payoffs = df_final_payoffs[df_final_payoffs[\"PromptType\"] != \"All\"]\n",
    "\n",
    "    # Calculate means and order\n",
    "    means = df_final_payoffs.groupby(\"PromptType\")[\"CumulativePayoff\"].mean()\n",
    "    order = means.sort_values(ascending=True).index.tolist()\n",
    "\n",
    "    # Create violin plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    sns.violinplot(\n",
    "        data=df_final_payoffs,\n",
    "        x=\"PromptType\",\n",
    "        y=\"CumulativePayoff\",\n",
    "        order=order,\n",
    "        palette=[COLOR_DICT.get(s, \"#888888\") for s in order],\n",
    "        inner=\"box\",\n",
    "        scale=\"area\",\n",
    "        bw_adjust=5,\n",
    "        cut=2\n",
    "    )\n",
    "\n",
    "    # Overlay mean trend line\n",
    "    x_positions = np.arange(len(order))\n",
    "    plt.plot(\n",
    "        x_positions,\n",
    "        means.loc[order].values,\n",
    "        marker=\"o\",\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2,\n",
    "        alpha=0.5,\n",
    "        color=\"black\"\n",
    "    )\n",
    "\n",
    "    plt.title(\"Multi-Pool Heterogeneous\\n\", fontsize=24, fontweight=\"bold\")\n",
    "    plt.ylabel(\"Average Cumulative Payoff\", fontsize=22)\n",
    "    plt.xlabel(\"Story Prompt\", fontsize=22, labelpad=16)\n",
    "    plt.xticks(rotation=90, ha=\"center\", fontsize=22)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(0, 200)  # Adjust based on your payoff range\n",
    "\n",
    "    sns.despine()\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    output_dir = Path(\"experiments\") / \"vis\" / \"multi_pool\" / \"different_story\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = output_dir / \"multipool_different_story_violin_payoff.pdf\"\n",
    "    plt.savefig(str(out_path), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(f\"→ Saved different story violin plot to {out_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# DIFFERENT STORY: Scatter Plot (Payoff vs Global Pool Allocation)\n",
    "\n",
    "def plot_different_story_scatter_payoff():\n",
    "    \"\"\"Scatter plot of payoff vs global pool allocation for multi-pool different story.\"\"\"\n",
    "    pattern = \"experiments/multi_pool/different_story/game_results_multi_pool_2_different_story_ag4_ro5_end10_mult1.5.csv\"\n",
    "    csv_files = glob.glob(pattern)\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"No files found for pattern: {pattern}\")\n",
    "        return\n",
    "\n",
    "    df_all = pd.read_csv(csv_files[0])\n",
    "\n",
    "    # Calculate payoffs (from non-final rounds, get final cumulative payoff per agent)\n",
    "    df_rounds = df_all[df_all[\"Round\"] != \"final\"].copy()\n",
    "    df_final_payoffs = df_rounds.loc[df_rounds.groupby([\"Game\", \"AgentName\"])[\"Round\"].idxmax()].copy()\n",
    "    df_final_payoffs = df_final_payoffs[df_final_payoffs[\"PromptType\"] != \"All\"]\n",
    "\n",
    "    # Calculate mean payoffs by story\n",
    "    payoff_means = df_final_payoffs.groupby(\"PromptType\")[\"CumulativePayoff\"].mean()\n",
    "\n",
    "    # Calculate global pool proportions (from all non-final rounds)\n",
    "    df_rounds[\"GlobalContrib\"] = pd.to_numeric(df_rounds[\"GlobalContrib\"], errors=\"coerce\")\n",
    "    df_rounds[\"TotalContrib\"] = pd.to_numeric(df_rounds[\"TotalContrib\"], errors=\"coerce\")\n",
    "    df_rounds = df_rounds[df_rounds[\"PromptType\"] != \"All\"]\n",
    "\n",
    "    global_props = {}\n",
    "    for story in payoff_means.index:\n",
    "        story_data = df_rounds[df_rounds[\"PromptType\"] == story]\n",
    "        if len(story_data) > 0:\n",
    "            story_data[\"GlobalProp\"] = story_data[\"GlobalContrib\"] / story_data[\"TotalContrib\"]\n",
    "            story_data[\"GlobalProp\"] = story_data[\"GlobalProp\"].fillna(0.5)\n",
    "            global_props[story] = story_data[\"GlobalProp\"].mean()\n",
    "\n",
    "    # Filter to stories with both metrics\n",
    "    valid_stories = [s for s in payoff_means.index if s in global_props]\n",
    "    xs = [global_props[s] for s in valid_stories]\n",
    "    ys = [payoff_means[s] for s in valid_stories]\n",
    "\n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    colors = [COLOR_DICT.get(s, \"#888888\") for s in valid_stories]\n",
    "    plt.scatter(xs, ys, s=120, c=colors, edgecolors=\"white\", linewidths=1.5, alpha=1.0)\n",
    "    plt.axvline(0.5, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "    # Create legend\n",
    "    legend_elems = [\n",
    "        Line2D([0], [0], marker=\"o\", color=\"w\", label=s,\n",
    "               markerfacecolor=COLOR_DICT.get(s, \"#888888\"),\n",
    "               markeredgecolor=\"white\", markersize=10)\n",
    "        for s in valid_stories\n",
    "    ]\n",
    "    plt.legend(handles=legend_elems, bbox_to_anchor=(1.05, 1), loc=\"upper left\",\n",
    "               title=\"Story\", fontsize=18, title_fontsize=20, frameon=True)\n",
    "\n",
    "    plt.title(\"Multi-Pool Heterogeneous\\nPayoff vs Global-Pool Contribution Fraction\",\n",
    "              fontsize=20, fontweight=\"bold\", pad=20)\n",
    "    plt.xlabel(\"Proportion of Contributions to Global Pool\", fontsize=22)\n",
    "    plt.ylabel(\"Average Cumulative Payoff\", fontsize=22)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, max(ys) * 1.1 if ys else 100)  # Dynamic y-limit\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.grid(ls=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    output_dir = Path(\"experiments\") / \"vis\" / \"multi_pool\" / \"different_story\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = output_dir / \"multipool_different_story_scatter_payoff_vs_global.pdf\"\n",
    "    plt.savefig(str(out_path), dpi=300, bbox_inches=\"tight\", pad_inches=0.2)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(f\"→ Saved different story scatter plot to {out_path}\")\n",
    "\n",
    "\n",
    "# Run all 4 visualisastions\n",
    "\n",
    "print(\"Generating ALL multi-pool visualizations...\")\n",
    "\n",
    "print(\"\\\\n=== 1. Same Story: Violin Plot (Collaboration Scores) ===\")\n",
    "plot_same_story_violin_collaboration()\n",
    "\n",
    "print(\"\\\\n=== 2. Same Story: Scatter Plot (Collaboration vs Global Pool) ===\")\n",
    "plot_same_story_scatter_collaboration()\n",
    "\n",
    "print(\"\\\\n=== 3. Different Story: Violin Plot (Payoffs) ===\")\n",
    "plot_different_story_violin_payoff()\n",
    "\n",
    "print(\"\\\\n=== 4. Different Story: Scatter Plot (Payoff vs Global Pool) ===\")\n",
    "plot_different_story_scatter_payoff()\n",
    "\n",
    "print(\"\\\\nAll multi-pool visualizations completed!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOZ13+u9MQmmdq9nWSnkTWX",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llm_deliberation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
