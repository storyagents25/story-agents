{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install Required Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB88abH4UDBM"
      },
      "outputs": [],
      "source": [
        "# Install all required dependencies\n",
        "pip install -U langchain-community langchain_openai numpy pandas matplotlib seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1-trKEVJgOPH"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Global Settings and Imports\n",
        "# -----------------------------\n",
        "\n",
        "import os\n",
        "import time\n",
        "import requests  # Needed for api requests\n",
        "import pickle\n",
        "import random\n",
        "import statistics\n",
        "import csv\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import sys\n",
        "import glob\n",
        "import itertools\n",
        "import statistics\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "import matplotlib.lines as mlines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vOwokPWUnZv"
      },
      "source": [
        "# Load and Configure LLM Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqJOQxoqUnpn"
      },
      "outputs": [],
      "source": [
        "# Set your backend here: choose \"llama\" or \"openai\"\n",
        "LLM_CHOICE = \"llama\"\n",
        "\n",
        "# For OpenAI backend (if used)\n",
        "# SET YOUR OPENAI API KEY HERE\n",
        "OPENAI_API_KEY = \"YOUR_OPENAI_API_KEY\"\n",
        "LLAMA_API_URL = os.getenv(\"LLAMA_API_URL\")\n",
        "# -----------------------------\n",
        "# Llama Helper Class\n",
        "# -----------------------------\n",
        "# This class implements an LLM interface similar to ChatOpenAI,\n",
        "# but calls the llama endpoint via a POST request.\n",
        "class Llama:\n",
        "    def __init__(self, model_name, temperature, max_tokens, openai_api_key=None):\n",
        "        self.model_name = model_name\n",
        "        self.temperature = temperature\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "    def invoke(self, history):\n",
        "        # Build messages in a robust way.\n",
        "        messages = []\n",
        "        for msg in history:\n",
        "            # Try to extract role and content, whether msg is an object or dict.\n",
        "            if hasattr(msg, \"role\") and hasattr(msg, \"content\"):\n",
        "                role = msg.role\n",
        "                content = msg.content\n",
        "            elif isinstance(msg, dict):\n",
        "                role = msg.get(\"role\", \"unknown\")\n",
        "                content = msg.get(\"content\", str(msg))\n",
        "            else:\n",
        "                role = \"unknown\"\n",
        "                content = str(msg)\n",
        "            messages.append({\"role\": role, \"content\": content})\n",
        "\n",
        "        data = {\n",
        "            \"model\": self.model_name,\n",
        "            \"messages\": messages,\n",
        "            \"max_tokens\": self.max_tokens,\n",
        "            \"temperature\": self.temperature\n",
        "        }\n",
        "        response = requests.post(LLAMA_API_URL, json=data)\n",
        "        # Optionally add error checking for response status here\n",
        "        response_data = response.json()\n",
        "        content = response_data[\"choices\"][0][\"message\"][\"content\"]\n",
        "        from langchain.schema import AIMessage\n",
        "        return AIMessage(content=content)\n",
        "\n",
        "\n",
        "def log_records(message: str, records_file):\n",
        "    \"\"\"\n",
        "    Helper function to log a message to records file so we can keep track of all history.\n",
        "    \"\"\"\n",
        "    records_file.write(message + \"\\n\")\n",
        "    records_file.flush()  # make sure it's written immediately\n",
        "\n",
        "\n",
        "def calculate_payoffs(contributions, e, m, na):\n",
        "    \"\"\"\n",
        "    Given a list of contributions from each agent, compute the payoff for each agent.\n",
        "    \"\"\"\n",
        "    total = sum(contributions)\n",
        "    shared_bonus = m * total / na\n",
        "    return [e - c + shared_bonus for c in contributions]\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    A simple agent that uses an LLM backend determined by LLM_CHOICE.\n",
        "    It maintains its own conversation history so that previous rounds and summaries\n",
        "    can influence its responses.\n",
        "    \"\"\"\n",
        "    def __init__(self, name, system_message, records_file):\n",
        "        self.name = name\n",
        "        # Start the conversation with a system message.\n",
        "        self.history = [SystemMessage(content=system_message)]\n",
        "\n",
        "        # Log the creation of this agent and its system prompt to records.\n",
        "        log_records(f\"CREATING AGENT: {self.name}\", records_file)\n",
        "        log_records(f\"System Prompt for {self.name}: {system_message}\", records_file)\n",
        "        self.records_file = records_file\n",
        "\n",
        "\n",
        "        # Create the appropriate LLM instance\n",
        "        if LLM_CHOICE == \"llama\":\n",
        "            self.llm = Llama(\n",
        "            model_name=\"meta-llama-3.3-70b-instruct-fp8\",  # adjust as needed\n",
        "            temperature=0.6,\n",
        "            max_tokens=500,\n",
        "        )\n",
        "        else:\n",
        "            # Create one instance of ChatOpenAI per agent.\n",
        "            self.llm = ChatOpenAI(\n",
        "                model_name=\"gpt-4o-mini\",  # adjust as needed\n",
        "                temperature=1.0,\n",
        "                max_tokens=10,\n",
        "                openai_api_key=OPENAI_API_KEY\n",
        "            )\n",
        "\n",
        "    def chat(self, message: str) -> str:\n",
        "        \"\"\"\n",
        "        Append a human message, call the LLM, append the assistant's reply,\n",
        "        log everything, and return the response content.\n",
        "        \"\"\"\n",
        "        log_records(f\"{self.name} receives HUMAN message: {message}\", self.records_file)\n",
        "        self.history.append(HumanMessage(content=message))\n",
        "        # Use the pre-created ChatOpenAI instance.\n",
        "        response = self.llm.invoke(self.history)\n",
        "        # Store the response in the conversation history\n",
        "        self.history.append(response)\n",
        "        log_records(f\"{self.name} responds ASSISTANT: {response.content}\", self.records_file)\n",
        "        # Brief pause to help avoid rate limits\n",
        "        time.sleep(0.01)\n",
        "        return response.content\n",
        "\n",
        "class DummyAgent:\n",
        "    \"\"\"\n",
        "    A 'dummy' agent that does NOT connect to an LLM and always contributes 0.\n",
        "    \"\"\"\n",
        "    def __init__(self, name, system_message, records_file):\n",
        "        self.name = name\n",
        "        self.history = [SystemMessage(content=system_message)]\n",
        "        self.records_file = records_file\n",
        "\n",
        "        log_records(f\"CREATING DUMMY AGENT: {self.name}\", records_file)\n",
        "        log_records(f\"(Dummy) System Prompt for {self.name}: {system_message}\", records_file)\n",
        "        # No LLM needed.\n",
        "\n",
        "    def chat(self, message: str) -> str:\n",
        "        \"\"\"\n",
        "        This agent always contributes \"0\".\n",
        "        We still log the conversation but do not call any LLM.\n",
        "        \"\"\"\n",
        "        log_records(f\"{self.name} (dummy) receives HUMAN message: {message}\", self.records_file)\n",
        "        # We return \"0\" as a string to emulate the minimal integer-based response.\n",
        "        response_content = \"<TOKEN>0</TOKEN>\"\n",
        "        log_records(f\"{self.name} (dummy) responds ASSISTANT: {response_content}\", self.records_file)\n",
        "        time.sleep(0.01)  # a brief pause, mirroring the normal agent\n",
        "        return response_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FYu4yGHVSS0"
      },
      "source": [
        "# Define Agent Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hrOukVO1VScq"
      },
      "outputs": [],
      "source": [
        "def prepare_experiment(exp_name, csv_header):\n",
        "    \"\"\"Prepares output files for logging results.\"\"\"\n",
        "\n",
        "    records_file = open(f\"records_{exp_name}.txt\", \"w\", encoding=\"utf-8\")\n",
        "    game_results_file = open(f\"game_results_{exp_name}.csv\", \"w\", newline=\"\", encoding=\"utf-8\")\n",
        "    csv_writer = csv.writer(game_results_file)\n",
        "    csv_writer.writerow(csv_header)\n",
        "    return records_file, game_results_file, csv_writer\n",
        "\n",
        "def load_intermediate_results(exp_name, is_dict=True):\n",
        "    \"\"\"Loads intermediate results from a pickle file if it exists.\"\"\"\n",
        "    intermediate_results_filename = f\"results_{exp_name}.pkl\"\n",
        "    if os.path.exists(intermediate_results_filename):\n",
        "        with open(intermediate_results_filename, 'rb') as f:\n",
        "            intermediate_results = pickle.load(f)\n",
        "        print(f\"Loaded intermediate results from {intermediate_results_filename}.\")\n",
        "    else:\n",
        "        intermediate_results = {} if is_dict else []\n",
        "    return intermediate_results, intermediate_results_filename\n",
        "\n",
        "def save_intermediate_results(intermediate_results, intermediate_results_filename):\n",
        "    \"\"\"Saves intermediate results to a pickle file.\"\"\"\n",
        "    with open(intermediate_results_filename, 'wb') as f:\n",
        "        pickle.dump(intermediate_results, f)\n",
        "\n",
        "def get_system_prompt(na, e, m):\n",
        "    \"\"\"\n",
        "    Generates the system prompt.\n",
        "\n",
        "    Args:\n",
        "        na (int): Number of agents.\n",
        "        e (int): Endowment per round (tokens given to each agent).\n",
        "        m (float): Contribution multiplier affecting payoffs.\n",
        "\n",
        "    Returns:\n",
        "        str: The formatted system prompt.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        f\"You are participating in a repeated public goods game with {na - 1} other agents. \"\n",
        "        f\"In each round, you receive {e} tokens. You must decide how many tokens (an integer between 0 and {e}) \"\n",
        "        f\"to contribute to a public pool. After everyone contributes, the total contribution is multiplied by {m} \"\n",
        "        f\"and divided equally among all {na} agents. This means your payoff for a round is calculated as: \\n\\n\"\n",
        "        f\"    {e} - (your contribution) + ({m} * total contributions / {na})\\n\\n\"\n",
        "        \"Please respond **only** with the number of tokens you wish to contribute for this round, enclosed within `<TOKEN>` and `</TOKEN>` tags. \"\n",
        "        \"For example: `<TOKEN>5</TOKEN>`. Do not provide any additional text, explanations, or summaries. \"\n",
        "        \"STORY\"\n",
        "    )\n",
        "\n",
        "def compute_and_print_statistics(scores_by_prompt, exp_name):\n",
        "    \"\"\"Computes and prints mean and standard deviation of scores for each prompt.\"\"\"\n",
        "    print(f\"\\n=== Comparison of Effective Collaboration Scores Over Games for experiment {exp_name} ===\")\n",
        "    for prompt_label, score_list in scores_by_prompt.items():\n",
        "        mean_val = statistics.mean(score_list)\n",
        "        stdev_val = statistics.stdev(score_list) if len(score_list) > 1 else 0\n",
        "        print(f\"{prompt_label}: Mean = {mean_val:.2f}, SD = {stdev_val:.2f}\")\n",
        "\n",
        "def extract_contribution(response_str):\n",
        "    \"\"\"\n",
        "    Extracts a valid contribution from an agent's response.\n",
        "    - First, looks for a contribution enclosed in <TOKEN>...</TOKEN>.\n",
        "    - If found, extracts the first standalone integer from the match.\n",
        "    \"\"\"\n",
        "\n",
        "    # Look for <TOKEN>...</TOKEN> first\n",
        "    match = re.search(r\"<TOKEN>(\\d+)</TOKEN>\", response_str)\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_valid_contribution(agent, round_num, e, max_retries=5):\n",
        "    \"\"\"\n",
        "    Repeatedly queries the agent for a valid contribution up to max_retries.\n",
        "    If the response is invalid for more than max_retries, defaults to 0.\n",
        "    \"\"\"\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        prompt = f\"Round {round_num}: What is your contribution (0-{e})?\"\n",
        "\n",
        "        if retries > 0:\n",
        "            prompt += \" Your previous response was invalid. **Only provide a number inside `<TOKEN>...</TOKEN>`** with no extra text. Example: `<TOKEN>5</TOKEN>`.\"\n",
        "\n",
        "        response_str = agent.chat(prompt).strip()\n",
        "        print(f\"{agent.name} (Story: {agent.story_label}) response (attempt {retries + 1}): {response_str}\")\n",
        "\n",
        "        contribution = extract_contribution(response_str)\n",
        "\n",
        "        if contribution is not None:\n",
        "            return contribution\n",
        "\n",
        "        print(f\"Warning: {agent.name} provided an invalid response. Retrying... ({retries + 1}/{max_retries})\")\n",
        "        retries += 1\n",
        "\n",
        "    # If all retries fail, default to 0 and log the failure\n",
        "    print(f\"Error: {agent.name} failed to provide a valid response after {max_retries} attempts. Defaulting to 0.\")\n",
        "    return 0\n",
        "\n",
        "def execute_game_rounds(agents, na, nr, e, m, csv_writer, records_file, game_index, prompt_label, exp_type, num_dummy_agents):\n",
        "    \"\"\"\n",
        "    Executes a full game session consisting of multiple rounds where agents contribute to a shared pool.\n",
        "    Args:\n",
        "        agents (list): List of agent objects.\n",
        "        na (int): Number of agents.\n",
        "        nr (int): Number of rounds.\n",
        "        e (int): Endowment per round.\n",
        "        m (float): Multiplier for contributions.\n",
        "        csv_writer (csv.writer): CSV writer object.\n",
        "        records_file (file object): File for logging agent responses.\n",
        "        game_index (int): Game identifier.\n",
        "        prompt_label (str): Label for the prompt or story used.\n",
        "        exp_type (str): \"same_story\" or \"different_story\" (to determine CSV formatting).\n",
        "        num_dummy_agents: Number of dummy agents in the game\n",
        "    Returns:\n",
        "        effective_score (float): The overall collaboration score.\n",
        "        total_rewards (list): Cumulative rewards for each agent.\n",
        "    \"\"\"\n",
        "    total_game_contributions = 0\n",
        "    total_rewards = [0 for _ in range(na)]\n",
        "\n",
        "    print(\"\\n=== Starting a New Game ===\")\n",
        "    for round_num in range(1, nr + 1):\n",
        "        print(f\"\\n--- Round {round_num} ---\")\n",
        "        contributions = []\n",
        "\n",
        "        # Each agent selects a contribution\n",
        "        for i, agent in enumerate(agents):\n",
        "            contribution = get_valid_contribution(agent, round_num, e)\n",
        "\n",
        "            # Enforce valid contribution range\n",
        "            available_tokens = e  # Each agent gets `e` tokens every round\n",
        "            if contribution > available_tokens:\n",
        "                print(f\"{agent.name} attempted to contribute {contribution} tokens but only has {available_tokens}. \"\n",
        "                    f\"Reducing contribution to {available_tokens}.\")\n",
        "                contribution = available_tokens\n",
        "            contribution = max(0, contribution)\n",
        "            contributions.append(contribution)\n",
        "\n",
        "        print(f\"Round Contributions: {contributions}\")\n",
        "        round_total = sum(contributions)\n",
        "        total_game_contributions += round_total\n",
        "\n",
        "        # Calculate payoffs for the round.\n",
        "        payoffs = calculate_payoffs(contributions, e, m, na)\n",
        "        print(f\"Round Payoffs: {payoffs}\")\n",
        "\n",
        "        # Update each agent's cumulative reward and send them a round summary.\n",
        "        for idx, agent in enumerate(agents):\n",
        "            total_rewards[idx] += payoffs[idx]\n",
        "            summary = (\n",
        "                f\"Round {round_num} Summary:\\n\"\n",
        "                f\" - Your contribution: {contributions[idx]}\\n\"\n",
        "                f\" - Total contributions: {round_total}\\n\"\n",
        "                f\" - Your payoff this round: {payoffs[idx]:.2f}\\n\"\n",
        "                f\" - Your cumulative reward: {total_rewards[idx]:.2f}\"\n",
        "            )\n",
        "            agent.chat(summary)\n",
        "            story_or_prompt_label = agent.story_label if exp_type == \"different_story\" else prompt_label\n",
        "            # Write per-round info to the CSV file.\n",
        "            csv_writer.writerow([\n",
        "                game_index,\n",
        "                story_or_prompt_label,\n",
        "                round_num,\n",
        "                agent.name,\n",
        "                contributions[idx],\n",
        "                f\"{payoffs[idx]:.2f}\",\n",
        "                f\"{total_rewards[idx]:.2f}\",\n",
        "                \"\"  # CollaborationScore left empty for per-round details.\n",
        "            ])\n",
        "\n",
        "    # Compute the effective collaboration score.\n",
        "    max_possible = (na - num_dummy_agents) * e * nr\n",
        "    effective_score = total_game_contributions / max_possible\n",
        "    print(f\"\\nEffective Collaboration Score for this game: {effective_score:.2f}\")\n",
        "\n",
        "    # Write the final row with the collaboration score.\n",
        "    csv_writer.writerow([\n",
        "        game_index,\n",
        "        prompt_label,\n",
        "        \"final\",\n",
        "        \"All\",\n",
        "        \"\",\n",
        "        \"\",\n",
        "        \"\",\n",
        "        f\"{effective_score:.2f}\"\n",
        "    ])\n",
        "\n",
        "    return effective_score, total_rewards\n",
        "\n",
        "\n",
        "def run_single_game(game_index: int, prompt_label: str, system_prompt_used: str,\n",
        "                    na: int, nr: int, e: int, m: float, csv_writer, records_file, num_dummy_agents, exp_type) -> float:\n",
        "    \"\"\"\n",
        "    Run a single game (with nr rounds) using the given system prompt and experiment parameters.\n",
        "    Returns the effective collaboration score for the game.\n",
        "    \"\"\"\n",
        "    # Create new agents for this game.\n",
        "    agents = []\n",
        "    for i in range(na):\n",
        "        if i < num_dummy_agents:\n",
        "            # Create a dummy agent\n",
        "            agent = DummyAgent(f\"Agent_{i+1}\", system_prompt_used, records_file)\n",
        "        else:\n",
        "            # Create a standard LLM-based agent\n",
        "            agent = Agent(f\"Agent_{i+1}\", system_prompt_used, records_file)\n",
        "        agents.append(agent)\n",
        "\n",
        "    for agent in agents:\n",
        "        agent.story_label = prompt_label\n",
        "\n",
        "    # Executes all rounds of the game, tracking contributions, payoffs, and collaboration scores.\n",
        "    effective_score, _ = execute_game_rounds(\n",
        "        agents, na, nr, e, m, csv_writer, records_file, game_index, prompt_label, exp_type, num_dummy_agents\n",
        "    )\n",
        "    return effective_score\n",
        "\n",
        "def run_single_game_random_story(game_index: int, system_prompt_story: str, na: int, nr: int, e: int, m: float,\n",
        "                                csv_writer, records_file, story_prompts: dict, exp_type) -> (float, list):\n",
        "    \"\"\"\n",
        "    Run a single game where each agent gets a random story.\n",
        "    Returns:\n",
        "    - effective_score: overall collaboration score (total game contributions divided by maximum possible)\n",
        "    - agent_results: list of tuples (agent_name, story_label, cumulative_reward) for each agent.\n",
        "    \"\"\"\n",
        "    agents = []\n",
        "\n",
        "    # Create agents with random story prompts.\n",
        "    for i in range(na):\n",
        "        chosen_label, chosen_story = random.choice(list(story_prompts.items()))\n",
        "        # Insert the chosen story into the base system prompt.\n",
        "        prompt_text = system_prompt_story.replace(\"STORY\", chosen_story)\n",
        "        agent = Agent(f\"Agent_{i+1}\", prompt_text, records_file)\n",
        "        agent.story_label = chosen_label\n",
        "        agents.append(agent)\n",
        "\n",
        "    # Executes all rounds of the game, tracking contributions, payoffs, and collaboration scores.\n",
        "    effective_score, total_rewards = execute_game_rounds(\n",
        "        agents, na, nr, e, m, csv_writer, records_file, game_index, \"All\", exp_type, 0\n",
        "    )\n",
        "\n",
        "    # Prepare results: (agent_name, story_label, cumulative_reward)\n",
        "    agent_results = [(agents[i].name, agents[i].story_label, total_rewards[i]) for i in range(na)]\n",
        "    return effective_score, agent_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yDnYCe0Vovd"
      },
      "source": [
        "# Configure and Launch Game Simulations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aNYW13HkVo-m"
      },
      "outputs": [],
      "source": [
        "def run_same_story_experiment(is_bad_apple, story_index, num_rounds_list, endowment_list, multiplier_list, num_games, num_agents_list, exp_type):\n",
        "    \"\"\"Runs the same story experiment using SLURM array index.\"\"\"\n",
        "    story_index = story_index\n",
        "    story_files = sorted(glob.glob(\"stories/*.txt\"))\n",
        "\n",
        "    if story_index >= len(story_files):\n",
        "        print(\"Invalid story index. Exiting.\")\n",
        "        sys.exit(1)\n",
        "    selected_story_file = story_files[story_index]\n",
        "    story_name = os.path.splitext(os.path.basename(selected_story_file))[0]\n",
        "    for na, nr, e, m in itertools.product(num_agents_list, num_rounds_list, endowment_list, multiplier_list):\n",
        "        exp_name = f\"{'bad_apple' if is_bad_apple else 'same_story'}_{story_name}_ag{na}_ro{nr}_end{e}_mult{m}_temp0.8\"\n",
        "        num_dummy_agents = 1 if is_bad_apple else 0\n",
        "\n",
        "        print(f\"\\n\\n######################\\nRunning experiment: {exp_name}\\n######################\\n\")\n",
        "\n",
        "        intermediate_results, intermediate_results_filename = load_intermediate_results(exp_name, is_dict=True)\n",
        "\n",
        "        records_file, game_results_file, csv_writer = prepare_experiment(exp_name, [\"Game\", \"PromptType\", \"Round\", \"AgentName\", \"Contribution\", \"RoundPayoff\", \"CumulativePayoff\", \"CollaborationScore\"])\n",
        "\n",
        "        # If intermediate results exist, re-populate the CSV file with final rows from previous games.\n",
        "        if intermediate_results:\n",
        "            for prompt_label, score_list in intermediate_results.items():\n",
        "                for game_index, score in enumerate(score_list, start=1):\n",
        "                    csv_writer.writerow([\n",
        "                        game_index,\n",
        "                        prompt_label,\n",
        "                        \"final\",\n",
        "                        \"All\",\n",
        "                        \"\",\n",
        "                        \"\",\n",
        "                        \"\",\n",
        "                        f\"{score:.2f}\"\n",
        "                    ])\n",
        "\n",
        "        # Build a dynamic system prompt using the current parameters.\n",
        "        system_prompt_story = get_system_prompt(na, e, m)\n",
        "\n",
        "        prompt_categories = dict()\n",
        "        with open(selected_story_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            story_content = f.read()\n",
        "\n",
        "        if story_name not in [\"maxreward\", \"noinstruct\"]:\n",
        "            story_content = \"Your behavior is influenced by the following bedtime story your mother read to you every night: \" + story_content\n",
        "\n",
        "        prompt_categories[story_name] = system_prompt_story.replace(\"STORY\", story_content)\n",
        "        # Ensure all prompt labels are present in intermediate_results.\n",
        "        for prompt_label in prompt_categories:\n",
        "            if prompt_label not in intermediate_results:\n",
        "                intermediate_results[prompt_label] = []\n",
        "\n",
        "        # Run num_games for every story prompt.\n",
        "        scores_by_prompt = {}\n",
        "        for prompt_label, prompt_text in prompt_categories.items():\n",
        "            scores_by_prompt[prompt_label] = intermediate_results[prompt_label][:]\n",
        "            print(f\"\\n=== Running Games with prompt: {prompt_label} for experiment {exp_name} ===\")\n",
        "            # Determine how many games have already been run for this prompt.\n",
        "            games_already_run = len(intermediate_results[prompt_label])\n",
        "            for game in range(games_already_run + 1, num_games + 1):\n",
        "                print(f\"\\n=== Game {game} ({prompt_label}) for experiment {exp_name} ===\")\n",
        "                score = run_single_game(game, prompt_label, prompt_text, na, nr, e, m, csv_writer, records_file, num_dummy_agents, exp_type)\n",
        "                scores_by_prompt[prompt_label].append(score)\n",
        "                intermediate_results[prompt_label].append(score)\n",
        "                # Save intermediate results after each game.\n",
        "                save_intermediate_results(intermediate_results, intermediate_results_filename)\n",
        "\n",
        "        # Compute and print statistics for each prompt category.\n",
        "        compute_and_print_statistics(scores_by_prompt, exp_name)\n",
        "        game_results_file.close()\n",
        "        records_file.close()\n",
        "\n",
        "def run_different_story_experiment(num_rounds_list, endowment_list, multiplier_list, num_games, num_agents_list, exp_type):\n",
        "    \"\"\"Runs the different story experiment where each agent has a unique story.\"\"\"\n",
        "    exp_name = \"different_story\"\n",
        "\n",
        "    for na, nr, e, m in itertools.product(num_agents_list, num_rounds_list, endowment_list, multiplier_list):\n",
        "        exp_name = f\"different_story_ag{na}_ro{nr}_end{e}_mult{m}_temp0.8\"\n",
        "        print(f\"\\n\\n######################\\nRunning experiment: {exp_name}\\n######################\\n\")\n",
        "\n",
        "        # Load all story files from the \"stories\" folder.\n",
        "        story_prompts = {}\n",
        "        for story_file in sorted(glob.glob(\"stories/*.txt\")):\n",
        "            story_name = os.path.splitext(os.path.basename(story_file))[0]\n",
        "            with open(story_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                story_content = f.read()\n",
        "            # If the story is not a special case, prepend an influence message.\n",
        "            if story_name not in [\"maxreward\", \"noinstruct\"]:\n",
        "                story_content = \"Your behavior is influenced by the following bedtime story your mother read to you every night: \" + story_content\n",
        "            story_prompts[story_name] = story_content\n",
        "\n",
        "        # Build a base system prompt that includes a placeholder \"STORY\".\n",
        "        system_prompt_story = get_system_prompt(na, e, m)\n",
        "\n",
        "        intermediate_results, intermediate_results_filename = load_intermediate_results(exp_name, is_dict=False)\n",
        "\n",
        "        records_file, game_results_file, csv_writer = prepare_experiment(exp_name, [\"Game\", \"PromptType\", \"Round\", \"AgentName\", \"Contribution\", \"RoundPayoff\", \"CumulativePayoff\", \"CollaborationScore\"])\n",
        "\n",
        "\n",
        "        # Run num_games games.\n",
        "        for game in range(1, num_games + 1):\n",
        "            # Skip games that have already been run.\n",
        "            if game <= len(intermediate_results):\n",
        "                print(f\"Skipping game {game} as it has already been run.\")\n",
        "                continue\n",
        "            print(f\"\\n=== Game {game} for experiment {exp_name} ===\")\n",
        "            effective_score, agent_results = run_single_game_random_story(\n",
        "                game, system_prompt_story, na, nr, e, m, csv_writer, records_file, story_prompts, exp_type\n",
        "            )\n",
        "            intermediate_results.append((game, effective_score, agent_results))\n",
        "            # Save intermediate results after each game.\n",
        "            save_intermediate_results(intermediate_results, intermediate_results_filename)\n",
        "\n",
        "\n",
        "        # Compute and print statistics for each story.\n",
        "        rewards_by_story = {story_label: [] for story_label in story_prompts.keys()}\n",
        "        scores_list = []  # to collect overall effective collaboration scores\n",
        "        for game_tuple in intermediate_results:\n",
        "            _, effective_score, agent_results = game_tuple\n",
        "            scores_list.append(effective_score)\n",
        "            for _, story_label, reward in agent_results:\n",
        "                rewards_by_story[story_label].append(reward)\n",
        "\n",
        "        print(f\"\\n=== Rewards by Story for experiment {exp_name} ===\")\n",
        "        compute_and_print_statistics(rewards_by_story, exp_name)\n",
        "        overall_mean = statistics.mean(scores_list) if scores_list else 0\n",
        "        overall_stdev = statistics.stdev(scores_list) if len(scores_list) > 1 else 0\n",
        "        print(f\"\\nOverall Effective Collaboration Score: Mean = {overall_mean:.2f}, SD = {overall_stdev:.2f}\")\n",
        "\n",
        "        # Close files for the experiment.\n",
        "        game_results_file.close()\n",
        "        records_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sYsML6xVxsq"
      },
      "source": [
        "# Main Execution Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpQ_1GJ7fN98"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Configurable Run Function\n",
        "# -----------------------------\n",
        "def run_experiment(exp_type, story_index=0):\n",
        "\n",
        "    # -----------------------------\n",
        "    # Experiment Configurations\n",
        "    # -----------------------------\n",
        "    num_rounds_list = [5]\n",
        "    endowment_list = [10]\n",
        "    multiplier_list = [1.5]\n",
        "\n",
        "    if exp_type in [\"same_story\", \"bad_apple\"]:\n",
        "        num_games = 100\n",
        "        num_agents_list = [4, 16, 32] if exp_type == \"same_story\" else [16]\n",
        "        run_same_story_experiment(\n",
        "            is_bad_apple=(exp_type == \"bad_apple\"),\n",
        "            story_index=story_index,\n",
        "            num_rounds_list=num_rounds_list,\n",
        "            endowment_list=endowment_list,\n",
        "            multiplier_list=multiplier_list,\n",
        "            num_games=num_games,\n",
        "            num_agents_list=num_agents_list,\n",
        "            exp_type=exp_type\n",
        "        )\n",
        "    elif exp_type == \"different_story\":\n",
        "        num_games = 400\n",
        "        num_agents_list = [16]\n",
        "        run_different_story_experiment(\n",
        "            num_rounds_list, endowment_list, multiplier_list, num_games, num_agents_list, exp_type\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hpQpsN3V_LW"
      },
      "source": [
        "# Homogenous Experiment\n",
        "\n",
        "Runs 100 games per story for agent sizes [4, 16, 32].\n",
        "\n",
        "(a) Cooperation Among Homogeneous Agents\n",
        "To run across the experiment for all stories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikvqrCqLWRH2"
      },
      "outputs": [],
      "source": [
        "# Run all 12 stories for same_story\n",
        "for i in range(12):\n",
        "    print(f\"Running same_story experiment for story {i}\")\n",
        "    run_experiment(\"same_story\", story_index=i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uom47ImfuD_"
      },
      "source": [
        "(b) Robustness Experiment\n",
        "Same as the same story experiment, but introduces one dummy agent who always contributes 0.\n",
        "\n",
        "To run across the experiment for all stories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CmdzIrAfyLB"
      },
      "outputs": [],
      "source": [
        "# Run all 12 stories for bad_apple\n",
        "for i in range(12):\n",
        "    print(f\"Running bad_apple experiment for story {i}\")\n",
        "    run_experiment(\"bad_apple\", story_index=i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxilIE3Cf1t1"
      },
      "source": [
        "# Heterogenous Experiment\n",
        "\n",
        "Assigns a random story to each agent and runs 200 games with 4 agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qj-R-iN6f5cp"
      },
      "outputs": [],
      "source": [
        "# Run different_story experiment once\n",
        "print(\"Running different_story experiment\")\n",
        "run_experiment(\"different_story\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualization\n",
        "\n",
        "\n",
        "The project includes scripts to visualize collaboration and scaling results.\n",
        "\n",
        "# 1. Distribution Analysis Plots\n",
        "\n",
        "Generates violin plots for different experiment types:\n",
        "\n",
        "Collaboration Score for Homogenous and Robustness experiments.\n",
        "Payoff per Agent for Heterogenous experiment.\n",
        "\n",
        "Run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOa8WPkFhYAk"
      },
      "outputs": [],
      "source": [
        "# Define color dictionary for plot consistency\n",
        "COLOR_DICT = {\n",
        "    # Baseline condition (Shades of Blue)\n",
        "    \"maxreward\": \"#87CEFA\",\n",
        "    \"noinstruct\": \"#4682B4\",\n",
        "    \"nsCarrot\": \"#4169E1\",\n",
        "    \"nsPlumber\": \"#00008B\",\n",
        "    # Meaningful stories (Shades of Purple/Pink)\n",
        "    \"Odyssey\": \"#FFB3E6\",\n",
        "    \"Soup\": \"#FF99CC\",\n",
        "    \"Peacemaker\": \"#FF66B3\",\n",
        "    \"Musketeers\": \"#FF4D9E\",\n",
        "    \"Teamwork\": \"#F02278\",\n",
        "    \"Spoons\": \"#D81B60\",\n",
        "    \"Turnip\": \"#B83B7D\",\n",
        "    \"OldManSons\": \"#B22272\",\n",
        "}\n",
        "\n",
        "# Ensure vectorized rendering\n",
        "mpl.rcParams['savefig.format'] = 'pdf'\n",
        "mpl.rcParams['pdf.fonttype'] = 42\n",
        "mpl.rcParams['ps.fonttype'] = 42\n",
        "\n",
        "def load_csv_files(pattern):\n",
        "    \"\"\"Loads all CSV files matching a pattern and merges them into a DataFrame.\"\"\"\n",
        "    files = glob.glob(pattern)\n",
        "    if not files:\n",
        "        print(f\"No files found for pattern: {pattern}\")\n",
        "        return None\n",
        "    return pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
        "\n",
        "\n",
        "def preprocess_data(df, metric):\n",
        "    \"\"\"\n",
        "    Prepares data by filtering only final round rows and converting columns to numeric types.\n",
        "    Metric can be 'CollaborationScore' or 'CumulativePayoff'.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    if metric == \"CollaborationScore\":\n",
        "        df = df[df[\"Round\"] == \"final\"].copy()\n",
        "    else:  # \"CumulativePayoff\"\n",
        "        df = df[df[\"Round\"] != \"final\"].copy()\n",
        "        df[\"Round\"] = pd.to_numeric(df[\"Round\"], errors=\"coerce\")\n",
        "        df.dropna(subset=[\"Round\"], inplace=True)\n",
        "        df = df.loc[df.groupby([\"Game\", \"AgentName\"])[\"Round\"].idxmax()].copy()\n",
        "\n",
        "    df[metric] = pd.to_numeric(df[metric], errors=\"coerce\")\n",
        "    df.dropna(subset=[metric], inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def plot_violin(df, metric, title, output_pdf, plot_mean_line=True, show_legend=False):\n",
        "    \"\"\"\n",
        "    Plots a violin plot for CollaborationScore or CumulativePayoff.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        print(f\"No data to visualize for {title}\")\n",
        "        return\n",
        "\n",
        "    # Compute x-axis order based on mean metric values\n",
        "    order = (df.groupby(\"PromptType\")[metric]\n",
        "            .mean()\n",
        "            .sort_values(ascending=True)\n",
        "            .index.tolist())\n",
        "\n",
        "    plt.figure(figsize=(12, 7))\n",
        "\n",
        "    # Define color palette\n",
        "    palette = {cat: COLOR_DICT.get(cat, \"#888888\") for cat in order}\n",
        "\n",
        "    # Violin plot with embedded box plot\n",
        "    ax = sns.violinplot(\n",
        "        data=df,\n",
        "        x=\"PromptType\",\n",
        "        y=metric,\n",
        "        hue=\"PromptType\",\n",
        "        palette=palette,\n",
        "        inner=\"box\",\n",
        "        dodge=False,\n",
        "        order=order,\n",
        "        bw_adjust=5, # Adjusting KDE bandwidth\n",
        "        scale=\"area\", # Uniform width across all violins\n",
        "    )\n",
        "\n",
        "    if ax.get_legend():\n",
        "        ax.get_legend().remove()\n",
        "\n",
        "    # Overlay scatter points\n",
        "    sns.stripplot(\n",
        "        data=df,\n",
        "        x=\"PromptType\",\n",
        "        y=metric,\n",
        "        color=\"black\",\n",
        "        dodge=False,\n",
        "        alpha=0.2,\n",
        "        size=2,\n",
        "        zorder=2,\n",
        "        order=order\n",
        "    )\n",
        "\n",
        "    # (Optional) Plot mean trend line\n",
        "    if plot_mean_line:\n",
        "        means = df.groupby(\"PromptType\")[metric].mean().loc[order]\n",
        "        x_positions = list(range(len(order)))\n",
        "        plt.plot(\n",
        "            x_positions, means.values, marker='o',\n",
        "            color='black', linestyle='-', linewidth=2,\n",
        "            markersize=6, alpha=0.5, label=\"Mean Trend\"\n",
        "        )\n",
        "        if show_legend:\n",
        "            plt.legend([\"Mean Trend\"])\n",
        "\n",
        "    # Customize plot\n",
        "\n",
        "    if metric == \"CollaborationScore\":\n",
        "        plt.ylim(0, 1.3)  # Set range for Collaboration Score\n",
        "    elif metric == \"CumulativePayoff\":\n",
        "        plt.ylim(0, 120) #Set range for Cumulative Payoff\n",
        "    plt.xlabel(\"Story Prompt\", fontsize=18, labelpad=15)\n",
        "\n",
        "    ylabel_text = \"Payoff per Agent\" if metric == \"CumulativePayoff\" else \"Collaboration Score\"\n",
        "\n",
        "    plt.ylabel(f\"{ylabel_text}\", fontsize=18, labelpad=15)\n",
        "    plt.title(title, fontsize=20, weight=\"bold\" , pad=20)\n",
        "    plt.xticks(rotation=90 if len(order) > 5 else 0, fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "\n",
        "    sns.despine()\n",
        "    plt.grid(False)\n",
        "\n",
        "    # Save the plot as Pdf\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_pdf, bbox_inches='tight', format='pdf', transparent=False)  # Fully vectorized PDF\n",
        "    print(f\"Figure saved as {output_pdf}\")\n",
        "\n",
        "# Define experiment types and file patterns\n",
        "CATEGORIES = {\n",
        "    \"same_story_4_agents\": \"game_results_same_story_*_ag4_ro5_end10_mult1.5.csv\",\n",
        "    \"same_story_16_agents\": \"game_results_same_story_*_ag16_ro5_end10_mult1.5.csv\",\n",
        "    \"same_story_32_agents\": \"game_results_same_story_*_ag32_ro5_end10_mult1.5.csv\",\n",
        "    \"different_story_16_agents\": \"game_results_different_story_ag16_ro5_end10_mult1.5.csv\",\n",
        "    \"bad_apple_16_agents\": \"game_results_bad_apple_*_ag16_ro5_end10_mult1.5.csv\"\n",
        "}\n",
        "\n",
        "for category, pattern in CATEGORIES.items():\n",
        "    agent_count = category.split(\"_\")[-2]  # Extract agent count dynamically\n",
        "\n",
        "    if \"different_story\" in category:\n",
        "        # Different story -> Cumulative Payoff\n",
        "        csv_files = glob.glob(pattern)\n",
        "        for csv_file in csv_files:\n",
        "            output_file = csv_file.replace(\"game_results\", \"cumulative_payoffs\").replace(\".csv\", \".jpg\")\n",
        "            df = preprocess_data(pd.read_csv(csv_file), \"CumulativePayoff\")\n",
        "\n",
        "            title = f\"Heterogenous Experiment\"\n",
        "\n",
        "            plot_violin(df, \"CumulativePayoff\", title, output_file)\n",
        "    else:\n",
        "        # Same story & bad apple -> Collaboration Score\n",
        "        df = load_csv_files(pattern)\n",
        "        df = preprocess_data(df, \"CollaborationScore\")\n",
        "        if df is not None:\n",
        "            output_file = f\"{category}_collaboration_scores.pdf\"\n",
        "\n",
        "            if \"bad_apple\" in category:\n",
        "                title = f\"Robustness\"\n",
        "            else:\n",
        "                title = f\"Homogenous Experiment\"\n",
        "\n",
        "            plot_violin(df, \"CollaborationScore\", title, output_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzY4p1R5iFkp"
      },
      "source": [
        "# 2. Scaling Experiment Visualization\n",
        "\n",
        "Plots the mean collaboration score across agent sizes to analyze scaling effects in Homogenous Experiment\n",
        "\n",
        "Run:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0bXJYQniJ1R"
      },
      "outputs": [],
      "source": [
        "# Define agent sizes used in the experiment\n",
        "agent_sizes = [4, 16, 32]\n",
        "\n",
        "# Define color gradients for baseline (blue) and meaningful stories (pink)\n",
        "BLUE_SHADES = [\"#87CEFA\", \"#4682B4\", \"#4169E1\", \"#00008B\"]  # Light to Dark Blue\n",
        "PINK_SHADES = [\"#FFB3E6\", \"#FF99CC\", \"#FF66B3\", \"#FF4D9E\", \"#F02278\", \"#D81B60\", \"#B83B7D\", \"#B22272\"]  # Light to Dark Pink\n",
        "\n",
        "# Define file categories for different temperatures\n",
        "CATEGORY_GROUPS = {\n",
        "    \"temp_0.6\": {\n",
        "        4: \"game_results_same_story_*_ag4_ro5_end10_mult1.5_temp_0.6.csv\",\n",
        "        16: \"game_results_same_story_*_ag16_ro5_end10_mult1.5_temp_0.6.csv\",\n",
        "        32: \"game_results_same_story_*_ag32_ro5_end10_mult1.5_temp_0.6.csv\",\n",
        "    },\n",
        "}\n",
        "\n",
        "def process_category(temp_label, CATEGORIES):\n",
        "    \"\"\"Processes a single temperature condition and generates a visualization.\"\"\"\n",
        "    story_scores = {}\n",
        "\n",
        "    # Load CSV data and extract mean collaboration scores\n",
        "    for agent_count, pattern in CATEGORIES.items():\n",
        "        files = glob.glob(pattern)\n",
        "        if not files:\n",
        "            print(f\"No files found for pattern: {pattern}\")\n",
        "            continue\n",
        "\n",
        "        df = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
        "\n",
        "        # Compute mean collaboration scores per story\n",
        "        story_means = df.groupby(\"PromptType\")[\"CollaborationScore\"].mean()\n",
        "        story_scores[agent_count] = story_means.to_dict()  # Store scores\n",
        "\n",
        "    # Extract the order of stories as they appear at N=4 in ascending order\n",
        "    starting_order = sorted(story_scores[4].items(), key=lambda x: x[1])\n",
        "    starting_stories = [story for story, _ in starting_order]\n",
        "\n",
        "    print(f\"Processing {temp_label}: starting_stories = {starting_stories}\")\n",
        "\n",
        "    # Dynamically assign colors based on order in starting_stories\n",
        "    COLOR_DICT = {}\n",
        "    blue_idx, pink_idx = 0, 0  # Track index for blue and pink shades\n",
        "\n",
        "    for story in starting_stories:\n",
        "        if story in [\"noinstruct\", \"nsCarrot\", \"nsPlumber\", \"maxreward\"]:  # Baseline stories\n",
        "            COLOR_DICT[story] = BLUE_SHADES[blue_idx]\n",
        "            blue_idx += 1  # Move to next darker shade\n",
        "        else:  # Meaningful stories\n",
        "            COLOR_DICT[story] = PINK_SHADES[pink_idx]\n",
        "            pink_idx += 1  # Move to next darker shade\n",
        "\n",
        "    # Plot settings\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    legend_handles = []\n",
        "\n",
        "    # Plot each story's progression across agent sizes\n",
        "    for story in COLOR_DICT.keys():  # Only plot stories in COLOR_DICT\n",
        "        positions = []\n",
        "\n",
        "        for agent_count in agent_sizes:\n",
        "            if agent_count in story_scores and story in story_scores[agent_count]:\n",
        "                x_pos = story_scores[agent_count][story]\n",
        "                y_pos = agent_sizes.index(agent_count)\n",
        "                positions.append((x_pos, y_pos))\n",
        "\n",
        "                # Scatter plot for each point\n",
        "                plt.scatter(x_pos, y_pos, s=70, facecolors=\"none\", edgecolors=COLOR_DICT[story], linewidths=1.5, label=story if agent_count == 4 else \"\")\n",
        "\n",
        "        if len(positions) > 1:\n",
        "            x_vals, y_vals = zip(*positions)\n",
        "            plt.plot(x_vals, y_vals, linestyle=\"dashed\", color=COLOR_DICT[story], alpha=0.7)\n",
        "\n",
        "    for story in starting_stories:\n",
        "        legend_handles.append(\n",
        "            mlines.Line2D(\n",
        "                [], [], marker=\"o\", linestyle=\"None\", markersize=8, color=COLOR_DICT.get(story, \"#888888\"), label=story\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Customizing plot\n",
        "    plt.xlabel(\"Mean Collaboration Score\", fontsize=18, labelpad=15)\n",
        "    plt.ylabel(\"Agent Size\", fontsize=18, labelpad=15)\n",
        "    plt.yticks(range(len(agent_sizes)), [f\"N = {n}\" for n in agent_sizes], fontsize=14, weight=\"bold\")\n",
        "    plt.title(f\"Scaling Experiment\", fontsize=20, weight=\"bold\", pad=20)\n",
        "    plt.grid(axis=\"y\", linestyle=\"dotted\")\n",
        "\n",
        "    plt.legend(\n",
        "        handles=legend_handles, title=\"Story\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=12\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save as Pdf\n",
        "    filename_base = f\"scaling_experiment_collab_score_{temp_label}\"\n",
        "    plt.savefig(f\"{filename_base}.pdf\", bbox_inches=\"tight\", format=\"pdf\")\n",
        "\n",
        "    print(f\"Scaling experiment figures saved as {filename_base}.pdf\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Run the process for each category\n",
        "for temp_label, category_dict in CATEGORY_GROUPS.items():\n",
        "    process_category(temp_label, category_dict)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm_deliberation",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
